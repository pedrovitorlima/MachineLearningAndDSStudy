{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqoTb9ogAlus"
   },
   "source": [
    "# Clustering Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6MdVjNASKyu"
   },
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnAlx5i257O_"
   },
   "source": [
    "\n",
    "So far, we've looked at regression and classification. These are both types of [supervised machine learning](https://en.wikipedia.org/wiki/Supervised_learning). In supervised learning, you train an algorithm to predict an unknown variable from known variables.\n",
    "\n",
    "Another major type of machine learning is called [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning). In unsupervised learning, we aren't trying to predict anything. Instead, we're finding patterns in data.\n",
    "\n",
    "One of the main unsupervised learning techniques is called **clustering**. We use clustering when we're trying to explore a dataset, and understand the connections between the various rows and columns. For example, we can cluster NBA players based on their statistics. Here's how such a clustering might look:\n",
    "\n",
    "<center><img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1NUuctg1HZwvCQlFJTAYzsqgrhlDpi5VB\"></center>\n",
    "\n",
    "\n",
    "**The clusters made it possible to discover player roles that might not have been noticed otherwise**. [Here's](https://flowingdata.com/2012/03/21/redefining-nba-basketball-positions/) an article that describes how the clusters were created.\n",
    "\n",
    "Clustering algorithms group similar rows together. There can be one or more groups in the data, and these groups form the clusters. As we look at the clusters, we can start to better understand the structure of the data.\n",
    "\n",
    "Clustering is a key way to explore unknown data, and it's a very commonly used machine learning technique. In this lesson we'll work on **clustering US Senators based on how they voted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UahH2amJSKyw"
   },
   "source": [
    "## The dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7hXTJ1059OH"
   },
   "source": [
    "\n",
    "\n",
    "In the US, the **Senate votes on proposed legislation**. Getting a bill passed by the Senate is a key step towards getting its provisions enacted. A **majority vote is required** to get a bill passed.\n",
    "\n",
    "The results of these votes, known as roll call votes, are public, and available in a few places, including [here](https://github.com/unitedstates/congress). Read more about the US legislative system [here](https://en.wikipedia.org/wiki/Federal_government_of_the_United_States#Legislative_branch).\n",
    "\n",
    "**Senators typically vote in accordance with how their political party votes**, known as voting along party lines. In the US, the 2 main political parties are the [Democrats](https://en.wikipedia.org/wiki/Democratic_Party), who tend to be [liberal](https://en.wikipedia.org/wiki/Liberalism), and the [Republicans](https://en.wikipedia.org/wiki/Republican_Party), who tend to be [conservative](https://en.wikipedia.org/wiki/Conservatism). **Senators** can also choose to be unaffiliated with a party, and vote as **Independents**, although very few choose to do so.\n",
    "\n",
    "**114_congress.csv** contains all of the results of roll call votes from the **114th Senate**. Each row represents a single Senator, and each column represents a vote. A 0 in a cell means the Senator voted No on the bill, 1 means the Senator voted Yes, and 0.5 means the Senator abstained.\n",
    "\n",
    "Here are the relevant columns:\n",
    "\n",
    "- **name** -- The last name of the Senator.\n",
    "- **party** -- the party of the Senator. The valid values are D for Democrat, R for Republican, and I for Independent.\n",
    "- Several columns numbered like **00001**, **00004**, etc. Each of these columns represents the results of a single roll call vote.\n",
    "\n",
    "Below are the first three rows of the data. As you can see, the number of each bill is used as the column heading for its votes:\n",
    "\n",
    "```python\n",
    "name,party,state,00001,00004,00005,00006,00007,00008,00009,00010,00020,00026,00032,00038,00039,00044,00047\n",
    "Alexander,R,TN,0,1,1,1,1,0,0,1,1,1,0,0,0,0,0\n",
    "Ayotte,R,NH,0,1,1,1,1,0,0,1,0,1,0,1,0,1,0\n",
    "```\n",
    "\n",
    "**Clustering voting data of Senators is particularly interesting because it can expose patterns that go deeper than party affiliation**. For example, some Republicans are more liberal than the rest of their party. Looking at voting data can help us discover the Senators who are more or less in the mainstream of their party.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Import the Pandas library.\n",
    "- Use the **read_csv()** method in Pandas to read **114_congress.csv** into the variable **votes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h227-Os7-DLP"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aG7vBZWC_nmy"
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KPlzNoL_un0"
   },
   "source": [
    "Now that we've read the data in, let's do some initial exploration of the data.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Find how many Senators are in each party.\n",
    "  - Use the **value_counts()** method on the **party** column of **votes**. Print the results.\n",
    "- Find what the \"average\" vote for each bill was.\n",
    "  - Use the **mean()** method on the **votes** Dataframe. If the mean for a column is less than .5, more Senators voted against the bill, and vice versa if it's over .5. Print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLpL38o9A0kR"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kf8iH0QlSKy6"
   },
   "source": [
    "## Distance between Senators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvlkmFkVA4x-"
   },
   "source": [
    "To group Senators together, we need some way to figure out how \"close\" the Senators are to each other. **We'll then group together the Senators that are the closest**. We can actually discover this distance mathematically, by finding how similar the votes of two Senators are. The closer together the voting records of two Senators, the more ideologically similar they are (voting the same way indicates that you share the same views).\n",
    "\n",
    "To find the distance between two rows, we can use [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). The formula is:\n",
    "\n",
    "$$d=\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+\\ldots+(q_n-p_n)^2}$$\n",
    "\n",
    "Let's say we have two Senator's voting records:\n",
    "\n",
    "```python\n",
    "name,party,state,00001,00004,00005,00006,00007,00008,00009,00010,00020,00026,00032,00038,00039,00044,00047\n",
    "Alexander,R,TN,0,1,1,1,1,0,0,1,1,1,0,0,0,0,0\n",
    "Ayotte,R,NH,0,1,1,1,1,0,0,1,0,1,0,1,0,1,0\n",
    "```\n",
    "\n",
    "If we took only the numeric vote columns, we'd have this:\n",
    "\n",
    "```python\n",
    "00001,00004,00005,00006,00007,00008,00009,00010,00020,00026,00032,00038,00039,00044,00047\n",
    "0,1,1,1,1,0,0,1,1,1,0,0,0,0,0\n",
    "0,1,1,1,1,0,0,1,0,1,0,1,0,1,0\n",
    "```\n",
    "\n",
    "If we wanted to compute the Euclidean distance, we'd plug the vote numbers into our formula:\n",
    "\n",
    "$$d=\\sqrt{(0-0)^2+(1-1)^2+(1-1)^2+(1-1)^2+(1-1)^2+(0-0)^2+\\ldots+(0-0)^2}$$\n",
    "\n",
    "As you can see, these Senators are very similar! If you look at the votes above, they only disagree on 3 bills. The final Euclidean distance between these two Senators is **1.73**.\n",
    "\n",
    "To compute Euclidean distance in Python, we can use the [euclidean_distances()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) method in the [scikit-learn](http://scikit-learn.org/stable/) library. The code below will find the Euclidean distance between the Senator in the first row and the Senator in the second row.\n",
    "\n",
    "```python\n",
    "euclidean_distances(votes.iloc[0,3:], votes.iloc[1,3:])\n",
    "```\n",
    "\n",
    "It's necessary to only select columns after the first 3 because the first 3 are name, party, and state, which aren't numeric.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Compute the **Euclidean distance** between the **first row** and the **third row**.\n",
    "  - Assign the result to **distance**.\n",
    "  - tip: use series.values.reshape(1,-1) in euclidean_distance parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKw8BZztCizL"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHKbZM6XSKy-"
   },
   "source": [
    "## Initial clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIY_5qbHFZUe"
   },
   "source": [
    "\n",
    "\n",
    "We'll use an algorithm called [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) to split our data into clusters. k-means clustering uses Euclidean distance to form clusters of similar Senators. We'll dive more into the theory of k-means clustering and build the algorithm from the ground up in the later section. For now, it's important to understand clustering at a high level, so we'll leverage the [scikit-learn](http://scikit-learn.org/stable/) library to train a **k-means model.**\n",
    "\n",
    "The **k-means** algorithm will **group Senators who vote similarly** on bills together, in clusters. Each cluster is assigned a center, and the Euclidean distance from each Senator to the center is computed. Senators are assigned to clusters based on which one they are closest to. **From our background knowledge, we think that Senators will cluster along party lines**.\n",
    "\n",
    "The k-means algorithm requires us to specify the number of clusters upfront. Because we suspect that clusters will occur along party lines, and the vast majority of Senators are either Republicans or Democrats, we'll pick 2 for our number of clusters.\n",
    "\n",
    "We'll use the [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class from scikit-learn to perform the clustering. **Because we aren't predicting anything, there's no risk of overfitting, so we'll train our model on the whole dataset.** After training, we'll be able to extract cluster labels that indicate what cluster each Senator belongs to.\n",
    "\n",
    "We can initialize the model like this:\n",
    "\n",
    "```python\n",
    "kmeans_model = KMeans(n_clusters=2, random_state=1)\n",
    "```\n",
    "\n",
    "The above code will initialize the k-means model with 2 clusters, and a random state of 1 to allow for the same results to be reproduced whenever the algorithm is run.\n",
    "\n",
    "We'll then be able to use the [fit_transform()](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit_transform) method to fit the model to **votes** and get the distance of each Senator to each cluster. The result will look like this:\n",
    "\n",
    "```python\n",
    "array([[ 3.12141628,  1.3134775 ],\n",
    "   [ 2.6146248 ,  2.05339992],\n",
    "   [ 0.33960656,  3.41651746],\n",
    "   [ 3.42004795,  0.24198446],\n",
    "   [ 1.43833966,  2.96866004],\n",
    "   [ 0.33960656,  3.41651746],\n",
    "   [ 3.42004795,  0.24198446],\n",
    "   [ 0.33960656,  3.41651746],\n",
    "   [ 3.42004795,  0.24198446],\n",
    "   [ 0.31287498,  3.30758755],\n",
    "   ...\n",
    "```\n",
    "\n",
    "This is a NumPy array with two columns. The first column is the Euclidean distance from each Senator to the first cluster, and the second column is the Euclidean distance to the the second cluster. The values in the columns will indicate how \"far\" the Senator is from each cluster. The further away from the cluster, the less the Senator's voting history aligns with the voting history of the cluster.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the **fit_transform()** method to fit **kmeans_model** on the **votes** DataFrame. Only select columns after the first 3 from **votes** when fitting.\n",
    "- Assign the result to **senator_distances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2Ey82QNHZ8_"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktr13DOCSKzB"
   },
   "source": [
    "## Exploring the clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y__8i_zcHlDf"
   },
   "source": [
    "\n",
    "\n",
    "We can use the Pandas method [crosstab()](http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.crosstab.html) to compute and display how many Senators from each party ended up in each cluster. The **crosstab()** method takes in two vectors or Pandas Series and computes how many times each unique value in the second vector occurs for each unique value in the first vector.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    "is_smoker =       [0,1,1,0,0,1]\n",
    "has_lung_cancer = [1,0,1,0,1,0]\n",
    "\n",
    "pd.crosstab(np.array(has_lung_cancer),np.array(is_smoker),\n",
    "          colnames=[\"has_lung_cancer\"],rownames=[\"is_smoker\"])\n",
    "\n",
    "```\n",
    "\n",
    "A 0 means False, and a 1 means True. A crosstab for the two above lists would look like this:\n",
    "\n",
    "```python\n",
    "has_lung_cancer    0     1\n",
    "smoker\n",
    "0                  1     2\n",
    "1                  2     1\n",
    "```\n",
    "\n",
    "We can extract the cluster labels for each Senator from **kmeans_model** using **kmeans_model.labels_**, then we can make a table comparing these labels to **votes[\"party\"]** with crosstab(). This will show us if the clusters tend to break down along party lines or not.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the **labels_** attribute to extract the labels from **kmeans_model**. Assign the result to the variable **labels**.\n",
    "- Use the **crosstab()** method to print out a table comparing labels to **votes[\"party\"]**, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROa_3rtwLB__"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VQ1jQyvSKzF"
   },
   "source": [
    "## Exploring Senators in the wrong cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LxFL_jqxLN0n"
   },
   "source": [
    "\n",
    "\n",
    "It looks like both of our clusters mostly broke down along party lines. The first cluster contains 41 Democrats, and **two** Independents. The second cluster contains 3 Democrats, and 54 Republicans.\n",
    "\n",
    "No Republicans seem to have broken party ranks to vote with the Democrats, but 3 Democrats are more similar to Republicans in their voting than their own party. Let's explore these 3 in more depth so we can figure out why that is.\n",
    "\n",
    "We can do this by subsetting votes to only select rows where the party column is D, and the labels variable is 1, indicating that the Senator is in the second cluster.\n",
    "\n",
    "We can perform this subsetting with Pandas. The below code will select all Independents in the first cluster:\n",
    "\n",
    "```python\n",
    "votes[(labels == 0) & (votes[\"party\"] == \"I\")]\n",
    "```\n",
    "\n",
    "When subsetting a DataFrame with multiple conditions, each condition needs to be in parentheses, and separated by **&**.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Select all senators who were assigned to the second cluster that were Democrats. Assign the subset to **democratic_outliers**.\n",
    "- Print out **democratic_outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhXmWEpYNjLN"
   },
   "outputs": [],
   "source": [
    "#put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOLDd1F6Nsjg"
   },
   "source": [
    "## Plotting out the clusters\n",
    "\n",
    "One great way to explore clusters is to visualize them using **matplotlib**. Earlier, we computed a **senator_distances** array that shows the distance from each Senator to the center of each cluster. We can treat these distances as x and y coordinates, and make a scatterplot that shows the position of each Senator. This works because the distances are relative to the cluster centers.\n",
    "\n",
    "While making the scatterplot, we can also shade each point according to party affiliation. This will enable us to quickly look at the layout of the Senators, and see who crosses party lines.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Make a scatterplot using [plt.scatter()](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter). Pass in the following keyword arguments:\n",
    "  - x should be the first column of **senator_distances.**\n",
    "  - y should be the second column of **senator_distances.**\n",
    "  - c should be **labels**. This will shade the points according to label.\n",
    "  - **cmap** should be set to \"RdBu\"\n",
    "- Use **plt.show()** to show the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iksh37LcN34z"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8BnpvUcSKzO"
   },
   "source": [
    "## Finding the most extreme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5A757B1tUDSv"
   },
   "source": [
    "\n",
    "\n",
    "The **most extreme** Senators are those who are the furthest away from one cluster. For example, a radical Republican would be as far from the Democratic cluster as possible. Senators who are in between both clusters are more moderate, as they fall in between the views of the two parties.\n",
    "\n",
    "If we look at the first few rows of **senator_distances**, we can start to see who is more extreme:\n",
    "\n",
    ">```python\n",
    "[\n",
    "       [ 3.12141628,  1.3134775 ], # Slightly moderate, far from cluster 1, close to cluster 2.\n",
    "       [ 2.6146248 ,  2.05339992], # Moderate, far from cluster 1, far from cluster 2.\n",
    "       [ 0.33960656,  3.41651746], # Somewhat extreme, very close to cluster 1, very far from cluster 2.\n",
    "       [ 3.42004795,  0.24198446], # Fairly extreme, very far from cluster 1, very close to cluster 2.\n",
    "       ...\n",
    "   ]\n",
    "```\n",
    "\n",
    "We'll create a formula to find extremists -- we'll cube the distances in both columns of **senator_distances**, then add them together. The higher the exponent we raise a set of numbers to, the more separation we'll see between small values and low values. For instance, squaring [1,2,3] results in [1,4,9], and cubing it results in [1,8,27].\n",
    "\n",
    "We cube the distances so that we can get a good amount of separation between the extremists who are farther away from a party, who have distances that look like extremist = [3.4, .24], and moderates, whose distances look like moderate = [2.6, 2]. If we left the distances as is, we'd end up with 3.4 + .24 = 3.64, and 2.6 + 2 = 4.6, which would make the moderate, who is between both parties, seem extreme. If we cube, we instead end up with $3.4^ 3 + .24^3 = 39.3$, and $2.6^3 + 2^3 = 25.5$, which correctly identifies the extremist.\n",
    "\n",
    "Here's how the first few ratings would look:\n",
    "\n",
    "```python\n",
    "[\n",
    "       [ 3.12141628,  1.3134775 ], # 32.67\n",
    "       [ 2.6146248 ,  2.05339992], # 26.5\n",
    "       [ 0.33960656,  3.41651746], # 39.9\n",
    "       [ 3.42004795,  0.24198446], # 40\n",
    "       ...\n",
    "   ]\n",
    "```\n",
    "\n",
    "We can cube every value in **senator_distances** by typing $senator\\_distances^3$. To find the sum across every row, we'll need to use the NumPy [sum()](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.sum.html) method, and pass in the keyword argument axis=1.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "\n",
    "- Compute an extremism rating by cubing every value in **senator_distances**, then finding the **sum** across each row. Assign the result to **extremism.**\n",
    "- Assign the **extremism** variable to the **extremism** column of **votes**.\n",
    "- Sort **votes** on the **extremism** column, in **descending** order, using the **sort_values()** method on DataFrames.\n",
    "- Print the top 10 most extreme Senators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3XpghOTDWSqq"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CjKZg-wSKzR"
   },
   "source": [
    "## Next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oX7KZ6ksWat8"
   },
   "source": [
    "\n",
    "\n",
    "**Clustering is a powerful way to explore data and find patterns.** Unsupervised learning is very commonly used with large datasets where it isn't obvious how to start with supervised machine learning. In general, it's a good idea to try unsupervised learning to explore a dataset before trying to use supervised learning machine learning models.\n",
    "\n",
    "In the next section, we'll dive more into the **k-means** clustering algorithm and build our own from the ground up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoSlXrhaSKzS"
   },
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3M_CM24KXvgs"
   },
   "source": [
    "## Clustering NBA Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "an7qxkSU1kKq"
   },
   "source": [
    "In NBA media coverage, sports reporters usually focus on a handful of players and paint stories of how unique these players' stats are. With our data science hats on, we can't help but feel a slight sense of skepticism to how different the players are from one another. Let's see how we can use data science to explore that thread further.\n",
    "\n",
    "Let's look at the dataset of player performance from the 2013-2014 season.\n",
    "\n",
    "Here are some selected columns:\n",
    "\n",
    "- **player** -- name of the player\n",
    "- **pos** -- the position of the player\n",
    "- **g** -- number of games the player was in\n",
    "- **pts** -- total points the player scored\n",
    "- **fg.** -- field goal percentage\n",
    "- **ft.** -- free throw percentage\n",
    "\n",
    "Check out [Database Basketball](https://www.rotowire.com/basketball/stats.php?season=2013) for an explanation of all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dK9ilOvB2qme"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualize more columns\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "nba = pd.read_csv(\"nba_2013.csv\")\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NrD8Qlk21RI"
   },
   "source": [
    "## Point Guards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18gPJBsA4OOK"
   },
   "source": [
    "\n",
    "**Point guards** play one of the most crucial roles on a team because their **primary responsibility is to create scoring opportunities** for the team. We are going to focus our lesson on a machine learning technique called **clustering**, which allows us to visualize the types of point guards as well as group similar point guards together. **Using 2 features** allows us to easily visualize the players and will also make it easier to grasp how clustering works. For point guards, it's widely accepted that the **Assist to Turnover Ratio** is a good indicator for performance in games as it quantifies the number of scoring opportunities that player created. Let's also use **Points Per Game**, since effective Point Guards not only set up scoring opportunities but also take a lot of the shots themselves\n",
    "\n",
    "\n",
    "<center><img width=\"300\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1TRx73TAAQozxnflLXPhWXuDbM-Q2KHh4\"></center>\n",
    "\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Create a new Dataframe which contains just the point guards from the data set.\n",
    "  - Point guards are specified as **PG** in the **pos** column.\n",
    "- Assign the filtered data frame to **point_guards**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6kWD5K24h7k"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eB6pdbp68qA"
   },
   "source": [
    "## Points Per Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRJf3LjI7RUl"
   },
   "source": [
    "While our dataset doesn't come with **Points Per Game** values, we can easily calculate those using each player's total points (**pts**) and the number of games (g) they played. Let's take advantage of pandas' ability to multiply and divide columns to create the Points Per Game **ppg** column by dividing the pts and g columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4J8zejW7bJC"
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "point_guards[\"ppg\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wu9Sy1wu8GRN"
   },
   "source": [
    "## Assist Turnover Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvi1SQlc-4Q-"
   },
   "source": [
    "Now let's create a column, **atr**, for the Assist Turnover Ratio, which is calculated by dividing total assists (**ast**) by total turnovers (**tov**):\n",
    "\n",
    "$$\n",
    "\\displaystyle atr = \\frac{ast}{tov}\n",
    "$$\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Drop the players who have 0 turnovers.\n",
    "  - Not only did these players only play a few games, making it hard to understand their true abilities, but we also cannot divide by 0 when we calculate **atr**.\n",
    "- Utilize the same division technique we used with Points Per Game to create the Assist Turnover Ratio (**atr**) column for **point_guards.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oLFqAtK_Hqm"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdpJoIdJBz6d"
   },
   "source": [
    "## Visualizing the Point Guards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWwlKKSjB1BC"
   },
   "source": [
    "Use **matplotlib** to create a **scatter** plot with Points Per Game (**ppg**) on the X axis and Assist Turnover Ratio (**atr**) on the Y axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyOeV0hVCAqK"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyjlESKgCVa-"
   },
   "source": [
    "## Clustering players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XGFwMaRDYAK"
   },
   "source": [
    "**There seem to be 5 general regions**, or clusters, that the point guards fall into (with a few outliers of course!). We can use a technique called clustering to segment all of the point guards into groups of alike players. While regression and other supervised machine learning techniques work well when we have a clear metric we want to optimize for and lots of pre-labelled data, we need to instead use **unsupervised machine learning techniques to explore the structure within a data set that doesn't have a clear value to optimize.**\n",
    "\n",
    "There are multiple ways of clustering data but here we will focus on **centroid based clustering** for this lesson. Centroid based clustering works well when the clusters resemble circles with centers (or centroids). **The centroid represent the arithmetic mean of all of the data points in that cluster.**\n",
    "\n",
    "**K-Means Clustering is a popular centroid-based clustering algorithm that we will use.** The K in K-Means refers to the number of clusters we want to segment our data into. The key part with K-Means (and most unsupervised machine learning techniques) is that we have to specify what k is. There are advantages and disadvantages to this, but one advantage is that we can pick the k that makes the most sense for our use case. We'll set k to 5 since we want K-Means to segment our data into 5 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXc6OZqyDivH"
   },
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUef5S05Ejnb"
   },
   "source": [
    "Setup K-Means is an iterative algorithm that switches between recalculating the centroid of each cluster and the players that belong to that cluster. **To start, select 5 players at random and assign their coordinates as the initial centroids of the just created clusters.**\n",
    "\n",
    "- **Step 1 (Assign Points to Clusters)** - for each player, calculate the Euclidean distance between that player's coordinates, or values for **atr & ppg**, and each of the centroids' coordinates. Assign the player to the cluster whose centroid is the closest to, or has the lowest Euclidean distance to, the player's values.\n",
    "\n",
    "- **Step 2 (Update New Centroids of the Clusters)** - for each cluster, compute the new centroid by calculating the arithmetic mean of all of the points (players) in that cluster. We calculate the arithmetic mean by taking the average of all of the X values (atr) and the average of all of the Y values (ppg) of the points in that cluster.\n",
    "\n",
    "- Iterate Repeat steps 1 & 2 until the clusters are no longer moving and have converged.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUyAW6uCE8_P"
   },
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "\n",
    "# Use numpy's random function to generate a list, \n",
    "# length: num_clusters, of indices\n",
    "random_initial_points = np.random.choice(point_guards.index, size=num_clusters)\n",
    "\n",
    "# Use the random indices to create the centroids\n",
    "centroids = point_guards.loc[random_initial_points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8OGcD4NJOpn"
   },
   "source": [
    "### Visualize the centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bgC5avuJjeJ"
   },
   "source": [
    "Let's plot the **centroids**, in addition to the **point_guards**, so we can see where the randomly chosen centroids started out.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Plot a scatter figure using **ppg** and **atr** from **point_guards**.\n",
    "- In the same figure,  create a scatter plot using **ppg** and **atr** from **centroids**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bWor8T_9WxF"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8B_EH3tJwFp"
   },
   "source": [
    "### Setup (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCXf2yv5LS5u"
   },
   "source": [
    "While the **centroids** dataframe object worked well for the initial centroids, where the centroids were just a subset of players, as we iterate the centroids' values will be coordinates that may not match another player's coordinates. Moving forward, **let's use a dictionary object instead to represent the centroids.**\n",
    "\n",
    "We will need a unique identifier, like **cluster_id**, to refer to each cluster's centroid and a list representation of the centroid's coordinates (or values for **ppg** and **atr**). Let's create a dictionary then with the following mapping:\n",
    "\n",
    "- key: **cluster_id** of that centroid's cluster\n",
    "- value: centroid's coordinates expressed as a list ( **ppg** value first, **atr** value second )\n",
    "\n",
    "To generate the **cluster_ids**, let's iterate through each centroid and assign an integer from 0 to k-1. For example, the first centroid will have a **cluster_id** of 0, while the second one will have a **cluster_id** of 1. We'll write a function, **centroids_to_dict**, that takes in the centroids data frame object, creates a **cluster_id** and converts the **ppg** and **atr** values for that centroid into a list of coordinates, and adds both the **cluster_id** and **coordinates_list** into the dictionary that's returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHFkeynmMb5R"
   },
   "outputs": [],
   "source": [
    "def centroids_to_dict(centroids):\n",
    "    dictionary = dict()\n",
    "    # iterating counter we use to generate a cluster_id\n",
    "    counter = 0\n",
    "\n",
    "    # iterate a pandas data frame row-wise using .iterrows()\n",
    "    for index, row in centroids.iterrows():\n",
    "        coordinates = [row['ppg'], row['atr']]\n",
    "        dictionary[counter] = coordinates\n",
    "        counter += 1\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "centroids_dict = centroids_to_dict(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgI1jC5XNOln"
   },
   "source": [
    "### Step 1 (Euclidean Distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbuxIJ7-NrG9"
   },
   "source": [
    "Before we can assign players to clusters, we need a way to compare the **ppg** and **atr** values of the players with each cluster's centroids. **Euclidean distance** is the most common technique used in data science for measuring distance between vectors and works extremely well in 2 and 3 dimensions. While in higher dimensions, Euclidean distance can be misleading, in 2 dimensions Euclidean distance is essentially the **Pythagorean theorem**. The formula is :\n",
    "\n",
    "\n",
    "$$\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+\\ldots+(q_n-p_n)^2}$$\n",
    "\n",
    "\n",
    "where q and p are the 2 vectors we are comparing. If q is **[5, 2]** and p is **[3, 1]**, the distance comes out to:\n",
    "\n",
    "$$\\sqrt{(5-3)^2+(2-1)^2} = \\sqrt{5} =2.24 $$\n",
    "\n",
    "Let's create the function **calculate_distance**, which takes in 2 lists (the player's values for **ppg** and **atr** and the centroid's values for **ppg** and **atr**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1540756850218,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "https://lh4.googleusercontent.com/-baHwkIBEacY/AAAAAAAAAAI/AAAAAAAAFo0/aWRaXNQgy7Q/s64/photo.jpg",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "2lM_-XQYOlkp",
    "outputId": "9bdd5993-32cc-4500-eee2-117aaf05f013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_distance(centroid, player_values):\n",
    "    root_distance = 0\n",
    "    \n",
    "    for x in range(0, len(centroid)):\n",
    "        difference = centroid[x] - player_values[x]\n",
    "        squared_difference = difference**2\n",
    "        root_distance += squared_difference\n",
    "\n",
    "    euclid_distance = math.sqrt(root_distance)\n",
    "    return euclid_distance\n",
    "\n",
    "q = [5, 2]\n",
    "p = [3,1]\n",
    "\n",
    "# Sqrt(5) = ~2.24\n",
    "print(calculate_distance(q, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxzIeLwxPs_a"
   },
   "source": [
    "### Step 1 (Continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdfBaA5nRRpx"
   },
   "source": [
    "Now we need a way to assign data points to clusters based on Euclidean distance. Instead of creating a new variable or data structure to house the clusters, let's keep things simple and just add a column to the **point_guards** data frame that contains the **cluster_id** of the cluster it belongs to.\n",
    "\n",
    "Note: Even though we don't seed the random numbers to generate the centroids, the answer is seeded and will produce the same results everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1W5sc2PSRljg"
   },
   "outputs": [],
   "source": [
    "def assign_to_cluster(row):\n",
    "    lowest_distance = -1\n",
    "    closest_cluster = -1\n",
    "    \n",
    "    for cluster_id, centroid in centroids_dict.items():\n",
    "        df_row = [row['ppg'], row['atr']]\n",
    "        euclidean_distance = calculate_distance(centroid, df_row)\n",
    "        \n",
    "        if lowest_distance == -1:\n",
    "            lowest_distance = euclidean_distance\n",
    "            closest_cluster = cluster_id \n",
    "        elif euclidean_distance < lowest_distance:\n",
    "            lowest_distance = euclidean_distance\n",
    "            closest_cluster = cluster_id\n",
    "    return closest_cluster\n",
    "\n",
    "point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vk6vB8ziSkad"
   },
   "source": [
    "### Visualizing Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwBF_YYaS2OW"
   },
   "source": [
    "Let's write a function, **visualize_clusters**, that we can use to visualize the clusters easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CH3Vck_CS86G"
   },
   "outputs": [],
   "source": [
    "# Visualizing clusters\n",
    "def visualize_clusters(df, num_clusters):\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    for n in range(num_clusters):\n",
    "        clustered_df = df[df['cluster'] == n]\n",
    "        plt.scatter(clustered_df['ppg'], clustered_df['atr'], c=colors[n-1])\n",
    "        plt.xlabel('Points Per Game', fontsize=13)\n",
    "        plt.ylabel('Assist Turnover Ratio', fontsize=13)\n",
    "    plt.show()\n",
    "\n",
    "visualize_clusters(point_guards, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8nTN2N5TJ6d"
   },
   "source": [
    "### Step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrcrtlqpTsw6"
   },
   "source": [
    "How colorful! Now let's code Step 2, where we recalculate the centroids for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkSorXa0T69o"
   },
   "outputs": [],
   "source": [
    "def recalculate_centroids(df):\n",
    "    new_centroids_dict = dict()\n",
    "    \n",
    "    for cluster_id in range(0, num_clusters):\n",
    "        values_in_cluster = df[df['cluster'] == cluster_id]\n",
    "        # Calculate new centroid using mean of values in the cluster\n",
    "        new_centroid = [np.average(values_in_cluster['ppg']), \n",
    "                        np.average(values_in_cluster['atr'])]\n",
    "        new_centroids_dict[cluster_id] = new_centroid\n",
    "    return new_centroids_dict\n",
    "\n",
    "centroids_dict = recalculate_centroids(point_guards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnTDro0uUU7r"
   },
   "source": [
    "### Repeat Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNx1pxJCUdGm"
   },
   "source": [
    "Now that we recalculated the centroids, let's re-run Step 1 (**assign_to_cluster**) and see how the clusters shifted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynRCOD5AUwXL"
   },
   "outputs": [],
   "source": [
    "point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)\n",
    "visualize_clusters(point_guards, num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C81h3C8_U-xT"
   },
   "source": [
    "### Repeat Step 2 and Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f24o4ebmVKrn"
   },
   "source": [
    "Now we need to recalculate the centroids, and shift the clusters again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XboNtKTUVPJS"
   },
   "outputs": [],
   "source": [
    "centroids_dict = recalculate_centroids(point_guards)\n",
    "point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)\n",
    "visualize_clusters(point_guards, num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5ieJI0ZVT9M"
   },
   "source": [
    "## Challenges of K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Em_UqnlVe8c"
   },
   "source": [
    "As you repeat Steps 1 and 2 and run **visualize_clusters**, you'll notice that a few of the points are changing clusters between every iteration (especially in areas where 2 clusters almost overlap), but otherwise, the clusters visually look like they don't move a lot after every iteration. This means 2 things:\n",
    "\n",
    "- K-Means doesn't cause massive changes in the makeup of clusters between iterations, meaning that it will always converge and become stable\n",
    "- Because K-Means is conservative between iterations, where we pick the initial centroids and how we assign the players to clusters initially matters a lot\n",
    "\n",
    "To counteract these problems, the **sklearn** implementation of K-Means does some intelligent things like re-running the entire clustering process lots of times with random initial centroids so the final results are a little less biased on one passthrough's initial centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7gw4TYDV03u"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(point_guards[['ppg', 'atr']])\n",
    "point_guards['cluster'] = kmeans.labels_\n",
    "\n",
    "visualize_clusters(point_guards, num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptRHMvHrWfGz"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgpOK51SYn6G"
   },
   "source": [
    "In this lesson, we explored how to segment NBA players into groups with similar traits. Our exploration helped us get a sense of the 5 types of point guards as based on each player's Assist Turnover Ratio and Points Per Game. In future challenges, you can explore how to cluster using many more features, as well as alternative ways of clustering data without using centroids.\n",
    "\n",
    "We also got to experience the beauty of sklearn and the simplicity of using sophisticated models to get what we need accomplished quickly. In just a few lines, we ran and visualized the results of a robust K-means clustering implementation. When using K-means in the real world, use the sklearn implementation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson 08 -  Clustering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
