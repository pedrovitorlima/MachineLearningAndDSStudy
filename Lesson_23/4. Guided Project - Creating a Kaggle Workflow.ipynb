{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iapz_gk4bL4Q"
   },
   "source": [
    "# Introducing data science workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MG9jSDw-wVhY"
   },
   "source": [
    "\n",
    "\n",
    "In this guided project, we're going to put together all that we've learned in this course and create a data science workflow.\n",
    "\n",
    "By defining a workflow for yourself, you can give yourself a framework with which to make iterating on ideas quicker and easier, allowing yourself to work more efficiently.\n",
    "\n",
    "In this mission, we're going to explore a workflow to make competing in the Kaggle Titanic competition easier, using a pipeline of functions to reduce the number of dimensions you need to focus on.\n",
    "\n",
    "To get started, we'll read in the original **train.csv** and **test.csv** files from Kaggle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWquy7FRbL4R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "holdout = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPE04u4RbL4V"
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P92sW6bzbL4a"
   },
   "outputs": [],
   "source": [
    "holdout.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpDHDy1PbL4e"
   },
   "outputs": [],
   "source": [
    "survived = train[\"Survived\"]\n",
    "train = train.drop(\"Survived\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXGN8riJbL4h"
   },
   "outputs": [],
   "source": [
    "holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBcr5bYhbL4j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKIcnJyhbL4n"
   },
   "outputs": [],
   "source": [
    "## concatenate all data to guarantee that dataset have the same columns\n",
    "all_data = pd.concat([train,holdout],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGbcWYNjbL4p"
   },
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPZr6A-JbL4s"
   },
   "source": [
    "# Exploring the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwwXZJhMwsER"
   },
   "source": [
    "\n",
    "\n",
    "In the first three missions of this course, we have done a variety of activities, mostly in isolation: **Exploring the data**, **creating features**, **selecting features**, **selecting and tuning different models**.\n",
    "\n",
    "The Kaggle workflow we are going to build will combine all of these into a process.\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1swb6PxXUJuDvv83ylqh9eUh992lXTu47\">\n",
    "\n",
    "- **Data exploration**, to find patterns in the data\n",
    "- **Feature engineering**, to create new features from those patterns or through pure experimentation\n",
    "- **Feature selection**, to select the best subset of our current set of features\n",
    "- **Model selection/tuning**, training a number of models with different hyperparameters to find the best performer.\n",
    "\n",
    "We can continue to repeat this cycle as we work to optimize our predictions. At the end of any cycle we wish, we can also use our model to make predictions on the holdout set and then **Submit to Kaggle** to get a leaderboard score.\n",
    "\n",
    "While the first two steps of our workflow are relatively freeform, later in this project we'll create some functions that will help automate the complexity of the latter two steps so we can move faster.\n",
    "\n",
    "For now, let's practice the first stage, exploring the data. We're going to examine the two columns that contain information about the family members each passenger had onboard: **SibSp** and **Parch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqMGzwrQbL4t"
   },
   "source": [
    "# Preprocesing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VddvfkrFbL4u"
   },
   "outputs": [],
   "source": [
    "def process_ticket(df):\n",
    "    # see https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n",
    "    Ticket = []\n",
    "    for i in list(df.Ticket):\n",
    "        if not i.isdigit():\n",
    "            #Take prefix\n",
    "            Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) \n",
    "        else:\n",
    "            Ticket.append(\"X\")\n",
    "    df[\"Ticket\"] = Ticket\n",
    "    return df\n",
    "\n",
    "def process_missing(df):\n",
    "    \"\"\"Handle various missing values from the data set\n",
    "\n",
    "    Usage\n",
    "    ------\n",
    "\n",
    "    holdout = process_missing(holdout)\n",
    "    \"\"\"\n",
    "    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")\n",
    "    return df\n",
    "\n",
    "def process_age(df):\n",
    "    \"\"\"Process the Age column into pre-defined 'bins' \n",
    "\n",
    "    Usage\n",
    "    ------\n",
    "\n",
    "    train = process_age(train)\n",
    "    \"\"\"\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n",
    "    cut_points = [-1,0,5,12,18,35,60,100]\n",
    "    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n",
    "    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
    "    \n",
    "    #df = df.drop(\"Age\",axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_fare(df):\n",
    "    \"\"\"Process the Fare column into pre-defined 'bins' \n",
    "\n",
    "    Usage\n",
    "    ------\n",
    "\n",
    "    train = process_fare(train)\n",
    "    \"\"\"\n",
    "    cut_points = [-1,12,50,100,1000]\n",
    "    label_names = [\"0-12\",\"12-50\",\"50-100\",\"100+\"]\n",
    "    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n",
    "    \n",
    "    df = df.drop(\"Fare\",axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_cabin(df):\n",
    "    \"\"\"Process the Cabin column into pre-defined 'bins' \n",
    "\n",
    "    Usage\n",
    "    ------\n",
    "\n",
    "    train process_cabin(train)\n",
    "    \"\"\"\n",
    "    df[\"Cabin_type\"] = df[\"Cabin\"].str[0]\n",
    "    df[\"Cabin_type\"] = df[\"Cabin_type\"].fillna(\"Unknown\")\n",
    "    df = df.drop('Cabin',axis=1)\n",
    "    return df\n",
    "\n",
    "def process_titles(df):\n",
    "    \"\"\"Extract and categorize the title from the name column \n",
    "\n",
    "    Usage\n",
    "    ------\n",
    "\n",
    "    train = process_titles(train)\n",
    "    \"\"\"\n",
    "    titles = {\n",
    "        \"Mr\" :         \"Mr\",\n",
    "        \"Mme\":         \"Mrs\",\n",
    "        \"Ms\":          \"Mrs\",\n",
    "        \"Mrs\" :        \"Mrs\",\n",
    "        \"Master\" :     \"Master\",\n",
    "        \"Mlle\":        \"Miss\",\n",
    "        \"Miss\" :       \"Miss\",\n",
    "        \"Capt\":        \"Officer\",\n",
    "        \"Col\":         \"Officer\",\n",
    "        \"Major\":       \"Officer\",\n",
    "        \"Dr\":          \"Officer\",\n",
    "        \"Rev\":         \"Officer\",\n",
    "        \"Jonkheer\":    \"Royalty\",\n",
    "        \"Don\":         \"Royalty\",\n",
    "        \"Sir\" :        \"Royalty\",\n",
    "        \"Countess\":    \"Royalty\",\n",
    "        \"Dona\":        \"Royalty\",\n",
    "        \"Lady\" :       \"Royalty\"\n",
    "    }\n",
    "    extracted_titles = df[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\n",
    "    df[\"Title\"] = extracted_titles.map(titles)\n",
    "    return df\n",
    "\n",
    "def create_dummies(df,column_name):\n",
    "    \"\"\"Create Dummy Columns (One Hot Encoding) from a single Column\n",
    "\n",
    "    Usage\n",
    "    ------\n",
    "\n",
    "    train = create_dummies(train,\"Age\")\n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9jIVSrSbL4w"
   },
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    df = process_ticket(df)\n",
    "    df = process_missing(df)\n",
    "    df = process_age(df)\n",
    "    df = process_fare(df)\n",
    "    df = process_titles(df)\n",
    "    df = process_cabin(df)\n",
    "\n",
    "    for col in [\"Age_categories\",\"Fare_categories\",\n",
    "                \"Title\",\"Cabin_type\",\"Sex\",\"Ticket\",\"Pclass\"]:\n",
    "        df = create_dummies(df,col)\n",
    "    \n",
    "    #df = df.drop([\"Age_categories\",\"Fare_categories\",\n",
    "                #\"Title\",\"Cabin_type\",\"Sex\",\"Ticket\"],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "all_data = pre_process(all_data)\n",
    "\n",
    "train = all_data.iloc[:891]\n",
    "train = pd.concat([train,survived],axis=1)\n",
    "holdout = all_data.iloc[891:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvTOyioebL4z"
   },
   "source": [
    "# Exploring Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlNqa8AmbL4z"
   },
   "outputs": [],
   "source": [
    "explore_cols = [\"SibSp\",\"Parch\",\"Survived\"]\n",
    "explore = train[explore_cols].copy()\n",
    "explore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kekhl5qbL44"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "explore.drop(\"Survived\",axis=1).plot.hist(alpha=0.5,bins=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75z57Jx_bL47"
   },
   "outputs": [],
   "source": [
    "explore[\"familysize\"] = explore[[\"SibSp\",\"Parch\"]].sum(axis=1)\n",
    "explore.drop(\"Survived\",axis=1).plot.hist(alpha=0.5,bins=10)\n",
    "plt.xticks(range(11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrMEhJ2GbL5A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.clf()\n",
    "for col in explore.columns.drop(\"Survived\"):\n",
    "    pivot = explore.pivot_table(index=col,values=\"Survived\")\n",
    "    pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HdONhP9bL5E"
   },
   "source": [
    "The SibSp column shows the number of siblings and/or spouses each passenger had on board, while the Parch columns shows the number of parents or children each passenger had onboard. Neither column has any missing values.\n",
    "\n",
    "The distribution of values in both columns is skewed right, with the majority of values being zero.\n",
    "\n",
    "You can sum these two columns to explore the total number of family members each passenger had onboard. The shape of the distribution of values in this case is similar, however there are less values at zero, and the quantity tapers off less rapidly as the values increase.\n",
    "\n",
    "Looking at the survival rates of the the combined family members, you can see that few of the over 500 passengers with no family members survived, while greater numbers of passengers with family members survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAn_tn1bbL5G"
   },
   "source": [
    "# Engineering New Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9R33kK4bL5H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_isalone(df):\n",
    "    df[\"familysize\"] = df[[\"SibSp\",\"Parch\"]].sum(axis=1)\n",
    "    df[\"isalone\"] = 0\n",
    "    df.loc[(df[\"familysize\"] == 0),\"isalone\"] = 1\n",
    "    #df = df.drop(\"familysize\",axis=1)\n",
    "    return df\n",
    "\n",
    "train = process_isalone(train)\n",
    "holdout = process_isalone(holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2OtjzNRbbL5K"
   },
   "source": [
    "# Selecting the Best-Performing Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-2kSrkUbL5L"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def select_features(df,index):\n",
    "    \n",
    "    # index\n",
    "    # 0 - random forest\n",
    "    # 1 - logistic regression\n",
    "    \n",
    "    # Remove non-numeric columns, columns that have null values\n",
    "    df = df.select_dtypes([np.number]).dropna(axis=1)\n",
    "    all_X = df.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
    "    all_y = df[\"Survived\"]\n",
    "    \n",
    "    clf_rf = RandomForestClassifier(random_state=1, n_estimators=100)\n",
    "    clf_lr = LogisticRegression()\n",
    "    clfs = [clf_rf,clf_lr]\n",
    "    \n",
    "    selector = RFECV(clfs[index],cv=10,n_jobs=-1)\n",
    "    selector.fit(all_X,all_y)\n",
    "    \n",
    "    best_columns = list(all_X.columns[selector.support_])\n",
    "    print(\"Best Columns \\n\"+\"-\"*12+\"\\n{}\\n\".format(best_columns))\n",
    "    \n",
    "    return best_columns\n",
    "\n",
    "cols_rf = select_features(train,0)\n",
    "cols_lr = select_features(train,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqooMKXdbL5N"
   },
   "source": [
    "# Selecting and Tuning Different Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8uxMv_UxbL5O"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "\n",
    "def select_model(df,features):\n",
    "    \n",
    "    all_X = df[features]\n",
    "    all_y = df[\"Survived\"]\n",
    "\n",
    "    # List of dictionaries, each containing a model name,\n",
    "    # it's estimator and a dict of hyperparameters\n",
    "    models = [\n",
    "        {\n",
    "            \"name\": \"LogisticRegression\",\n",
    "            \"estimator\": LogisticRegression(),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n",
    "                }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"KNeighborsClassifier\",\n",
    "            \"estimator\": KNeighborsClassifier(),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"n_neighbors\": range(1,20,2),\n",
    "                    \"weights\": [\"distance\", \"uniform\"],\n",
    "                    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "                    \"p\": [1,2]\n",
    "                }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RandomForestClassifier\",\n",
    "            \"estimator\": RandomForestClassifier(random_state=1),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"n_estimators\": [200],\n",
    "                    \"criterion\": [\"entropy\", \"gini\"],\n",
    "                    \"max_depth\": [10,20],\n",
    "                    \"max_features\": [\"log2\", \"sqrt\"],\n",
    "                    \"min_samples_leaf\": [1],\n",
    "                    \"min_samples_split\": [2]\n",
    "                }\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"SVC\",\n",
    "            \"estimator\":SVC(),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                  \"kernel\": ['rbf'],  \n",
    "                  \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "                  \"gamma\": [0.001, 0.01, 0.1, 1]\n",
    "                }\n",
    "        },\n",
    "        {\n",
    "            # reference\n",
    "            # https://github.com/UltravioletAnalytics/kaggle-titanic/blob/master/sgdclassifier.py\n",
    "            \"name\":\"SGDC\",\n",
    "            \"estimator\": SGDClassifier(),\n",
    "            \"hyperparameters\":\n",
    "            {\n",
    "                \"loss\": [\"log\"],\n",
    "                \"alpha\": [0.001],\n",
    "                \"penalty\": [\"elasticnet\"],\n",
    "                \"l1_ratio\": [0.8],\n",
    "                \"shuffle\": [True],\n",
    "                \"learning_rate\": ['optimal'],\n",
    "                \"max_iter\":[1000]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for model in models:\n",
    "        print(model['name'])\n",
    "        print('-'*len(model['name']))\n",
    "\n",
    "        grid = GridSearchCV(model[\"estimator\"],\n",
    "                            param_grid=model[\"hyperparameters\"],\n",
    "                            cv=10)\n",
    "        grid.fit(all_X,all_y)\n",
    "        model[\"best_params\"] = grid.best_params_\n",
    "        model[\"best_score\"] = grid.best_score_\n",
    "        model[\"best_model\"] = grid.best_estimator_\n",
    "\n",
    "        print(\"Best Score: {}\".format(model[\"best_score\"]))\n",
    "        print(\"Best Parameters: {}\\n\".format(model[\"best_params\"]))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LeDDIj4TbL5Q",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_a = select_model(train,cols_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-m8KwVTWbL5T"
   },
   "outputs": [],
   "source": [
    "result_b = select_model(train,cols_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0L9BQt8zbL5W"
   },
   "source": [
    "# Making a Submission to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgvGjgR1bL5X"
   },
   "outputs": [],
   "source": [
    "def save_submission_file(model,cols,filename):\n",
    "    holdout_data = holdout[cols]\n",
    "    predictions = model.predict(holdout_data)\n",
    "    \n",
    "    holdout_ids = holdout[\"PassengerId\"]\n",
    "    submission_df = {\"PassengerId\": holdout_ids,\n",
    "                 \"Survived\": predictions}\n",
    "    submission = pd.DataFrame(submission_df)\n",
    "\n",
    "    submission.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbc3u4DlbL5Z"
   },
   "outputs": [],
   "source": [
    "best_rf_model = result_b[3][\"best_model\"]\n",
    "save_submission_file(best_rf_model,cols_lr,\"submission_22.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yum7k-wwbL5b"
   },
   "source": [
    "#  Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pNKdURxyJpK"
   },
   "source": [
    "\n",
    "\n",
    "We encourage you to continue working on this Kaggle competition. Here are some suggestions of next steps:\n",
    "\n",
    "- Continue to explore the data and create new features, following the workflow and using the functions we created.\n",
    "- Read more about the titanic and this Kaggle competition to get ideas for new features.\n",
    "- Use some different algorithms in the select_model() function, like [stochastic gradient descent](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) or [perceptron linear models](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html).\n",
    "- Experiment with [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) instead of **GridSearchCV** to speed up your **select_features()** function.\n",
    "\n",
    "Lastly, while the Titanic competition is great for learning about how to approach your first Kaggle competition, we recommend against spending many hours focused on trying to get to the top of the leaderboard. With such a small data set, there is a limit to how good your predictions can be, and your time would be better spent moving onto more complex competitions.\n",
    "\n",
    "Once you feel like you have a good understanding of the Kaggle workflow, you should look at some other competitions - a great next competition is the [House Prices Competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). A start point you can find [here](https://www.dataquest.io/blog/kaggle-getting-started/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4. Guided Project - Creating a Kaggle Workflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
