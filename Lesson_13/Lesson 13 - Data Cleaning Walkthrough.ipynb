{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Data Cleaning Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At many points in your career, you'll need to be able to build complete, end-to-end data science projects on your own. **Data science projects usually consist of one of two things:**\n",
    "\n",
    "- An exploration and analysis of a set of data. One example might involve analyzing donors to political campaigns, creating a plot, and then sharing an analysis of the plot with others.\n",
    "- An operational system that generates predictions based on data that updates continually. An algorithm that pulls in daily stock ticker data and predicts which stock prices will rise and fall would be one example.\n",
    "\n",
    "You'll find the ability to create data science projects useful in several different contexts:\n",
    "\n",
    "- Projects will help you build a portfolio, which is critical to finding a job as a data analyst or scientist.\n",
    "- Working on projects will help you learn new skills and reinforce existing concepts.\n",
    "- Most \"real-world\" data science and analysis work consists of developing internal projects.\n",
    "- Projects allow you to investigate interesting phenomena and satisfy your curiosity.\n",
    "\n",
    "Whether you aim to become a data scientist or analyst or you're just curious about the world, building projects can be immensely rewarding.\n",
    "\n",
    "**In this section, we'll walk through the first part of a complete data science project, including how to acquire the raw data.** The project will focus on exploring and analyzing a data set. We'll develop our **data cleaning** and **storytelling skills**, which will enable us to build complete projects on our own.\n",
    "\n",
    "We'll focus **primarily on data exploration** in this section. We'll also combine several messy data sets into a single clean one to make analysis easier. Over the next few missions, we'll work through the rest of our project and perform the actual analysis.\n",
    "\n",
    "**The first step in creating a project is to decide on a topic**. You want the topic to be something you're interested in and motivated to explore. It's very obvious when people are making projects just to make them, rather than out of a genuine interest in the topic.\n",
    "\n",
    "Here are two ways to go about finding a good topic:\n",
    "\n",
    "- Think about what sectors or angles you're really interested in, then find data sets relating to those sectors.\n",
    "- Review several data sets, and find one that seems interesting enough to explore.\n",
    "\n",
    "Whichever approach you take, you can start your search at these sites:\n",
    "\n",
    "- [Data.gov](https://www.data.gov/) - A directory of government data downloads\n",
    "- [/r/datasets](https://reddit.com/r/datasets) - A subreddit that has hundreds of interesting data sets\n",
    "- [Awesome datasets](https://github.com/caesar0301/awesome-public-datasets) - A list of data sets hosted on GitHub\n",
    "- [rs.io](http://rs.io/100-interesting-data-sets-for-statistics/) - A great blog post with hundreds of interesting data sets\n",
    "In real-world data science, you may not find an ideal data set. You might have to aggregate disparate data sources instead, or do a good amount of data cleaning.\n",
    "\n",
    "For the purposes of this project, we'll be using data about New York City public schools, which can be found [here](https://data.cityofnewyork.us/browse?category=Education)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding All of the Relevant Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've chosen a topic, you'll want to pick an angle to investigate. It's important to choose an angle that has enough depth to analyze, but isn't so complicated that it's difficult to get started. You want to finish the project, and you want your results to be interesting to others.\n",
    "\n",
    "One of the most controversial issues in the U.S. educational system is the efficacy of standardized tests, and whether they're unfair to certain groups. Given our prior knowledge of this topic, investigating the correlations between [SAT scores](https://en.wikipedia.org/wiki/SAT) and demographics might be an interesting angle to take. We could correlate SAT scores with factors like race, gender, income, and more.\n",
    "\n",
    "The SAT, or Scholastic Aptitude Test, is an exam that U.S. high school students take before applying to college. Colleges take the test scores into account when deciding who to admit, so it's fairly important to perform well on it.\n",
    "\n",
    "The test consists of three sections, each of which has 800 possible points. The combined score is out of 2,400 possible points (while this number has changed a few times, the data set for our project is based on 2,400 total points). **Organizations often rank high schools by their average SAT scores.** The scores are also considered a measure of overall school district quality.\n",
    "\n",
    "New York City makes its data on [high school SAT scores](https://data.cityofnewyork.us/Education/SAT-Results/f9bf-2cp4) available online, as well as the [demographics for each high school](https://data.cityofnewyork.us/Education/DOE-High-School-Directory-2014-2015/n3p6-zve2). The first few rows of the SAT data look like this:\n",
    "\n",
    "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1_1f_s0jbjSH1YeWs-W3Ioy8mqDqaYYNb\"></left>\n",
    "\n",
    "Unfortunately, combining both of the data sets won't give us all of the demographic information we want to use. We'll need to supplement our data with other sources to do our full analysis.\n",
    "\n",
    "The same website has several related data sets covering demographic information and test scores. Here are the links to all of the data sets we'll be using:\n",
    "\n",
    "- [SAT scores by school](https://data.cityofnewyork.us/Education/SAT-Results/f9bf-2cp4) - SAT scores for each high school in New York City\n",
    "- [School attendance](https://data.cityofnewyork.us/Education/School-Attendance-and-Enrollment-Statistics-by-Dis/7z8d-msnt) - Attendance information for each school in New York City\n",
    "- [Class size](https://data.cityofnewyork.us/Education/2010-2011-Class-Size-School-level-detail/urz7-pzb3) - Information on class size for each school\n",
    "- [AP test results](https://data.cityofnewyork.us/Education/AP-College-Board-2010-School-Level-Results/itfs-ms3e) - Advanced Placement (AP) exam results for each high school (passing an optional AP exam in a particular subject can earn a student college credit in that subject)\n",
    "- [Graduation outcomes](https://data.cityofnewyork.us/Education/Graduation-Outcomes-Classes-Of-2005-2010-School-Le/vh2h-md7a) - The percentage of students who graduated, and other outcome information\n",
    "- [Demographics](https://data.cityofnewyork.us/Education/School-Demographics-and-Accountability-Snapshot-20/ihfw-zy9j) - Demographic information for each school\n",
    "- [School survey](https://data.cityofnewyork.us/Education/NYC-School-Survey-2011/mnz3-dyi8) - Surveys of parents, teachers, and students at each school\n",
    "All of these data sets are interrelated. We'll need to combine them into a single data set before we can find correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Background Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move into coding, we'll need to do some background research. A thorough understanding of the data will help us avoid costly mistakes, such as thinking that a column represents something other than what it does. Background research will also give us a better understanding of how to combine and analyze the data.\n",
    "\n",
    "In this case, we'll want to research:\n",
    "\n",
    "- [New York City](https://en.wikipedia.org/wiki/New_York_City)\n",
    "- [The SAT](https://en.wikipedia.org/wiki/SAT)\n",
    "- [Schools in New York City](https://en.wikipedia.org/wiki/List_of_high_schools_in_New_York_City)\n",
    "- [Our data](https://data.cityofnewyork.us/browse?category=Education)\n",
    "\n",
    "We can learn a few different things from these resources. For example:\n",
    "\n",
    "- Only high school students take the SAT, so we'll want to focus on high schools.\n",
    "- New York City is made up of five boroughs, which are essentially distinct regions.\n",
    "- New York City schools fall within several different school districts, each of which can contains dozens of schools.\n",
    "- Our data sets include several different types of schools. We'll need to clean them so that we can focus on high schools only.\n",
    "- Each school in New York City has a unique code called a **DBN**, or district borough number.\n",
    "- Aggregating data by district will allow us to use the district mapping data to plot district-by-district differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done our background research, we're ready to read in the data. For your convenience, we've placed all the data into the schools folder. Here are all of the files in the folder:\n",
    "\n",
    "- **ap_2010.csv** - [Data on AP test results](https://data.cityofnewyork.us/Education/AP-College-Board-2010-School-Level-Results/itfs-ms3e)\n",
    "- **class_size.csv** - Data on [class size](https://data.cityofnewyork.us/Education/2010-2011-Class-Size-School-level-detail/urz7-pzb3)\n",
    "- **demographics.csv** - Data on [demographics](https://data.cityofnewyork.us/Education/School-Demographics-and-Accountability-Snapshot-20/ihfw-zy9j)\n",
    "- **graduation.csv** - Data on [graduation outcomes](https://data.cityofnewyork.us/Education/Graduation-Outcomes-Classes-Of-2005-2010-School-Le/vh2h-md7a)\n",
    "- **hs_directory.csv** - [A directory of high schools](https://data.cityofnewyork.us/Education/DOE-High-School-Directory-2014-2015/n3p6-zve2)\n",
    "- **sat_results.csv** - Data on [SAT scores](https://data.cityofnewyork.us/Education/SAT-Results/f9bf-2cp4)\n",
    "- **survey_all.txt** - Data on [surveys](https://data.cityofnewyork.us/Education/NYC-School-Survey-2011/mnz3-dyi8) from all schools\n",
    "- **survey_d75.txt** - Data on surveys from New York City district 75\n",
    "\n",
    "**survey_all.txt** and **survey_d75.txt** are in more complicated formats than the other files. For now, we'll focus on reading in the - CSV files only, and then explore them.\n",
    "\n",
    "We'll read each file into a [pandas dataframe](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), and then store all of the dataframes in a dictionary. This will give us a convenient way to store them, and a quick way to reference them later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Read each of the files in the list **data_files** into a pandas dataframe using the **pandas.read_csv()** function.\n",
    "- Recall that all of the data sets are in the **datasets** folder. That means the path to **ap_2010.csv** is **datasets/ap_2010.csv**.\n",
    "- Add each of the dataframes to the **dictionary data**, using the base of **filename as the key**. For example, you'd enter **ap_2010** for the file **ap_2010.csv.**\n",
    "- Afterwards, **data** should have the following keys:\n",
    "    - **ap_2010**\n",
    "    - **class_size**\n",
    "    - **demographics**\n",
    "    - **graduation**\n",
    "    - **hs_directory**\n",
    "    - **sat_results**\n",
    "- In addition, each key in **data** should have the corresponding dataframe as its value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "import pandas as pd\n",
    "data_files = [\n",
    "    \"ap_2010.csv\",\n",
    "    \"class_size.csv\",\n",
    "    \"demographics.csv\",\n",
    "    \"graduation.csv\",\n",
    "    \"hs_directory.csv\",\n",
    "    \"sat_results.csv\"\n",
    "]\n",
    "data = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the SAT Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're mainly interested in is the SAT data set, which corresponds to the dictionary key **sat_results**. This data set contains the **SAT scores** for each high school in New York City. We eventually want to correlate selected information from this data set with information in the other data sets.\n",
    "\n",
    "Let's explore **sat_results** to see what we can discover. Exploring the dataframe will help us understand the structure of the data, and make it easier for us to analyze it.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Display the first five rows of the SAT scores data.\n",
    "    - Use the key **sat_results** to access the SAT scores dataframe stored in the dictionary **data.**\n",
    "    - Use the **pandas.DataFrame.head()** method along with the **print()** function to display the first five rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Remaining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we printed the first five rows of the SAT data, the output looked like this:\n",
    "\n",
    "```python\n",
    "DBN                                    SCHOOL NAME  \\\n",
    "0  01M292  HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES\n",
    "1  01M448            UNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n",
    "2  01M450                     EAST SIDE COMMUNITY SCHOOL\n",
    "3  01M458                      FORSYTH SATELLITE ACADEMY\n",
    "4  01M509                        MARTA VALLE HIGH SCHOOL\n",
    "\n",
    "  Num of SAT Test Takers SAT Critical Reading Avg. Score SAT Math Avg. Score  \\\n",
    "0                     29                             355                 404\n",
    "1                     91                             383                 423\n",
    "2                     70                             377                 402\n",
    "3                      7                             414                 401\n",
    "4                     44                             390                 433\n",
    "\n",
    "  SAT Writing Avg. Score\n",
    "0                    363\n",
    "1                    366\n",
    "2                    370\n",
    "3                    359\n",
    "4                    384\n",
    "```\n",
    "\n",
    "We can make a few observations based on this output:\n",
    "\n",
    "- The **DBN** appears to be a unique ID for each school.\n",
    "- We can tell from the first few rows of names that we only have data about high schools.\n",
    "- There's only a single row for each high school, so each **DBN** is unique in the SAT data.\n",
    "- We may eventually want to combine the three columns that contain SAT scores -- **SAT Critical Reading Avg.**, **Score SAT Math Avg. Score**, and **SAT Writing Avg. Score** -- into a single column to make the scores easier to analyze.\n",
    "Given these observations, let's explore the other data sets to see if we can gain any insight into how to combine them.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Loop through each **key** in **data**. For each key:\n",
    "    - Display the first five rows of the dataframe associated with the **key**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can make some observations based on the first few rows of each one.\n",
    "\n",
    "- Each data set appears to either have a **DBN** column, or the information we need to create one. That means we can use a **DBN** column to combine the data sets. First we'll pinpoint matching rows from different data sets by looking for identical **DBNs**, then group all of their columns together in a single data set.\n",
    "- Some fields look interesting for mapping -- particularly **Location 1**, which contains coordinates inside a larger string.\n",
    "- Some of the data sets appear to contain multiple rows for each school (because the rows have duplicate **DBN** values). That means we’ll have to do some preprocessing to ensure that each **DBN** is unique within each data set. If we don't do this, we'll run into problems when we combine the data sets, because we might be merging two rows in one data set with one row in another data set.\n",
    "\n",
    "Before we proceed with the merge, we should make sure we have all of the data we want to unify. We mentioned the survey data earlier (**survey_all.txt** and **survey_d75.txt**), but we didn't read those files in because they're in a slightly more complex format.\n",
    "\n",
    "Each survey text file looks like this:\n",
    "\n",
    "```python\n",
    "dbn bn  schoolname  d75 studentssurveyed    highschool  schooltype  rr_s\n",
    "\"01M015\"    \"M015\"  \"P.S. 015 Roberto Clemente\" 0   \"No\"    0   \"Elementary School\"     88\n",
    "```\n",
    "\n",
    "The files are tab delimited and encoded with **Windows-1252** encoding. An encoding defines how a computer stores the contents of a file in binary. The most common encodings are **UTF-8** and **ASCII**. **Windows-1252** is rarely used, and can cause errors if we read such a file in without specifying the encoding. If you'd like to read more about encodings, [here's](http://kunststube.net/encoding/) a good primer.\n",
    "\n",
    "We'll need to specify the encoding and delimiter to the pandas [pandas.read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function to ensure it reads the surveys in properly.\n",
    "\n",
    "After we read in the survey data, we'll want to combine it into a single dataframe. We can do this by calling the [pandas.concat()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) function:\n",
    "\n",
    "```python\n",
    "z = pd.concat([x,y], axis=0)\n",
    "```\n",
    "\n",
    "The code above will combine dataframes x and y by essentially appending y to the end of x. The combined dataframe z will have the number of rows in x plus the number of rows in y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Read in **survey_all.txt.**\n",
    "    - Use the **pandas.read_csv()** function to read **survey_all.txt** into the variable **all_survey**. Recall that this file is located in the **datasets** folder.\n",
    "    - Specify the keyword argument **delimiter=\"\\t\".**\n",
    "    - Specify the keyword argument **encoding=\"windows-1252\".**\n",
    "- Read in **survey_d75.txt.**\n",
    "    - Use the **pandas.read_csv()** function to read **datasets/survey_d75.txt** into the variable **d75_survey**. Recall that this file is located in the **datasets** folder.\n",
    "    - Specify the keyword argument **delimiter=\"\\t\".**\n",
    "    - Specify the keyword argument **encoding=\"windows-1252\".**\n",
    "- Combine **d75_survey** and **all_survey** into a single dataframe.\n",
    "    - Use the pandas [concat()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) function with the keyword argument **axis=0** to combine **d75_survey** and **all_survey** into the dataframe **survey**.\n",
    "    - Pass in **all_survey** first, then **d75_survey** when calling the **pandas.concat()** function.\n",
    "- Display the first five rows of **survey** using the **pandas.DataFrame.head()** function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up the Surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step, the expected output was:\n",
    "\n",
    "```python\n",
    "    N_p  N_s  N_t  aca_p_11  aca_s_11  aca_t_11  aca_tot_11    bn  com_p_11  \\\n",
    "0   90  NaN   22       7.8       NaN       7.9         7.9  M015       7.6   \n",
    "1  161  NaN   34       7.8       NaN       9.1         8.4  M019       7.6\n",
    "```\n",
    "\n",
    "There are two immediate facts that we can see in the data:\n",
    "\n",
    "- There are over **2000** columns, nearly all of which we don't need. We'll have to filter the data to remove the unnecessary ones. Working with fewer columns will make it easier to print the dataframe out and find correlations within it.\n",
    "- The survey data has a **dbn** column that we'll want to convert to uppercase (**DBN**). The conversion will make the column name consistent with the other data sets.\n",
    "\n",
    "First, we'll need filter the columns to remove the ones we don't need. Luckily, there's a data dictionary at the [original data download](https://data.cityofnewyork.us/Education/NYC-School-Survey-2011/mnz3-dyi8) location. The dictionary tells us what each column represents. Based on our knowledge of the problem and the analysis we're trying to do, we can use the data dictionary to determine which columns to use.\n",
    "\n",
    "Here's a preview of the data dictionary:\n",
    "\n",
    "\n",
    "<left><img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=145GxwJiDOodRgyXfV15ybUBqfbDLFwEU\"></left>\n",
    "\n",
    "\n",
    "Based on the dictionary, it looks like these are the relevant columns:\n",
    "\n",
    "```python\n",
    "[\"dbn\", \"rr_s\", \"rr_t\", \"rr_p\", \"N_s\", \"N_t\", \"N_p\", \"saf_p_11\", \"com_p_11\", \n",
    " \"eng_p_11\", \"aca_p_11\", \"saf_t_11\", \"com_t_11\", \"eng_t_11\", \n",
    " \"aca_t_11\", \"saf_s_11\", \"com_s_11\", \"eng_s_11\", \"aca_s_11\",\n",
    " \"saf_tot_11\", \"com_tot_11\", \"eng_tot_11\", \"aca_tot_11\"]\n",
    "```\n",
    "\n",
    "These columns will give us aggregate survey data about how parents, teachers, and students feel about school safety, academic performance, and more. It will also give us the **DBN**, which allows us to uniquely identify the school.\n",
    "\n",
    "Before we filter columns out, we'll want to copy the data from the **dbn** column into a new column called **DBN**. We can copy columns like this:\n",
    "\n",
    "```python\n",
    "survey[\"new_column\"] = survey[\"old_column\"]\n",
    "```\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Copy the data from the **dbn** column of **survey** into a new column in **survey** called **DBN**.\n",
    "- Filter **survey** so it only contains the columns we listed above. You can do this using **pandas.DataFrame.loc[]**.\n",
    "    - Remember that we renamed **dbn** to **DBN**; be sure to change the list of columns we want to keep accordingly.\n",
    "- Assign the dataframe **survey** to the key **survey** in the dictionary **data**.\n",
    "- When you're finished, the value in **data[\"survey\"]** should be a dataframe with 23 columns and 1702 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting DBN Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we explored all of the data sets, we noticed that some of them, like **class_size** and **hs_directory**, don't have a DBN column. **hs_directory** does have a **dbn** column, though, so we can just rename it.\n",
    "\n",
    "However, **class_size** doesn't appear to have the column at all. Here are the first few rows of the data set:\n",
    "\n",
    "```python\n",
    "    CSD BOROUGH   SCHOOL CODE         SCHOOL NAME GRADE  PROGRAM TYPE  \\\n",
    "0    1       M        M015  P.S. 015 Roberto Clemente     0K       GEN ED\n",
    "1    1       M        M015  P.S. 015 Roberto Clemente     0K          CTT\n",
    "2    1       M        M015  P.S. 015 Roberto Clemente     01       GEN ED\n",
    "3    1       M        M015  P.S. 015 Roberto Clemente     01          CTT\n",
    "4    1       M        M015  P.S. 015 Roberto Clemente     02       GEN ED\n",
    "```\n",
    "\n",
    "Here are the first few rows of the **sat_results** data, which does have a **DBN** column:\n",
    "\n",
    "```python\n",
    "DBN                                    SCHOOL NAME  \\\n",
    "0  01M292  HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES\n",
    "1  01M448            UNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n",
    "2  01M450                     EAST SIDE COMMUNITY SCHOOL\n",
    "3  01M458                      FORSYTH SATELLITE ACADEMY\n",
    "4  01M509                        MARTA VALLE HIGH SCHOOL\n",
    "```\n",
    "\n",
    "From looking at these rows, we can tell that the **DBN** in the **sat_results** data is just a combination of the **CSD** and **SCHOOL CODE** columns in the **class_size** data. The main difference is that the **DBN** is padded, so that the **CSD** portion of it always consists of two digits. That means we'll need to add a leading 0 to the **CSD** if the **CSD** is less than two digits long. Here's a diagram illustrating what we need to do:\n",
    "\n",
    "<center><img width=\"150\" src=\"https://drive.google.com/uc?export=view&id=1k1QuSctrJnMW6gfAbTs-DJbzTe23en2U\"></center>\n",
    "\n",
    "As you can see, whenever the **CSD** is less than two digits long, we need to add a leading 0. We can accomplish this using the **pandas.Series.apply()** method, along with a custom function that:\n",
    "\n",
    "- Takes in a number.\n",
    "- Converts the number to a string using the **str()** function.\n",
    "- Check the length of the string using the **len()** function.\n",
    "    - If the string is two digits long, returns the string.\n",
    "    - If the string is one digit long, adds a 0 to the front of the string, then returns it.\n",
    "        - You can use the string method [zfill()](https://docs.python.org/3/library/stdtypes.html#str.zfill) to do this.\n",
    "\n",
    "Once we've padded the **CSD**, we can use the addition operator (+) to combine the values in the **CSD** and **SCHOOL CODE** columns. Here's an example of how we would do this:\n",
    "\n",
    "```python\n",
    "dataframe[\"new_column\"] = dataframe[\"column_one\"] + dataframe[\"column_two\"]\n",
    "```\n",
    "\n",
    "And here's a diagram illustrating the basic concept:\n",
    "\n",
    "<center><img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=1Io7o-45Pixlv2tX8rTyZvL6KjIq9qMPz\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting DBN Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Copy the **dbn** column in **hs_directory** into a new column called **DBN**.\n",
    "- Create a new column called **padded_csd** in the **class_size** data set.\n",
    "    - Use the [pandas.Series.apply()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) method along with the function **pad_csd()** to generate this column.\n",
    "        - Make sure to apply the function along the **data[\"class_size\"][\"CSD\"]** column.\n",
    "- Use the addition operator (+) along with the **padded_csd** and **SCHOOL CODE** columns of **class_size**, then assign the result to the **DBN** column of **class_size**.\n",
    "- Display the first few rows of **class_size** to double check the **DBN** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n",
    "def pad_csd(num):\n",
    "    string_representation = str(num)\n",
    "    if len(string_representation) > 1:\n",
    "        return string_representation\n",
    "    else:\n",
    "        return string_representation.zfill(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the SAT Scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're almost ready to combine our data sets. Before we do, let's take some time to calculate variables that will be useful in our analysis. We've already discussed one such variable -- a column that totals up the SAT scores for the different sections of the exam. This will make it much easier to correlate scores with demographic factors because we'll be working with a single number, rather than three different ones.\n",
    "\n",
    "Before we can generate this column, we'll need to convert the **SAT Math Avg. Score**, **SAT Critical Reading Avg. Score**, and **SAT Writing Avg. Score** columns in the **sat_results** data set from the object (string) data type to a numeric data type. We can use the [pandas.to_numeric()](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.to_numeric.html) method for the conversion. If we don't convert the values, we won't be able to add the columns together.\n",
    "\n",
    "It's important to pass the keyword argument **errors=\"coerce\"** when we call **pandas.to_numeric()**, so that pandas treats any invalid strings it can't convert to numbers as missing values instead.\n",
    "\n",
    "After we perform the conversion, we can use the addition operator (+) to add all three columns together.\n",
    "\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Convert the **SAT Math Avg. Score**, **SAT Critical Reading Avg. Score**, and **SAT Writing Avg. Score** columns in the **sat_results** data set from the object (string) data type to a numeric data type.\n",
    "    - Use the **pandas.to_numeric()** function on each of the columns, and assign the result back to the same column.\n",
    "    - Pass in the keyword argument **errors=\"coerce\".**\n",
    "- Create a column called **sat_score** in **sat_results** that holds the combined SAT score for each student.\n",
    "    - Add up **SAT Math Avg. Score**, **SAT Critical Reading Avg. Score**, and **SAT Writing Avg. Score**, and assign the total to the **sat_score** column of **sat_results.**\n",
    "- Display the first few rows of the **sat_score column** of **sat_results** to verify that everything went okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Geographic Coordinates for Schools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to parse the **latitude** and **longitude** coordinates for each school. This will enable us to map the schools and uncover any geographic patterns in the data. The coordinates are currently in the text field **Location 1** in the **hs_directory** data set.\n",
    "\n",
    "Let's take a look at the first few rows:\n",
    "\n",
    "```python\n",
    "0    883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67...\n",
    "1    1110 Boston Road\\nBronx, NY 10456\\n(40.8276026...\n",
    "2    1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241...\n",
    "3    411 Pearl Street\\nNew York, NY 10038\\n(40.7106...\n",
    "4    160-20 Goethals Avenue\\nJamaica, NY 11432\\n(40...\n",
    "```        \n",
    "                                               \n",
    "As you can see, this field contains a lot of information we don't need. We want to extract the coordinates, which are in parentheses at the end of the field. Here's an example:\n",
    "\n",
    "```python\n",
    "1110 Boston Road\\nBronx, NY 10456\\n(40.8276026690005, -73.90447525699966)\n",
    "```\n",
    "\n",
    "We want to extract the **latitude**, 40.8276026690005, and the **longitude**, -73.90447525699966. Taken together, **latitude** and **longitude** make up a pair of coordinates that allows us to pinpoint any location on Earth.\n",
    "\n",
    "We can do the extraction with a **regular expression**. The following expression will pull out everything inside the parentheses:\n",
    "\n",
    "```python\n",
    "import re\n",
    "re.findall(\"\\(.+\\)\", \"1110 Boston Road\\nBronx, NY 10456\\n(40.8276026690005, -73.90447525699966)\")\n",
    "```\n",
    "\n",
    "This command will return (40.8276026690005, -73.90447525699966). We'll need to process this result further using the string methods [split()](https://docs.python.org/3/library/stdtypes.html#str.split) and [replace()](https://docs.python.org/3/library/stdtypes.html#str.replace) methods to extract each coordinate.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Write a function that:\n",
    "    - Takes in a string\n",
    "    - Uses the regular expression above to extract the coordinates\n",
    "    - Uses string manipulation functions to pull out the latitude\n",
    "    - Returns the latitude\n",
    "- Use the [Series.apply()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) method to apply the function across the **Location 1** column of **hs_directory**. Assign the result to the **lat** column of **hs_directory.**\n",
    "- Display the first few rows of **hs_directory** to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the last screen, we parsed the **latitude** from the **Location 1** column. Now we'll just need to do the same for the longitude.\n",
    "\n",
    "Once we have both coordinates, we'll need to convert them to numeric values. We can use the **pandas.to_numeric()** function to convert them from strings to numbers.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Write a function that:\n",
    "    - Takes in a string.\n",
    "    - Uses the regular expression above to extract the coordinates.\n",
    "    - Uses string manipulation functions to pull out the longitude.\n",
    "    - Returns the longitude.\n",
    "- Use the **Series.apply()** method to apply the function across the **Location 1** column of **hs_directory**. Assign the result to the **lon** column of **hs_directory.**\n",
    "- Use the **to_numeric()** function to convert the **lat** and **lon** columns of **hs_directory** to numbers.\n",
    "    - Specify the **errors=\"coerce\"** keyword argument to handle missing values properly.\n",
    "- Display the first few rows of hs_directory to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to combine our data sets! We've come a long way in this mission -- we've gone from choosing a topic for a project to acquiring the data to having clean data that we're almost ready to combine.\n",
    "\n",
    "Along the way, we've learned how to:\n",
    "\n",
    "- Handle files with different formats and columns\n",
    "- Prepare to merge multiple files\n",
    "- Use text processing to extract coordinates from a string\n",
    "- Convert columns from strings to numbers\n",
    "\n",
    "You'll always learn something new while working on a real-world data science project. Each project is unique, and there will always be quirks you don't quite know how to handle. The key is to be willing to try different approaches, and to have a general framework in your head for how to move from Step A to Step B.\n",
    "\n",
    "In the next section,  we'll finish cleaning the data sets, then combine them so we can start our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Walkthrough: Combining the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section, we began investigating possible relationships between **SAT scores** and **demographic factors**. In order to do this, we acquired several data sets about [New York City public schools](https://data.cityofnewyork.us/data?cat=education). We manipulated these data sets, and found that we could combine them all using the **DBN** column. All of the data sets are currently stored as **keys** in the **data** dictionary. Each individual data set is a pandas dataframe.\n",
    "\n",
    "In this section, **we'll clean the data a bit more**, then **combine** it. Finally, we'll **compute correlations** and perform some analysis.\n",
    "\n",
    "The first thing we'll need to do in preparation for the merge is condense some of the data sets. In the last section, we noticed that the values in the **DBN** column were unique in the **sat_results** data set. Other data sets like **class_size** had duplicate **DBN** values, however.\n",
    "\n",
    "We'll need to condense these data sets so that each value in the **DBN** column is unique. If not, we'll run into issues when it comes time to combine the data sets.\n",
    "\n",
    "While the main data set we want to analyze, **sat_results**, has unique **DBN** values for every high school in New York City, other data sets aren't as clean. A single row in the **sat_results** data set may match multiple rows in the **class_size** data set, for example. This situation will create problems, because we don't know which of the multiple entries in the **class_size** data set we should combine with the single matching entry in **sat_results**. Here's a diagram that illustrates the problem:\n",
    "\n",
    "\n",
    "<left><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1deYm5RdQXO2xMX6dUgHLvqDEWipk3axq\"></left>\n",
    "\n",
    "In the diagram above, we can't just combine the rows from both data sets because there are several cases where multiple rows in **class_size** match a single row in **sat_results.**\n",
    "\n",
    "To resolve this issue, we'll condense the **class_size**, **graduation**, and **demographics** data sets so that each **DBN** is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensing the Class Size Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first data set that we'll condense is **class_size**. The first few rows of **class_size** look like this:\n",
    "\n",
    "|__| CSD | BOROUGH | SCHOOL CODE | SCHOOL NAME               | GRADE | PROGRAM TYPE | CORE SUBJECT (MS CORE and 9-12 ONLY) | CORE COURSE (MS CORE and 9-12 ONLY) | SERVICE CATEGORY(K-9* ONLY) | NUMBER OF STUDENTS / SEATS FILLED | NUMBER OF SECTIONS |\n",
    "|---|-----|---------|-------------|---------------------------|-------|--------------|--------------------------------------|-------------------------------------|-----------------------------|-----------------------------------|--------------------|\n",
    "| 0 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 0K    | GEN ED       | -                                    | -                                   | -                           | 19.0                              | 1.0                |\n",
    "| 1 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 0K    | CTT          | -                                    | -                                   | -                           | 21.0                              | 1.0                |\n",
    "| 2 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 01    | GEN ED       | -                                    | -                                   | -                           | 17.0                              | 1.0                |\n",
    "| 3 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 01    | CTT          | -                                    | -                                   | -                           | 17.0                              | 1.0                |\n",
    "| 4 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 02    | GEN ED       | -                                    | -                                   | -                           | 15.0                              | 1.0                |\n",
    "\n",
    "As you can see, the first few rows all pertain to the same school, which is why the **DBN** appears more than once. It looks like each school has multiple values for **GRADE**, **PROGRAM TYPE**, **CORE SUBJECT (MS CORE and 9-12 ONLY)**, and **CORE COURSE (MS CORE and 9-12 ONLY)**.\n",
    "\n",
    "If we look at the unique values for **GRADE**, we get the following:\n",
    "\n",
    "```python\n",
    "array(['0K', '01', '02', '03', '04', '05', '0K-09', nan, '06', '07', '08',\n",
    "       'MS Core', '09-12', '09'], dtype=object)\n",
    "```\n",
    "\n",
    "Because we're dealing with high schools, we're only concerned with grades 9 through 12. That means we only want to pick rows where the value in the **GRADE** column is **09-12**.\n",
    "\n",
    "If we look at the unique values for **PROGRAM TYPE**, we get the following:\n",
    "\n",
    "```python\n",
    "array(['GEN ED', 'CTT', 'SPEC ED', nan, 'G&T'], dtype=object)\n",
    "```\n",
    "\n",
    "Each school can have multiple program types. Because **GEN ED** is the largest category by far, let's only select rows where **PROGRAM TYPE** is **GEN ED**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensing the Class Size Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Create a new variable called **class_size**, and assign the value of **data[\"class_size\"]** to it.\n",
    "- Filter **class_size** so the **GRADE** column only contains the value **09-12.** Note that the name of the **GRADE** column has a space at the end; you'll generate an error if you don't include it.\n",
    "- Filter **lass_size** so that the **PROGRAM TYPE** column only contains the value **GEN ED.**\n",
    "- Display the first five rows of **class_size** to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Average Class Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw when we displayed **class_size** on the last screen, **DBN** still isn't completely unique. This is due to the **CORE COURSE (MS CORE and 9-12 ONLY)** and **CORE SUBJECT (MS CORE and 9-12 ONLY)** columns.\n",
    "\n",
    "**CORE COURSE (MS CORE and 9-12 ONLY)** and **CORE SUBJECT (MS CORE and 9-12 ONLY)** seem to pertain to different kinds of classes. For example, here are the unique values for **CORE SUBJECT (MS CORE and 9-12 ONLY)**:\n",
    "\n",
    "```python\n",
    "array(['ENGLISH', 'MATH', 'SCIENCE', 'SOCIAL STUDIES'], dtype=object)\n",
    "```\n",
    "\n",
    "This column only seems to include certain subjects. We want our class size data to include every single class a school offers -- not just a subset of them. What we can do is take the average across all of the classes a school offers. This will give us unique **DBN** values, while also incorporating as much data as possible into the average.\n",
    "\n",
    "Fortunately, we can use the [pandas.DataFrame.groupby()](http://pandas.pydata.org/pandas-docs/stable/groupby.html) method to help us with this. The **DataFrame.groupby()** method will split a dataframe up into unique groups, based on a given column. We can then use the **agg()** method on the resulting **pandas.core.groupby** object to find the **mean** of each column.\n",
    "\n",
    "Let's say we have this data set:\n",
    "\n",
    "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1sJjENlTRR56RwYzBmmsU8aIMELgjx8zg\"></left>\n",
    "\n",
    "Using the **groupby()** method, we'll split this dataframe into four separate groups -- one with the **DBN 01M292**, one with the **DBN 01M332**, one with the **DBN 01M378**, and one with the **DBN 01M448**:\n",
    "\n",
    "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1y9imbMLKRDI50wQqPn7P6TAd6MfCL4Nq\"></left>\n",
    "\n",
    "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1FitnyClxHDQLnoAB3jR7YI_jEPZZhkco\"></left>\n",
    "\n",
    "Then, we can compute the averages for the **AVERAGE CLASS SIZE** column in each of the four groups using the **agg()** method:\n",
    "\n",
    "<left><img width=\"200\" src=\"https://drive.google.com/uc?export=view&id=1gHVZixGOuGYYON_zU0OUPTJcC9Q_mKeV\"></left>\n",
    "\n",
    "After we group a dataframe and aggregate data based on it, the column we performed the grouping on (in this case **DBN**) will become the index, and will no longer appear as a column in the data itself. To undo this change and keep **DBN** as a column, we'll need to use [pandas.DataFrame.reset_index()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html). This method will reset the index to a list of integers and make **DBN** a column again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Average Class Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the average values for each column associated with each **DBN** in **class_size**.\n",
    "    - Use the [pandas.DataFrame.groupby()](http://pandas.pydata.org/pandas-docs/stable/groupby.html) method to group **class_size** by **DBN**.\n",
    "    - Use the [agg()](http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation) method on the resulting **pandas.core.groupby** object, along with the **numpy.mean()** function as an argument, to calculate the average of each group.\n",
    "    - Assign the result back to **class_size**.\n",
    "- Reset the index to make **DBN** a column again.\n",
    "    - Use the [pandas.DataFrame.reset_index()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html) method, along with the keyword argument **inplace=True**.\n",
    "- Assign **class_size** back to the **class_size** key of the **data** dictionary.\n",
    "- Display the first few rows of **data[\"class_size\"]** to verify that everything went okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensing the Demographics Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished condensing **class_size**, let's condense **demographics**. The first few rows look like this:\n",
    "\n",
    "| _| DBN    | Name                      | schoolyear | fl_percent | frl_percent | total_enrollment | prek | k  | grade1 | grade2 |\n",
    "|---|--------|---------------------------|------------|------------|-------------|------------------|------|----|--------|--------|\n",
    "| 0 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20052006   | 89.4       | NaN         | 281              | 15   | 36 | 40     | 33     |\n",
    "| 1 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20062007   | 89.4       | NaN         | 243              | 15   | 29 | 39     | 38     |\n",
    "| 2 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20072008   | 89.4       | NaN         | 261              | 18   | 43 | 39     | 36     |\n",
    "| 3 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20082009   | 89.4       | NaN         | 252              | 17   | 37 | 44     | 32     |\n",
    "| 4 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20092010   |  _          | 96.5        | 208              | 16   | 40 | 28     | 32     |\n",
    "\n",
    "In this case, the only column that prevents a given **DBN** from being unique is **schoolyear**. We only want to select rows where schoolyear is **20112012**. This will give us the most recent year of data, and also match our SAT results data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensing the Demographics Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Filter **demographics**, only selecting rows in **data[\"demographics\"]** where **schoolyear** is **20112012.**\n",
    "    - **schoolyear** is actually an integer, so be careful about how you perform your comparison.\n",
    "- Display the first few rows of **data[\"demographics\"]** to verify that the filtering worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensing the Graduation Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll need to condense the **graduation** data set. Here are the first few rows:\n",
    "\n",
    "| _ | Demographic  | DBN    | School Name                           | Cohort   | Total Cohort | Total Grads - n |\n",
    "|---|--------------|--------|---------------------------------------|----------|--------------|-----------------|\n",
    "| 0 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2003     | 5            | s               |\n",
    "| 1 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2004     | 55           | 37              |\n",
    "| 2 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2005     | 64           | 43              |\n",
    "| 3 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006     | 78           | 43              |\n",
    "| 4 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006 Aug | 78           | 44              |\n",
    "\n",
    "The **Demographic** and **Cohort** columns are what prevent **DBN** from being unique in the **graduation** data. A **Cohort** appears to refer to the year the data represents, and the **Demographic** appears to refer to a specific demographic group. In this case, we want to pick data from the most recent Cohort available, which is 2006. We also want data from the full cohort, so we'll only pick rows where **Demographic** is **Total Cohort**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensing the Graduation Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Filter **graduation**, only selecting rows where the **Cohort** column equals **2006.**\n",
    "- Filter **graduation**, only selecting rows where the **Demographic** column equals **Total Cohort**.\n",
    "- Display the first few rows of **data[\"graduation\"]** to verify that everything worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting AP Test Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to combine all of the data sets. The only remaining thing to do is convert the [Advanced Placement (AP)](https://en.wikipedia.org/wiki/Advanced_Placement_exams) test scores from strings to numeric values. High school students take the AP exams before applying to college. There are several AP exams, each corresponding to a school subject. High school students who earn high scores may receive college credit.\n",
    "\n",
    "AP exams have a 1 to 5 scale; 3 or higher is a passing score. Many high school students take AP exams -- particularly those who attend academically challenging institutions. AP exams are much more rare in schools that lack funding or academic rigor.\n",
    "\n",
    "It will be interesting to find out whether AP exam scores are correlated with SAT scores across high schools. To determine this, we'll need to convert the AP exam scores in the **ap_2010** data set to numeric values first.\n",
    "\n",
    "There are three columns we'll need to convert:\n",
    "\n",
    "- **AP Test Takers** (note that there's a trailing space in the column name)\n",
    "- **Total Exams Taken**\n",
    "- **Number of Exams with scores 3 4 or 5**\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Convert each of the following columns in **ap_2010** to numeric values using the [pandas.to_numeric()](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.to_numeric.html) function with the keyword argument **errors=\"coerce\".**\n",
    "    - **AP Test Takers**\n",
    "    - **Total Exams Taken**\n",
    "    - **Number of Exams with scores 3 4 or 5**\n",
    "- Display the column types using the **dtypes** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left, Right, Inner, and Outer Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we merge our data, we'll need to decide on the merge strategy we want to use. We'll be using the pandas [pandas.DataFrame.merge()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) function, which supports four types of joins -- **left**, **right**, **inner**, and **outer**. Each of these join types dictates how pandas combines the rows.\n",
    "\n",
    "We'll be using the **DBN** column to identify matching rows across data sets. In other words, the values in that column will help us know which row from the first data set to combine with which row in the second data set.\n",
    "\n",
    "There may be **DBN** values that exist in one data set but not in another. This is partly because the data is from different years. Each data set also has inconsistencies in terms of how it was gathered. Human error (and other types of errors) may also play a role. Therefore, we may not find matches for the **DBN** values in **sat_results** in all of the other data sets, and other data sets may have **DBN** values that don't exist in **sat_results**.\n",
    "\n",
    "We'll merge two data sets at a time. For example, we'll merge **sat_results** and **hs_directory**, then merge the result with **ap_2010**, then merge the result of that with **class_size**. We'll continue combining data sets in this way until we've merged all of them. Afterwards, we'll have roughly the same number of rows, but each row will have columns from all of the data sets.\n",
    "\n",
    "The merge strategy we pick will affect the number of rows we end up with. Let's take a look at each strategy.\n",
    "\n",
    "Let's say we're merging the following two data sets:\n",
    "\n",
    "<left><img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=1Vlypix_SIkxCdRS0ABvO4tGiuvFLg321\"></left>\n",
    "\n",
    "With an **inner merge**, we'd only combine rows where the same **DBN** exists in both data sets. We'd end up with this result:\n",
    "\n",
    "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1LR4c8louX-JAZFYta_Y99FLsGkCf9grr\"></left>\n",
    "\n",
    "With a **left merge**, we'd only use **DBN** values from the dataframe on the \"left\" of the merge. In this case, **sat_results** is on the left. Some of the DBNs in **sat_results** don't exist in **class_size**, though. The merge will handle this by assiging null values to the columns in **sat_results** that don't have corresponding data in **class_size.**\n",
    "\n",
    "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1hPoJ5wLECEzz25jrTP5bw9oZ0eerNi2p\"></left>\n",
    "\n",
    "With a **right merge**, we'll only use **DBN** values from the dataframe on the \"right\" of the merge. In this case, **class_size** is on the right:\n",
    "\n",
    "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1YYdf4iEMtHYqRBMTEFcfuTyu9zdFlnx7\"></left>\n",
    "\n",
    "With an outer merge, we'll take any DBN values from either sat_results or class_size:\n",
    "\n",
    "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1sl5wCK3WZ3lTzJm8JUn-bg4MoXl3xSe9\"></left>\n",
    "\n",
    "As you can see, each merge strategy has its advantages. Depending on the strategy we choose, we may preserve rows at the expense of having more missing column data, or minimize missing data at the expense of having fewer rows. Choosing a merge strategy is an important decision; it's worth thinking about your data carefully, and what trade-offs you're willing to make.\n",
    "\n",
    "Because this project is concerned with determing demographic factors that correlate with SAT score, we'll want to preserve as many rows as possible from **sat_results** while minimizing null values.\n",
    "\n",
    "This means that we may need to use different merge strategies with different data sets. Some of the data sets have a lot of missing **DBN** values. This makes a **left** join more appropriate, because we don't want to lose too many rows when we merge. If we did an **inner** join, we would lose the data for many high schools.\n",
    "\n",
    "Some data sets have **DBN** values that are almost identical to those in **sat_results**. Those data sets also have information we need to keep. Most of our analysis would be impossible if a significant number of rows was missing from **demographics**, for example. Therefore, we'll do an inner join to avoid missing data in these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Performing the Left Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the **ap_2010** and the **graduation** data sets have many missing **DBN** values, so we'll use a left join when we merge the **sat_results** data set with them. Because we're using a **left** join, our final dataframe will have all of the same **DBN** values as the original **sat_results** dataframe.\n",
    "\n",
    "We'll need to use the pandas **df.merge()** method to merge dataframes. The \"left\" dataframe is the one we call the method on, and the \"right\" dataframe is the one we pass into **df.merge()**.\n",
    "\n",
    "Because we're using the **DBN** column to join the dataframes, we'll need to specify the keyword argument **on=\"DBN\"** when calling **pandas.DataFrame.merge().**\n",
    "\n",
    "First, we'll assign **data[\"sat_results\"]** to the variable **combined**. Then, we'll merge all of the other dataframes with **combined**. When we're finished, **combined** will have all of the columns from all of the data sets.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the pandas [pandas.DataFrame.merge()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) method to merge the **ap_2010** data set into **combined.**\n",
    "    - Make sure to specify **how=\"left\"** as a keyword argument to indicate the correct join type.\n",
    "    - Make sure to assign the result of the merge operation back to **combined.**\n",
    "- Use the pandas **df.merge()** method to merge the **graduation** data set into **combined.**\n",
    "    - Make sure to specify **how=\"left\"** as a keyword argument to get the correct join type.\n",
    "    - Make sure to assign the result of the merge operation back to **combined.**\n",
    "- Display the first few rows of **combined** to verify that the correct operations occurred.\n",
    "- Use the [pandas.DataFrame.shape](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.shape.html) attribute to display the shape of the dataframe and see how many rows now exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Inner Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've performed the left joins, we still have to merge **class_size**, **demographics**, **survey**, and **hs_directory** into **combined**. Because these files contain information that's more valuable to our analysis and also have fewer missing **DBN** values, we'll use the **inner** join type.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Merge **class_size** into **combined**. Then, merge **demographics**, **survey**, and **hs_directory** into **combined** one by one, in that order.\n",
    "    - Be sure to follow the exact order above.\n",
    "    - Remember to specify the correct column to join on, as well as the correct join type.\n",
    "- Display the first few rows of **combined** to verify that the correct operations occurred.\n",
    "- Call **pandas.DataFrame.shape()** to display the shape of the dataframe to see how many rows now exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Filling in Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the inner joins resulted in 116 fewer rows in **sat_results**. This is because pandas couldn't find the **DBN** values that existed in **sat_results** in the other data sets. While this is worth investigating, we're currently looking for high-level correlations, so we don't need to dive into which **DBNs** are missing.\n",
    "\n",
    "You may also have noticed that we now have many columns with null (**NaN**) values. This is because we chose to do **left** joins, where some columns may not have had data. The data set also had some missing values to begin with. If we hadn't performed a **left** join, all of the rows with missing data would have been lost in the merge process, which wouldn't have left us with many high schools in our data set.\n",
    "\n",
    "There are several ways to handle missing data, and we'll cover them in more detail later on. For now, we'll just fill in the missing values with the overall mean for the column, like so:\n",
    "\n",
    "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1OmhXzMuPrGSmyyugGXpmrRlLznDHxOeT\"></left>\n",
    "\n",
    "In the diagram above, the mean of the first column is (1800 + 1600 + 2200 + 2300) / 4, or 1975, and the mean of the second column is (20 + 30 + 30 + 50) / 4, or 32.5. We replace the missing values with the means of their respective columns, which allows us to proceed with analyses that can't handle missing values (like correlations).\n",
    "\n",
    "We can fill in missing data in pandas using the [pandas.DataFrame.fillna()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) method. This method will replace any missing values in a dataframe with the values we specify. We can compute the mean of every column using the [pandas.DataFrame.mean()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html) method. If we pass the results of the **df.mean()** method into the **df.fillna()** method, pandas will fill in the missing values in each column with the mean of that column.\n",
    "\n",
    "Here's an example of how we would accomplish this:\n",
    "\n",
    "```python\n",
    "means = df.mean()\n",
    "df = df.fillna(means)\n",
    "```\n",
    "\n",
    "Note that if a column consists entirely of null or **NaN** values, pandas won't be able to fill in the missing values when we use the **df.fillna()** method along with the **df.mean()** method, because there won't be a mean.\n",
    "\n",
    "We should fill any **NaN** or null values that remain after the initial replacement with the value 0. We can do this by passing 0 into the **df.fillna()** method.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Calculate the means of all of the columns in **combined** using the **pandas.DataFrame.mean()** method.\n",
    "- Fill in any missing values in **combined** with the means of the respective columns using the **pandas.DataFrame.fillna()** method.\n",
    "- Fill in any remaining missing values in **combined** with 0 using the **df.fillna()** method.\n",
    "- Display the first few rows of **combined** to verify that the correct operations occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a School District Column for Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've finished cleaning and combining our data! We now have a clean data set on which we can base our analysis. Mapping the statistics out on a school district level might be an interesting way to analyze them. Adding a column to the data set that specifies the school district will help us accomplish this.\n",
    "\n",
    "The school district is just the first two characters of the **DBN**. We can apply a function over the **DBN** column of **combined** that pulls out the first two letters.\n",
    "\n",
    "For example, we can use indexing to extract the first few characters of a string, like this:\n",
    "\n",
    "```python\n",
    "name = \"Sinbad\"\n",
    "print(name[0:2])\n",
    "```\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Write a function that extracts the first two characters of a string and returns them.\n",
    "- Apply the function to the **DBN** column of **combined**, and assign the result to the **school_dist** column of **combined**.\n",
    "- Display the first few items in the **school_dist** column of **combined** to verify the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a clean data set we can analyze! We've done a lot in this mission. We've gone from having several messy sources to one clean, combined, data set that's ready for analysis.\n",
    "\n",
    "Along the way, we've learned about:\n",
    "\n",
    "- How to handle missing values\n",
    "- Different types of merges\n",
    "- How to condense data sets\n",
    "- How to compute averages across dataframes\n",
    "\n",
    "Data scientists rarely start out with tidy data sets, which makes cleaning and combining them one of the most critical skills any data professional can learn.\n",
    "\n",
    "In the next mission, we'll analyze our clean data to find correlations and create maps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
