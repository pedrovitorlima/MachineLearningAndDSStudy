{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1UpwLVsQCIh"
   },
   "source": [
    "# 1 - What is Deep Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73cE6p9bTqX1"
   },
   "source": [
    "\n",
    "In the past few years, **artificial intelligence** (AI) has been a subject of intense media hype. **Machine learning**, **deep learning**, and **AI** come up in countless articles, often outside of technology-minded publications. We’re promised a future of intelligent chatbots, self-driving cars, and virtual assistants—a future sometimes painted in a grim light and other times as utopian, where human jobs will be scarce and most economic activity will be handled by robots or AI agents.\n",
    "\n",
    "So let’s tackle these questions: \n",
    "- What has deep learning achieved so far? \n",
    "- How significant is it? \n",
    "- Where are we headed next? \n",
    "- Should you believe the hype?\n",
    "\n",
    "\n",
    "<img width=\"300\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1r_LkSAgQOBAJMecKg-B9g7DnMR6e-otN\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_rW1b2vQCIj"
   },
   "source": [
    "## 1.1 Artificial intelligence, machine learning, and deep learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vg0kUbFxTsWy"
   },
   "source": [
    "\n",
    "### 1.1.1 Artificial intelligence\n",
    "\n",
    "A concise definition of the field would be as follows: **the effort to automate intellectual tasks normally performed by humans**. As such, AI is a general field that encompasses machine learning and deep learning, but that also includes many more approaches that don’t involve any learning. Early chess programs, for instance, only involved hardcoded rules crafted by programmers, and didn’t qualify as machine learning. For a fairly long time, many experts believed that human-level artificial intelligence could be achieved by having programmers handcraft a sufficiently large set of explicit rules for manipulating knowledge. This approach is known as **symbolic AI**, and it was the dominant paradigm in AI from the 1950s to the late 1980s. It reached its peak popularity during the **expert systems boom** of the 1980s.\n",
    "\n",
    "Although symbolic AI proved suitable to solve well-defined, logical problems, such as playing chess, it turned out to be intractable to figure out explicit rules for solving more complex, fuzzy problems, such as image classification, speech recognition, and language translation. A new approach arose to take symbolic AI’s place: **machine learning**.\n",
    "\n",
    "### 1.1.2 Machine Learning\n",
    "\n",
    "Machine learning arises from this question: **could a computer go beyond** “what we know how to order it to perform” and learn on its own how to perform a specified task? **Could a computer surprise us**? Rather than programmers crafting data-processing rules by hand, **could a computer automatically learn these rules by looking at data**?\n",
    "\n",
    "This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers (see figure 1.2). With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\n",
    "\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1ZQ0vk8lzgPcIrYPS8iKL1zuiTpLV9yjz\">\n",
    "\n",
    "**A machine-learning system is trained rather than explicitly programmed.**\n",
    "\n",
    "Although machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, **a trend driven by the availability of faster hardware and larger datasets**. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\n",
    "\n",
    "So that’s what machine learning is, technically: **searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal**. This simple idea allows for solving a remarkably broad range of intellectual tasks, from speech recognition to autonomous car driving.\n",
    "\n",
    "### 1.1.3 The “deep” in deep learning\n",
    "\n",
    "Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly meaningful representations. The deep in deep learning isn’t a reference to any kind of deeper understanding achieved by the approach; rather, it stands for this idea of **successive layers of representations**.\n",
    "\n",
    "Meanwhile, other approaches to machine learning tend to focus on learning only one or two layers of representations of the data; hence, they’re sometimes called **shallow learning**.\n",
    "\n",
    "**What do the representations learned by a deep-learning algorithm look like**? Let’s examine how a network several layers deep transforms an image of a digit in order to recognize what digit it is.\n",
    "\n",
    "As you can see in figure 1.6, the network transforms the digit image into representations that are increasingly different from the original image and increasingly informative about the final result. You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task).\n",
    "\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1C2r5nhH0c4cl_6rSvT0OVhBlOVwh89dt\">\n",
    "\n",
    "So that’s what deep learning is, technically: **a multistage way to learn data representations.** \n",
    "\n",
    "### 1.1.4 Understanding how deep learning works\n",
    "\n",
    "\n",
    "The fundamental trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example (see figure 1.9). This adjustment is the job of the optimizer, which implements what’s called the **Backpropagation algorithm**: the central algorithm in deep learning. The next chapter explains in more detail how backpropagation works.\n",
    "\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1aHb4jnSup4p1jQtHEERNpQo1cz33LBQj\">\n",
    "\n",
    "Initially, the weights of the network are assigned random values, so the network merely implements a series of random transformations. Naturally, its output is far from what it should ideally be, and the loss score is accordingly very high. But with every example the network processes, the weights are adjusted a little in the correct direction, and the loss score decreases. This is the training loop, which, repeated a sufficient number of times (typically tens of iterations over thousands of examples), yields weight values that minimize the loss function. **A network with a minimal loss is one for which the outputs are as close as they can be to the targets: a trained network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVfLHxoEQCIk",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 1.2. Before deep learning: a brief history of machine learning\n",
    "\n",
    "### 1.2.1 Probabilistic modeling\n",
    "\n",
    "Probabilistic modeling is the **application of the principles of statistics** to data analysis. It was one of the earliest forms of machine learning, and it’s still widely used to this day. One of the best-known algorithms in this category is the **Naive Bayes algorithm**.\n",
    "\n",
    "A closely related model is the **logistic regression** (logreg for short), which is sometimes considered to be the **“hello world” of modern machine learning**. Don’t be misled by its name—logreg is a classification algorithm rather than a regression algorithm. \n",
    "\n",
    "Much like **Naive Bayes**, **logreg** predates computing by a long time, yet it’s still useful to this day, thanks to its simple and versatile nature. It’s often the first thing a data scientist will try on a dataset to get a feel for the classification task at hand.\n",
    "\n",
    "### 1.2.2. Early neural networks\n",
    "Although the core ideas of neural networks were investigated in toy forms as early as the **1950s**, the approach took decades to get started. For a long time, **the missing piece was an efficient way to train large neural networks**. \n",
    "\n",
    "This changed in the **mid-1980s**, when multiple people independently rediscovered the **Backpropagation algorithm** -- a way to train chains of parametric operations using gradient-descent optimization -- and started applying it to neural networks.\n",
    "\n",
    "**The first successful practical application of neural nets came in 1989 from Bell Labs**, when [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) combined the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of classifying handwritten digits. The resulting network, dubbed LeNet, was used by the United States Postal Service in the 1990s to automate the reading of ZIP codes on mail envelopes.\n",
    "\n",
    "### 1.2.3. Kernel methods\n",
    "\n",
    "As neural networks started to gain some respect among researchers in the 1990s, thanks to this first success, a new approach to machine learning rose to fame and quickly sent neural nets back to oblivion: **kernel methods**. Kernel methods are a **group of classification algorithms**, the best known of which is the **support vector machine (SVM)**. The modern formulation of an SVM was developed by Vladimir Vapnik and Corinna Cortes in the early **1990s** at Bell Labs and published in **1995**, although an older linear formulation was published by Vapnik and Alexey Chervonenkis as early as **1963**.\n",
    "\n",
    "The technique of mapping data to a high-dimensional representation where a classification problem becomes simpler may look good on paper, but in practice it’s often computationally intractable.\n",
    "\n",
    "At the time they were developed, SVMs exhibited state-of-the-art performance on simple classification problems and were one of the few machine-learning methods backed by extensive theory and amenable to serious mathematical analysis, making them well understood and easily interpretable. Because of these useful properties, **SVMs became extremely popular in the field for a long time**.\n",
    "\n",
    "**But SVMs proved hard to scale to large datasets and didn’t provide good results for perceptual problems such as image classification**. Because an SVM is a shallow method, applying an SVM to perceptual problems requires first extracting useful representations manually (a step called **feature engineering**), which is difficult and brittle.\n",
    "\n",
    "### 1.2.4. Decision trees, random forests, and gradient boosting machines\n",
    "\n",
    "**Decision trees** are flowchart-like structures that let you classify input data points or predict output values given inputs. They’re **easy to visualize and interpret**. Decisions trees learned from data began to receive significant research interest in the 2000s, and by 2010 they were often preferred to kernel methods.\n",
    "\n",
    "\n",
    "In particular, the **Random Forest** algorithm introduced a robust, practical take on decision-tree learning that involves building a large number of specialized decision trees and then **ensembling** their outputs. Random forests are applicable to a wide range of problems—you could say that **they’re almost always the second-best algorithm for any shallow machine-learning task**. \n",
    "\n",
    "When the popular machine-learning competition website [Kaggle](http://kaggle.com) got started in **2010**, **random forests quickly became a favorite on the platform—until 2014**, when gradient boosting machines took over. \n",
    "\n",
    "**A gradient boosting machine**, much like a random forest, is a machine-learning technique based on **ensembling weak prediction models**, generally decision trees. It uses gradient boosting, a way to improve any machine-learning model by iteratively training new models that specialize in addressing the weak points of the previous models. Applied to decision trees, the use of the gradient boosting technique results in models that **strictly outperform random forests most of the time, while having similar properties**. **It may be one of the best, if not the best, algorithm for dealing with nonperceptual data today**. Alongside deep learning, it’s one of the most commonly used techniques in Kaggle competitions.\n",
    "\n",
    "### 1.2.5. Back to neural networks\n",
    "\n",
    "**Around 2010**, although neural networks were almost completely shunned by the scientific community at large, a number of people still working on neural networks started to make important breakthroughs: the groups of [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) at the University of Toronto, [Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) at the University of Montreal, [Yann LeCun]([Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) at New York University, and IDSIA in Switzerland.\n",
    "\n",
    "\n",
    "- **In 2011**, Dan Ciresan from IDSIA began to win academic image-classification competitions with **GPU-trained** deep neural networks.\n",
    "- **In 2012**, a team led by Alex Krizhevsky and advised by Geoffrey Hinton was able to achieve a top-five accuracy of **83.6%**—a significant breakthrough.\n",
    "- **By 2015**, the winner reached an accuracy of **96.4%**, and the classification task on ImageNet was considered to be a completely solved problem.\n",
    "- Since 2012, **deep convolutional neural networks (convnets)** have become the go-to algorithm for all computer vision tasks; more generally, they work on all perceptual tasks.\n",
    "- **At major computer vision conferences in 2015 and 2016**, it was nearly impossible to find presentations that didn’t involve convnets in some form.\n",
    "\n",
    "\n",
    "### 1.2.6. The modern machine-learning landscape\n",
    "\n",
    "\n",
    "A great way to get a sense of the current landscape of machine-learning algorithms and tools is to look at machine-learning competitions on [Kaggle](http://kaggle.com). \n",
    "\n",
    "**In 2016 and 2017**, Kaggle was dominated by two approaches: **gradient boosting machines** and **deep learning**. Specifically, **gradient boosting is used for problems where structured data is available**, whereas **deep learning is used for perceptual problems such as image classification**. \n",
    "\n",
    "These are the two techniques you should be the most familiar with in order to be successful in applied machine learning today: gradient boosting machines, for shallow-learning problems; and deep learning, for perceptual problems. In technical terms, this means you’ll need to be familiar with **XGBoost** and **Keras** —the two libraries that currently dominate Kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WUX8zLWQCIl"
   },
   "source": [
    "## 1.3. Why deep learning? Why now?\n",
    "\n",
    "The two key ideas of **deep learning** for computer vision — **convolutional neural networks** and **backpropagation** — were already well understood in **1989**. The **Long Short-Term Memory (LSTM)** algorithm, which is fundamental to deep learning for **timeseries**, was developed in **1997** and has barely changed since. So why did deep learning only take off after 2012? What changed in these two decades?\n",
    "\n",
    "In general, three technical forces are driving advances in machine learning:\n",
    "\n",
    "- Hardware\n",
    "- Datasets and benchmarks\n",
    "- Algorithmic advances\n",
    "\n",
    "Because the field is guided by experimental findings rather than by theory, algorithmic advances only become possible when appropriate data and hardware are available to try new ideas (or scale up old ideas, as is often the case). **Machine learning** isn’t mathematics or physics, where major advances can be done with a pen and a piece of paper. **It’s an engineering science.**\n",
    "\n",
    "The real **bottlenecks throughout the 1990s and 2000s** were **data** and **hardware**. But here’s what happened during that time: the internet took off, and **high-performance graphics chips were developed** for the needs of the gaming market.\n",
    "\n",
    "\n",
    "### 1.3.1. A new wave of investment\n",
    "\n",
    "**In 2011**, right before deep learning took the spotlight, the total venture capital investment in AI was around **19 million**, which went almost entirely to practical applications of shallow machine-learning approaches. **By 2014**, it had risen to a staggering **394 million**. Dozens of startups launched in these three years, trying to capitalize on the deep-learning hype. \n",
    "\n",
    "Meanwhile, large tech companies such as **Google**, **Facebook**, **Baidu**, and **Microsoft** have invested in internal research departments in amounts that would most likely dwarf the flow of venture-capital money. Only a few numbers have surfaced: **In 2013**, **Google acquired** the deep-learning startup **DeepMind** for a reported **500 million**—the largest acquisition of an AI company in history. **In 2014**, **Baidu** started a deep-learning research center in Silicon Valley, **investing 300 million** in the project. The deep-learning hardware **startup Nervana Systems** was acquired by **Intel** in **2016** for over **400 million.**\n",
    "\n",
    "Machine learning—in particular, deep learning—has become central to the product strategy of these tech giants. **In late 2015**, **Google CEO Sundar Pichai stated**, “Machine learning is a core, transformative way by which we’re rethinking how we’re doing everything. **We’re thoughtfully applying it across all our products**, be it search, ads, YouTube, or Play. And we’re in early days, but you’ll see us—in a systematic way—apply machine learning in all these areas.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83Z9dtRMwFfV"
   },
   "source": [
    "# 2 - Mathematical building blocks of neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yIPhrGnvpy9"
   },
   "source": [
    "## 2.1 - Introduction\n",
    "\n",
    "This section covers\n",
    "- A first example of a neural network\n",
    "- Tensors and tensor operations\n",
    "- How neural networks learn via backpropagation and gradient descent\n",
    "\n",
    "**Understanding deep learning** requires familiarity with many simple mathematical\n",
    "concepts: **tensors**, **tensor operations**, **differentiation**, **gradient descent**, and so on.\n",
    "Our goal in this section will be to build your intuition about these notions without\n",
    "getting overly technical. In particular, we’ll steer away from mathematical notation,\n",
    "which can be off-putting for those without any mathematics background and isn’t\n",
    "strictly necessary to explain things well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmbGwFRmvpy_"
   },
   "source": [
    "## 2.2 - A first look at a neural network\n",
    "\n",
    "Let’s look at a concrete example of a neural network that uses the [Python library Keras](https://keras.io/) to learn to **classify handwritten digits**. Unless you already have experience with Keras or similar libraries, you won’t understand everything about this first example right away.\n",
    "\n",
    "You probably haven’t even installed Keras yet (you can install in your machine even if it doesnt have a GPU):\n",
    "\n",
    ">```bash\n",
    "conda install -c conda-forge tensorflow keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXPss36avpzD"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "print('Using Keras version:', keras.__version__, \n",
    "      '\\nbackend:', K.backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdXjgmr5z5gg"
   },
   "source": [
    "If the backend is Tensorflow, we can display some further information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Upih9RkUz818"
   },
   "outputs": [],
   "source": [
    "if K.backend() == \"tensorflow\":\n",
    "    import tensorflow as tf\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name == '':\n",
    "        device_name = \"None\"\n",
    "    print('Using TensorFlow version:', tf.__version__,\n",
    "          ', GPU:', device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-I2YXs2Mqtc"
   },
   "outputs": [],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yikhZMinNKAG"
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "\n",
    "# get all GPU on machine\n",
    "GPUs = GPU.getGPUs()\n",
    "\n",
    "# colab give us only one GPU\n",
    "gpu = GPUs[0]\n",
    "\n",
    "def printm():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ),\n",
    "        \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "  print(\"GPU RAM Free: {0:.0f}MB  | Used: {1:.0f}MB | Util {2:3.0f}% |\\\n",
    "  Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, \n",
    "                                gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIJ-g739Xt0s"
   },
   "outputs": [],
   "source": [
    "# RSS - resident set size \n",
    "!ps -aux --sort -rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxuPrYmvvpzJ"
   },
   "source": [
    "The problem we’re trying to solve here is to **classify grayscale images of handwritten digits (28 × 28 pixels)** into their **10 categories (0 through 9)**. We’ll use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a classic in the machine-learning community, which has been around almost as long as the field itself and has been intensively studied. \n",
    "\n",
    "It’s a **set of 60,000 training images**, plus **10,000 test images**, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of **“solving” MNIST as the “Hello World” of deep learning** — it’s what you do to verify that your algorithms are working as expected. As you become a machine-learning practitioner, you’ll see MNIST come up over and over again, in scientific papers, blog posts, and so on.\n",
    "\n",
    "\n",
    "<img width=\"350\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1vlzxpafdf5r8mkcxeJ5tqgW6BhuLZIIZ\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zW6KEW2zvpzK"
   },
   "source": [
    "### 2.2.1 -  Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeVhsSOmvpzL"
   },
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset in Keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# The images are encoded as Numpy arrays, \n",
    "# and the labels are an array of digits, ranging\n",
    "# from 0 to 9. The images and labels have a one-to-one correspondence.\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFhjXzCIvpzO"
   },
   "outputs": [],
   "source": [
    "print(\"Type of train_images {}\\nType of train_labels {}\".format(type(train_images),type(train_labels)))\n",
    "print(\"\\nShape of train data: images {}, labels {}\".format(train_images.shape,train_labels.shape))\n",
    "print(\"Shape of test data: images {}, labels {}\".format(test_images.shape,test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDfpmXJaFXVi"
   },
   "outputs": [],
   "source": [
    "# first ten train labels\n",
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPUi3xkfvpzS"
   },
   "source": [
    "### 2.2.2 - The network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pL80RHRq-OH_"
   },
   "source": [
    "```python\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# The network architecture\n",
    "network = models.Sequential()\n",
    "\n",
    "# Hidden layer\n",
    "network.add(layers.Dense(512, \n",
    "                         activation='relu',\n",
    "                         input_shape=(28 * 28,)))\n",
    "\n",
    "# Output layer\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2CBfKzCvpzT"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# The network architecture\n",
    "network = models.Sequential()\n",
    "\n",
    "# Hidden layer\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "\n",
    "# Output layer\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJkyjRPEvpzW"
   },
   "source": [
    "To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
    "- **A loss function**: How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.\n",
    "- **An optimizer**: The mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "- **Metrics to monitor during training and testing**: Here, we’ll only care about accuracy (the fraction of the images that were correctly classified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smLsPWnHvpzX"
   },
   "source": [
    "### 2.2.3 - The compilation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nIX7cdjvpzY"
   },
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clMdhK5qvpzb"
   },
   "source": [
    "### 2.2.4 - Preparing data\n",
    "\n",
    "Before training, we’ll preprocess the data by **reshaping** it into the shape the network expects and scaling it so that **all values are in the [0, 1] interval**. Previously, our training images, for instance, were stored in an array of shape **(60000, 28, 28)** of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape **(60000, 28 * 28)** with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EA8zgRw-vpzc"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8oKguuOvpzg"
   },
   "source": [
    "### 2.2.5 - Preparing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cq-_dFCxvpzi"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqfoenPpF6QG"
   },
   "outputs": [],
   "source": [
    "# show the first ten train labels transformed to categorical\n",
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bN-1gn0Dvpzk"
   },
   "source": [
    "### 2.2.6 - Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rg9lieOovpzl"
   },
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKWRBcT4vpzp"
   },
   "source": [
    "Two quantities are displayed during training: the **loss of the network** over the training data, and the **accuracy of the network** over the training data. We quickly reach an accuracy of 0.9885 (98.9%) on the training data. Now let’s\n",
    "check that the model performs well on the test set, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8e4AM621pNBl"
   },
   "outputs": [],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yq2dPoB0vkus"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import IPython\n",
    "\n",
    "plot_model(network, to_file=\"model.png\", show_shapes=True, show_layer_names=True)\n",
    "IPython.display.Image(\"model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Naa4EWuTvpzr"
   },
   "source": [
    "### 2.2.7 - Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fB1uiLlDvpzs"
   },
   "source": [
    "The test-set accuracy turns out to be **97.8%** — that’s quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of **overfitting**: the fact that machine-learning models tend to perform worse on new data than on their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v777SZeUvpzs"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXvF5GD5vpzy"
   },
   "source": [
    "### 2.2.8 - Summarization\n",
    "\n",
    "This concludes our first example—you just saw how you can build and train a neural network to classify handwritten digits in less than 20 lines of Python code. The follow steps were performed:\n",
    "\n",
    "- Loading the data\n",
    "- Create the network architecture\n",
    "- Compilation\n",
    "- Preparing the data\n",
    "- Train the network \n",
    "- Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyB1coHGvpzz"
   },
   "source": [
    "## 2.3 - Data representations for neural networks\n",
    "\n",
    "In the previous example, we started from data stored in **multidimensional Numpy arrays**, also called **tensors**. \n",
    "\n",
    "So what’s a tensor?\n",
    "- At its core, a **tensor is a container for data** — almost always numerical data. So, it’s a\n",
    "container for numbers.\n",
    "- You may be already familiar with **matrices, which are 2D tensors**.\n",
    "- Tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors, a **dimension is often called an axis**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RNrVr3wvpz0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scalars (0D tensors) - a tensor that contains only one number is called a scalar\n",
    "scalar = np.array(12)\n",
    "print(\"0D tensor (dimension): {}\".format(scalar.ndim))\n",
    "\n",
    "# Vectors (1D tensors) - an array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly one axis.\n",
    "tensor1D = np.array([12, 3, 6, 14])\n",
    "print(\"1D tensor (dimension): {}\".format(tensor1D.ndim))\n",
    "\n",
    "# Matrices (2D tensors) - an array of vectors is a matrix, or 2D tensor. A matrix has two axes (rows and columns).\n",
    "tensor2D = np.array([[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]])\n",
    "print(\"2D tensor (dimension): {}\".format(tensor2D.ndim))\n",
    "\n",
    "# 3D tensors and higher-dimensional tensors\n",
    "# If you pack such matrices in a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers.\n",
    "tensor3D = np.array([[[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]],\n",
    "                      [[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]],\n",
    "                      [[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]]])\n",
    "print(\"3D tensor (dimension): {}\".format(tensor3D.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4n_2wmYrvpz6"
   },
   "source": [
    "### 2.3.1 - Real-world examples of data tensors\n",
    "\n",
    "Let’s make data tensors more concrete with a few examples similar to what you’ll encounter later. The data you’ll manipulate will almost always fall into one of the following categories:\n",
    "\n",
    "- **Vector**: data—2D tensors of shape **(samples, features)**\n",
    "- **Timeseries**: data or sequence data—3D tensors of shape **(samples, timesteps,features)**\n",
    "- **Images**: 4D tensors of shape **(samples, height, width, channels)** or **(samples,channels, height, width)**\n",
    "- **Video**: 5D tensors of shape **(samples, frames, height, width, channels)** or **(samples, frames, channels, height, width)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DcIX4TSvpz7"
   },
   "source": [
    "## 2.4 - The gears of neural networks: tensor operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om3XRiM1MiRs"
   },
   "source": [
    "\n",
    "All transformations learned by deep neural networks can be reduced to a handful of tensor operations applied to\n",
    "tensors of numeric data. For instance, it’s possible to **add tensors, multiply tensors**, **reshape tensors**, and so on.\n",
    "\n",
    "```python\n",
    "# this layer takes as input a 2D tensor and returns another 2D tensor\n",
    "keras.layers.Dense(512, activation='relu')\n",
    "```\n",
    "\n",
    "- Specifically, the function is as follows (where W is a 2D tensor and b is a vector, both attributes of the layer):\n",
    "\n",
    "```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "- We have three tensor operations here: \n",
    "    - a **dot product** (dot) between the input tensor and a tensor named W; \n",
    "        - Tensor product operation\n",
    "    - an **addition (+)** between the resulting 2D tensor and a vector b (1D); \n",
    "        - Broadcast operation\n",
    "    - a **relu** operation. relu(x) is max(x, 0).\n",
    "        - Element-wise operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ooW0f-nvvpz9"
   },
   "outputs": [],
   "source": [
    "''' Tensor product (similar to a matrix multiplication)\n",
    "(a, b, c, d) . (d,) -> (a, b, c)\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)\n",
    "'''\n",
    "\n",
    "tensor2D = np.array([[1,-1],\n",
    "                     [0,2]])\n",
    "tensor1D = np.array([1, 0])\n",
    "\n",
    "# Tensor product\n",
    "print(np.dot(tensor2D,tensor1D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hryvfoOkSTmU"
   },
   "outputs": [],
   "source": [
    "tensor2D = np.array([[1,-1],\n",
    "                     [0,2]])\n",
    "tensor1D = np.array([1, 0])\n",
    "\n",
    "# Element-wise operation\n",
    "print(tensor2D * tensor1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtBhPaS2vp0B"
   },
   "outputs": [],
   "source": [
    "''' Broadcast operation\n",
    "What happens with addition when the shapes \n",
    "of the two tensors being added differ?\n",
    "'''\n",
    "\n",
    "tensor2D = np.array([[1,-1],[0,2]])\n",
    "tensor1D = np.array([1, 0])\n",
    "\n",
    "tensor2D + tensor1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tR3ZV-4vp0F"
   },
   "outputs": [],
   "source": [
    "''' Element-wise operation \n",
    "tensor2D =  |1,-1|   ->   relu(tensor2D)   ->  |1,0|     \n",
    "            |0, 2|                             |0,2|\n",
    "'''\n",
    "\n",
    "tensor2D = np.array([[1,-1],[0,2]])\n",
    "# before relu()\n",
    "print(tensor2D)\n",
    "\n",
    "# after relu()\n",
    "print(np.maximum(tensor2D,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-ErtrBovp0I"
   },
   "source": [
    "## 2.5 - The engine of neural networks: gradient-based optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5YzLVaiMq_C"
   },
   "source": [
    "\n",
    "As you saw in the previous section, each neural layer from our first network example transforms its input data as follows:\n",
    "\n",
    ">```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "In this expression, **W** and **b** are tensors that are attributes of the layer. They’re called the **weights or trainable parameters** of the layer (**the kernel and bias attributes**, respectively). \n",
    "\n",
    "- These weights contain the **information learned** by the network from **exposure to training data**.\n",
    "- Initially, these **weight** matrices are filled with small random values (a step called **random initialization**). \n",
    "- **Gradually adjust** these weights, based on a feedback signal (also called **training**). A trainning loop:\n",
    "    1. Draw a **batch** of training samples x and corresponding targets y.\n",
    "    2. Run the network on x (a step called the **forward pass**) to **obtain predictions y_pred**.\n",
    "    3. **Compute the loss** of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "    4. **Update all weights** of the network in a way that slightly reduces the loss on this batch.\n",
    "    \n",
    "**Step 1** sounds easy enough—just I/O code. **Steps 2 and 3** are merely the application of a handful of tensor operations, so you could implement these steps purely from what you learned in the previous section. The difficult part is **step 4**: updating the network’s weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KT5cJKDlvp0J"
   },
   "source": [
    "### 2.5.1 - Question!!!\n",
    "Given an individual weight coefficient in the network, how can you compute whether the coefficient should be increased or decreased, and by how much?\n",
    "\n",
    "**One naive solution** would be to freeze all weights in the network except the one scalar coefficient being considered, and try different values for this coefficient. \n",
    "- Let’s say the initial value of the **coefficient is 0.3**. \n",
    "- After the forward pass on a batch of data, **the loss** of the network on the batch is **0.5**. \n",
    "- If you change the **coefficient’s value to 0.35** and rerun the forward pass, the **loss increases to 0.6**. \n",
    "- But if you lower the **coefficient to 0.25**, the **loss falls to 0.4**.\n",
    "- In this case, it seems that updating the coefficient by -0.05 would contribute to minimizing the loss. \n",
    "- This would have to be repeated for all coefficients in the network.\n",
    "\n",
    "But such an approach would be horribly inefficient, because you’d need to compute two forward passes (which are expensive) for every individual coefficient (of which there are many, usually thousands and sometimes up to millions). \n",
    "\n",
    "**A much better approach** is to take advantage of the fact that **all operations** used in the network are **differentiable**, and compute the gradient of the loss with regard to the network’s coefficients. You can then **move the coefficients** in the **opposite direction from the gradient**, thus **decreasing the loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_sW8b0Nvp0K"
   },
   "source": [
    "### 2.5.2 - What’s a derivative?\n",
    "\n",
    "Consider a continuous, smooth function $f(x)$, mapping a real number x to a new real number y. \n",
    "\n",
    "$$f(x) = y$$\n",
    "\n",
    "Because the function is continuous, a small change in $x$ can only result in a small change in $y$ — that’s the intuition behind continuity. Let’s say you increase $x$ by a small factor $epsilon\\_x$ will result in a small $epsilon\\_y$ change to $y$:\n",
    "\n",
    "$$f(x + epsilon\\_x) = y + epsilon\\_y$$\n",
    "\n",
    "In addition, because the function is *smooth* (its curve doesn’t have any abrupt angles), **when $epsilon\\_x$ is small enough**, around a certain point $p$, it’s possible to approximate $f$ as a linear function of slope $a$, so that $epsilon\\_y$ becomes $a * epsilon\\_x$:\n",
    "\n",
    "$$f(x + epsilon\\_x) = y + a * epsilon\\_x$$\n",
    "\n",
    "**The slope a** is called the **derivative** of $f$ in $p$. \n",
    "\n",
    "- If $a$ is negative, it means a small change of $x$ around $p$ will result in a decrease of $f(x)$\n",
    "- If f $a$ is positive, a small change in $x$ will result in an increase of $f(x)$. \n",
    "- Further, the absolute value of $a$ (the magnitude of the derivative) tells you how quickly this increase or decrease\n",
    "will happen.\n",
    "\n",
    "If you’re trying to update $x$ by a factor $epsilon\\_x$ in order to minimize $f(x)$, and you know the derivative of $f$, then your job is done: the derivative completely describes how $f(x)$ evolves as you change $x$. **If you want to reduce the value of $f(x)$, you just need to move $x$ a little in the opposite direction from the derivative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgPDysGfvp0K"
   },
   "source": [
    "### 2.5.3 Derivative of a tensor operation: the gradient\n",
    "\n",
    "A **gradient** is the **derivative of a tensor operation**. \n",
    "\n",
    "Consider:\n",
    "- input vector $x$\n",
    "- matrix $W$\n",
    "- target $y$\n",
    "- loss function. \n",
    "\n",
    "You can use $W$ to compute a target candidate $y\\_pred$, and compute the $loss$, or mismatch, between the target candidate $y\\_pred$ and the target $y$:\n",
    "\n",
    "$$\n",
    "y\\_pred = dot(W, x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "loss\\_value = loss(y\\_pred, y)\n",
    "$$\n",
    "\n",
    "If the data inputs $x$ and $y$ are frozen, then this can be interpreted as a function mapping values of $W$ to loss values:\n",
    "\n",
    "$$loss\\_value = f(W)$$\n",
    "\n",
    "Let’s say the current value of $W$ is $W0$. \n",
    "\n",
    "- The derivative of $f$ in the point $W0$ is a tensor $gradient(f)(W0)$ \n",
    "- Each coefficient $gradient(f) (W0)[i, j]$ indicates the direction and magnitude of the change in $loss\\_value$\n",
    "\n",
    "\n",
    "Thus, for a function $f(x)$, you can reduce the value of $f(x)$ by moving $x$ a little in the opposite direction from the derivative, with a function f(W) of a tensor, you can reduce f(W) by moving W in the opposite direction from the gradient: \n",
    "\n",
    "$$\n",
    "W1 = W0 - step * gradient(f)(W0)\n",
    "$$\n",
    "\n",
    "where step is a small scaling factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-w0rWLHWvp0K"
   },
   "source": [
    "### 2.5.4 - Stochastic gradient descent\n",
    "\n",
    "1. Draw a batch of training samples $x$ and corresponding targets $y$.\n",
    "2. Run the network on $x$ to obtain predictions $y\\_pred$.\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch between $y\\_pred$ and $y$.\n",
    "4. Compute the gradient of the loss with regard to the network’s parameters (a backward pass).\n",
    "5. Move the parameters a little in the opposite direction from the gradient \n",
    "    - $W -= step * gradient$\n",
    "    \n",
    "What I just described is called mini-batch stochastic gradient descent.\n",
    "\n",
    "Additionally, there exist multiple variants of SGD that differ by taking into account previous weight updates when computing the next weight update, rather than just looking at the current value of the gradients. There is, for instance, **SGD with momentum**, as well as **Adagrad**, **RMSProp**, and several others. **Such variants are known as optimization methods or optimizers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3XPbdgNvp0M"
   },
   "source": [
    "## 2.6 - Looking back at our first example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMwi_iiKMxU9"
   },
   "source": [
    "\n",
    "You’ve reached the end of this chapter, and you should now have a general understanding of what’s going on behind the scenes in a neural network. Let’s go back to the first example and review each piece of it in the light of what you’ve learned in the previous three sections.\n",
    "\n",
    "This was the input data:\n",
    "\n",
    ">```python\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "```\n",
    "\n",
    "Now you understand that the input images are stored in Numpy tensors, which are here formatted as float32 tensors of shape **(60000, 784) (training data)** and **(10000,784) (test data)**, respectively.\n",
    "\n",
    "This was our network:\n",
    "\n",
    ">```python\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "Now you understand that this network consists of a **chain of two Dense layers**, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. Weight tensors, which are attributes of the layers, are where the knowledge of the network persists.\n",
    "\n",
    "This was the network-compilation step:\n",
    "\n",
    ">```python\n",
    "network.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Now you understand that **categorical_crossentropy** is the loss function that’s used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimize. You also know that this **reduction of the loss happens via minibatch stochastic gradient descent**. The exact rules governing a specific use of gradient descent are defined by the **rmsprop optimizer** passed as the first argument.\n",
    "\n",
    "Finally, this was the training loop:\n",
    "\n",
    "```python\n",
    "network.fit(train_images, \n",
    "            train_labels, epochs=5, \n",
    "            batch_size=128)\n",
    "```\n",
    "\n",
    "Now you understand what happens when you call fit: the network will start to iterate on the training data in mini-batches of **128 samples, 5 times over** (each iteration over all the training data is called an epoch). At each iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. **After these 5 epochs, the network will have performed 2,345 gradient updates (469 per epoch - 60k/128)**, and the loss of the network will be sufficiently low that the network will be capable of classifying handwritten digits with high accuracy.\n",
    "\n",
    "**At this point, you already know most of what there is to know about neural networks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJFy4xbiCnOl"
   },
   "source": [
    "# 3 - Getting started with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31U8ivDDCSsw"
   },
   "source": [
    "## 3.1 - Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlFwkVWpDiK1"
   },
   "source": [
    "\n",
    "This section covers\n",
    "- **Core** components of **neural networks**\n",
    "- An introduction to **Keras**\n",
    "- Using neural networks to solve basic classification and regression problems\n",
    "    - Classifying movie reviews as positive or negative (**binary classification**)\n",
    "    - Classifying news wires by topic (**multiclass classification**)\n",
    "    - Estimating the price of a house, given real-estate data (**regression**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGu2LsGNCSsx"
   },
   "source": [
    "## 3.2 - Anatomy of a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKPS_NGSDgOe"
   },
   "source": [
    "\n",
    "As you saw in the previous chapters, training a neural network revolves around the following objects:\n",
    " - **Layers**, which are combined into a network (or model)\n",
    " - The **input data** and corresponding **targets**\n",
    " - The **loss function**, which defines the **feedback signal used for learning**\n",
    " - The **optimizer**, which determines **how learning proceeds**\n",
    " \n",
    " <img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1CFWZALw9MnJvStvh8VkCAZFiVLs7E4lW\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQWM6DtICSsy"
   },
   "source": [
    "### 3.2.1 - Layers: the building blocks of deep learning\n",
    "\n",
    "A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Different layers are appropriate for different tensor formats and different types of data processing.\n",
    "\n",
    "- **fully connected or dense layers**\n",
    "    - simple vector data, stored in 2D tensors of shape\n",
    "- **recurrent layers such as an LSTM layer**\n",
    "    - Sequence data, stored in 3D tensors of shape\n",
    "- **2D convolution layers (Conv2D)**\n",
    "    - image data, stored in 4D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbIqA5hSCSsz"
   },
   "source": [
    "### 3.2.2 -  Loss functions and optimizers: keys to configuring the learning process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqZRNY9MDp2u"
   },
   "source": [
    "\n",
    "Once the network architecture is defined, you still have to choose two more things:\n",
    "- **Loss function** (objective function) — The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "- **Optimizer** — Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).\n",
    "\n",
    "**A neural network that has multiple outputs may have multiple loss functions (one per output)**. But the gradient-descent process must be based on a single scalar loss value; so, for multiloss networks, all losses are combined (via averaging) into a single scalar quantity.\n",
    "\n",
    "Choosing the right objective function for the right problem is extremely important.\n",
    "\n",
    "- **binary crossentropy**\n",
    "    - two-class classification\n",
    "- **categorical crossentropy**\n",
    "    - many-class classification problem\n",
    "- **mean squared error** \n",
    "    - regression problem\n",
    "- **connectionist temporal classification (CTC)**\n",
    "    - sequence-learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWrSppYFCSs0"
   },
   "source": [
    "## 3.3 - Introduction to Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mcn4JNeD0RC"
   },
   "source": [
    "\n",
    "[Keras](https://keras.io/) is a **deep-learning framework for Python** that provides a convenient way to define and train almost any kind of deep-learning model.\n",
    "\n",
    "Keras has the following key features:\n",
    "- It allows the same code to run seamlessly on CPU or GPU.\n",
    "- It has a user-friendly API that makes it easy to quickly prototype deep-learning models.\n",
    "- It has built-in support for convolutional networks (for computer vision), recurrent\n",
    "networks (for sequence processing), and any combination of both.\n",
    "- It supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, and so on. \n",
    "\n",
    "\n",
    "Keras is appropriate for building essentially any **deep-learning model**, from a **generative adversarial network**\n",
    "to a **neural Turing machine**.\n",
    "\n",
    "Keras is distributed under the permissive MIT license, which means **it can be freely used in commercial projects**. It’s compatible with any version of Python from 2.7 to 3.6 (as of december 2018).\n",
    "\n",
    "\n",
    "**Keras is a model-level library**, providing high-level building blocks for developing deep-learning models. **It doesn’t handle low-level operations** such as tensor manipulation and differentiation. Instead, **it relies on a specialized, well-optimized tensor library** to do so, serving as the backend engine of Keras.\n",
    "\n",
    "<img width=\"300\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1XeOgdVuT35U9ahTWwO5vfW6EiUlklrql\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5sAF1n6CSs1"
   },
   "source": [
    "## 3.4 - Classifying movie reviews: a binary classification example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sl5q1TDWD5TZ"
   },
   "source": [
    "\n",
    "**Two-class classification**, or **binary classification**, may be the most widely applied kind of machine-learning problem. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYRxNvAMD-xg"
   },
   "source": [
    "\n",
    "### 3.4.1 - IMDB dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isf1rxsKEA49"
   },
   "source": [
    "\n",
    "You’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the\n",
    "Internet Movie Database. They’re split into:\n",
    "\n",
    "- 25,000 reviews for training\n",
    "- 25,000 reviews for testing\n",
    "\n",
    "each set consisting of 50% negative and 50% positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xeQLZrtqCSs2"
   },
   "outputs": [],
   "source": [
    "# loading the IMDB dataset\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# num_words=10000 means you’ll only keep the \n",
    "# top 10,000 most frequently occurring words\n",
    "#in the training data.\n",
    "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6e7JvEBCSs7"
   },
   "outputs": [],
   "source": [
    "# The variables train_data and test_data are lists of reviews; \n",
    "# each review is a list of word indices (encoding a sequence of words)\n",
    "print(train_data[0][:20])\n",
    "print(len(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXdrtICVCSs_"
   },
   "outputs": [],
   "source": [
    "# train_labels and test_labels are lists of 0s and 1s, \n",
    "# where 0 stands for negative and 1 stands for positive\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KsYcZt3CStD"
   },
   "outputs": [],
   "source": [
    "# Because you’re restricting yourself to the top 10,000\n",
    "# most frequent words, no word index will exceed 10,000\n",
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMbcasX-CStG"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# how to decode one sequence back to english\n",
    "\n",
    "# get all word indexes ('word': id)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# get a new dict (id: 'word')\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Note that the indices are offset by 3 because 0, 1, and 2 are reserved \n",
    "# indices for “padding,” “start of sequence,” and “unknown.”\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') \n",
    "                           for i in train_data[1]])\n",
    "\n",
    "\n",
    "print(\"\\n\".join(textwrap.wrap(decoded_review,90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ysieC0-CStK"
   },
   "source": [
    "### 3.4.2 Preparing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slT2jiNfEI8n"
   },
   "source": [
    "\n",
    "**You can’t feed lists of integers into a neural network**. You have to turn your lists into tensors. There are two ways to do that:\n",
    "- Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices)\n",
    "- One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WED5aaVoCStL"
   },
   "outputs": [],
   "source": [
    "# Encoding the integer sequences into a binary matrix (25000,10000)\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Di_1rq5CStO"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m27UsnyJCStS"
   },
   "outputs": [],
   "source": [
    "# You should also vectorize your labels, which is straightforward\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLqY_5vnCStU"
   },
   "source": [
    "### 3.4.3 - Building your network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dw7yCzpQENmY"
   },
   "source": [
    "\n",
    "- The input data is vectors\n",
    "- The labels are scalars (1s and 0s)\n",
    "\n",
    "A type of network that performs well on such a problem is **a simple stack of fully connected (Dense) layers** with relu activations \n",
    "\n",
    "```python\n",
    "Dense(16,activation='relu')\n",
    "```\n",
    "\n",
    "The argument being passed to each Dense layer (16) is the number of hidden units of the layer. **A hidden unit is a dimension in the representation space of the layer**. Remember that each such Dense layer with a relu activation implements the following chain of tensor operations:\n",
    "\n",
    "```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "Having 16 hidden units means the weight matrix W will have shape (input_dimension,16): the dot product with W will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector b and apply the relu operation). \n",
    "\n",
    "You can intuitively understand the dimensionality of your representation space as **“how much freedom you’re allowing the network to have when learning internal representations.”** \n",
    "\n",
    "- Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations\n",
    "- But, having more hidden units make the network more computationally expensive and may lead to **learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).**\n",
    "\n",
    "\n",
    "There are two **key architecture decisions** to be made about such a stack of Dense layers:\n",
    "- How many layers to use\n",
    "- How many hidden units to choose for each layer\n",
    "\n",
    "In the next chapter, you’ll learn formal principles to guide you in making these choices. For the time being, you’ll have to trust me with the following architecture choice:\n",
    "\n",
    "- Two intermediate layers with 16 hidden units each\n",
    "- A third layer that will output the scalar prediction regarding the sentiment of the current review\n",
    "\n",
    "The **intermediate layers will use relu** as their activation function, and the **final layer will use a sigmoid** activation so as to output a probability (a score between 0 and 1).\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1tGaZcNRpAmDaSEAOGCKGhH9QtPHlbDUi\" width=\"150\"> </td>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1ku-YktP--VeodXH1oaUPPtkvgfSVclm7\" width=\"300\"> </td>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1LoV52worh3m-S9uL4YvyDm2NVOcVDvaJ\" width=\"300\"> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4Hlr5gMCStU"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# the model definition\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMr94WBfCStY"
   },
   "source": [
    "### 3.4.4 - Validating your approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyagyyncEVaK"
   },
   "source": [
    "\n",
    "In order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJAGUol0CStY"
   },
   "outputs": [],
   "source": [
    "# Setting aside a validation set\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMm-Bs6XCSta"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Training your model\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Duration: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3OjhieJCSte"
   },
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "# trainning loss X validation loss\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# trainning accuracy X validation accuracy\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax1.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label='Validationb loss')\n",
    "ax1.set_title('Training and validation loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(epochs)\n",
    "\n",
    "ax2.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "ax2.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "ax2.set_title('Training and validation accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2_mEdaPCSti"
   },
   "source": [
    "- the training loss decreases with every epoch\n",
    "- the training accuracy increases with every epoch\n",
    "\n",
    "That’s what you would expect when running gradient descent optimization — the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is **overfitting**: after the second epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgPv1obBCStj"
   },
   "outputs": [],
   "source": [
    "# this approach achieves an accuracy of 85%. \n",
    "# With state-of-the-art approaches, \n",
    "# you should be able to get close to 95%\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fG6dd4FCStp"
   },
   "source": [
    "### 3.4.5 - Using a trained network to generate predictions on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FX61vyezEaSm"
   },
   "source": [
    "\n",
    "After having trained a network, you’ll want to use it in a practical setting. You can generate the likelihood of reviews being positive by using the predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfYKrBWSCStq"
   },
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "giTOA3tfCStw"
   },
   "outputs": [],
   "source": [
    "model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iD5lR0cQCSty"
   },
   "source": [
    "### 3.4.6 - Further experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geLxr67mErus"
   },
   "source": [
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    "\n",
    "\n",
    "The following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement:\n",
    "1. You used two hidden layers. **Try using one or three hidden layers**, and see how doing so affects validation and test accuracy.\n",
    "2. Try using layers with more **hidden units** or fewer hidden units: **32 units, 64 units**, and so on.\n",
    "3. Try using the **mse loss** function instead of binary_crossentropy.\n",
    "4. Try using the **tanh activation** (an activation that was popular in the early days of neural networks) instead of relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uT9tdx3DcKrs"
   },
   "outputs": [],
   "source": [
    "# parameters to be evaluated\n",
    "hidden_units = [16,32,64]\n",
    "activations_funct = ['relu','tanh']\n",
    "loss_funct = ['binary_crossentropy', 'mean_squared_error']\n",
    "training = []\n",
    "results = []\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NotVwltHCSty"
   },
   "source": [
    "#### 3.4.6.1 - Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CKfGnzRCStz"
   },
   "outputs": [],
   "source": [
    "# parameters to be evaluated\n",
    "\n",
    "hidden_units = [16,32,64]\n",
    "activations_funct = ['relu','tanh']\n",
    "loss_funct = ['binary_crossentropy', \n",
    "              'mean_squared_error']\n",
    "training = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17e1sOYWCSt3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# the model definition\n",
    "# layer_1,layer_2\n",
    "\n",
    "for hidden in hidden_units:\n",
    "    for activations in activations_funct:\n",
    "        for losses in loss_funct:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden, activation=activations, \n",
    "                                   input_shape=(10000,)))\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "            # compile the model\n",
    "            model.compile(optimizer='rmsprop',loss=losses,metrics=['accuracy'])\n",
    "            \n",
    "            # Training your model\n",
    "            history = model.fit(partial_x_train,\n",
    "                                partial_y_train,\n",
    "                                epochs=20,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(x_val, y_val))\n",
    "            training.append(history)\n",
    "            results.append(model.evaluate(x_test, y_test))\n",
    "            \n",
    "            \n",
    "end = time.time()\n",
    "\n",
    "print(\"Duration: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65r1MiU3CSt6"
   },
   "source": [
    "| Model           | Hidden Unit | Loss Function | Evaluation (accuracy) |\n",
    "|-----------------|-------------|---------------|-----------------------|\n",
    "| **relu,sigmoid(1)** | **16**          | **binary_cross**  |       ** 0.8574  **             |\n",
    "| relu,sigmoid(1) | 16          | mse           |        0.8551             |\n",
    "| tanh,sigmoid(1) | 16          | binary_cross  |        0.8513               |\n",
    "| tanh,sigmoid(1) | 16          | mse           |        0.8515               |\n",
    "| relu,sigmoid(1) | 32          | binary_cross  |         0.8500              |\n",
    "| relu,sigmoid(1) | 32          | mse           |         0.8538              |\n",
    "| tanh,sigmoid(1) | 32          | binary_cross  |          0.8460             |\n",
    "| tanh,sigmoid(1) | 32          | mse           |          0.8469             |\n",
    "| relu,sigmoid(1) | 64          | binary_cross  |         0.8470              |\n",
    "| relu,sigmoid(1) | 64          | mse           |          0.8502             |\n",
    "| tanh,sigmoid(1) | 64          | binary_cross  |         0.8464              |\n",
    "| tanh,sigmoid(1) | 64          | mse           |         0.7906              |\n",
    "\n",
    "The best accuary was 0.8575. \n",
    "\n",
    "- For all results the activation function relu was better than tanh\n",
    "- 16 hidden units was a better choice when compared to 32 and 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKHT5tWMCSuM"
   },
   "source": [
    "### 3.4.7 -  Wrapping up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iox3Z-7SFETr"
   },
   "source": [
    "\n",
    "Here’s what you should take away from this example:\n",
    "- You usually need to do quite a bit of **preprocessing on your raw data** in order to be able to feed it—as tensors—into a neural network. \n",
    "- Stacks of Dense layers with **relu activations can solve a wide range of problems** and you’ll likely use them frequently.\n",
    "- In a **binary classification** problem (two output classes), your network should **end with a Dense layer with one unit and a sigmoid activation**: the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "- The **rmsprop optimizer** is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.\n",
    "- As they get better on their training data, neural networks eventually start **overfitting** and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of\n",
    "the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCS5AHjDCSuN"
   },
   "source": [
    "## 3.5 - Classifying newswires:a multiclass classification example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzZK_hEq2A_T"
   },
   "source": [
    "\n",
    "In the previous section, you saw how to classify vector inputs into two mutually exclusive classes using a densely connected neural network. **But what happens when you have more than two classes?**\n",
    "\n",
    "In this section, you’ll build a network to **classify Reuters newswires into 46 mutually exclusive topics**. Because you have many classes, this problem is an instance of multiclass classification; and because each data point should be classified into only one category, the problem is more specifically an instance of **single-label, multiclass classification**. If each data point could belong to multiple categories (in this case, topics), you’d be facing a **multilabel, multiclass classification problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jzhEqAlCSuO"
   },
   "outputs": [],
   "source": [
    "# Loading the Reuters dataset\n",
    "\n",
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RH7dudbgCSuR"
   },
   "outputs": [],
   "source": [
    "# 8982 train data samples\n",
    "print(len(train_data))\n",
    "\n",
    "# 2246 test data samples\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u954mydbCSuT"
   },
   "outputs": [],
   "source": [
    "# print a train data sample\n",
    "print(train_data[3][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkjiH7qvCSuV"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Decoding newswires back to text\n",
    "word_index = reuters.get_word_index()\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') \n",
    "                             for i in train_data[3]])\n",
    "\n",
    "print(\"\\n\".join(textwrap.wrap(decoded_newswire,90)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RM_Y8KCmCSuZ"
   },
   "outputs": [],
   "source": [
    "# Encoding the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qn6c4thyCSua"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56eD_SrMCSuc"
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', \n",
    "                       input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KV8Mh-K8CSud"
   },
   "source": [
    "There are two other things you should note about this architecture:\n",
    "\n",
    "- **You end the network with a Dense layer of size 46**. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "- The **last layer uses a softmax activation**. You saw this pattern in the MNIST example. It means the network will output a probability distribution over the 46 different output classes—for every input sample, the network will produce a 46-\n",
    "dimensional output vector, where output[i] is the probability that the sample belongs to class i. **The 46 scores will sum to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6ty6VvACSue"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1GQa8LLCSug"
   },
   "source": [
    "**The best loss function to use in this case is categorical_crossentropy**. It measures the distance between two probability distributions: here, between the probability distribution output by the network and the true distribution of the labels. By minimizing the distance between these two distributions, you train the network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xEk7DSSCSuh"
   },
   "outputs": [],
   "source": [
    "# Setting aside a validation set\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcBhRMwQCSuk"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y23G-xnQCSuw"
   },
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "# trainning loss X validation loss\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# trainning accuracy X validation accuracy\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax1.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label='Validationb loss')\n",
    "ax1.set_title('Training and validation loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(epochs)\n",
    "\n",
    "ax2.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "ax2.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "ax2.set_title('Training and validation accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "caEsQkg3CSu1"
   },
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "model.evaluate(x_test,one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ju9WZPhxCSu6"
   },
   "outputs": [],
   "source": [
    "# Generating predictions for new data\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Each entry in predictions is a vector of length 46\n",
    "print(predictions[0].shape)\n",
    "\n",
    "# The coefficients in this vector sum to 1\n",
    "print(np.sum(predictions[0]))\n",
    "\n",
    "# The largest entry is the predicted class—the class with the highest probability\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXYacmXrCSu8"
   },
   "source": [
    "### 3.5.1 - A different way to handle the labels and the loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVVrhmFJ4brG"
   },
   "source": [
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like this:\n",
    "\n",
    ">```python\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "```\n",
    "\n",
    "The only thing this approach would change is the choice of the loss function. The loss function categorical_crossentropy, expects the labels to follow a categorical encoding. With integer labels, you should use sparse_categorical_crossentropy.\n",
    "\n",
    ">```python\n",
    "model.compile(optimizer='rmsprop',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "```\n",
    "\n",
    "This new loss function is still mathematically the same as categorical_crossentropy; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cUSucbxCSu9"
   },
   "source": [
    "###  3.5.2 -  The importance of having sufficiently large intermediate layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rhom5YqW4erA"
   },
   "source": [
    "\n",
    "We mentioned earlier that because the final outputs are 46-dimensional, you should avoid intermediate layers with many fewer than 46 hidden units. Now let’s see what happens when you introduce an information bottleneck by having intermediate layers\n",
    "that are significantly less than 46-dimensional: for example, 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_QZFHlCSu9"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGCDfmTWCSvA"
   },
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "model.evaluate(x_test,one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQxshU3BCSvC"
   },
   "source": [
    "**The network now peaks at ~70% validation accuracy, an 8% absolute drop**. This drop is mostly due to the fact that you’re trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8b8nq2ACSvC"
   },
   "source": [
    "### 3.5.3 - Further experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5kl3EK54hl2"
   },
   "source": [
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
    "\n",
    "- Try using larger or smaller layers: 32 units, 128 units, 256, and so on.\n",
    "- You used two hidden layers. Now try using a single hidden layer, three hidden layers or four hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0IBCIVoCSvD"
   },
   "outputs": [],
   "source": [
    "# parameters to be evaluate\n",
    "\n",
    "hidden_units = [32,64,128,256]\n",
    "activations_funct = ['relu']\n",
    "loss_funct = ['categorical_crossentropy']\n",
    "training = []\n",
    "results = []\n",
    "\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6dgceJ_CSvX"
   },
   "source": [
    "### 3.5.4 - Wrapping up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqu6ngX74mES"
   },
   "source": [
    "\n",
    "Here’s what you should take away from this example:\n",
    "\n",
    "1. If you’re trying to **classify data points among N classes**, your network should **end with a Dense layer of size N**.\n",
    "2. In a **single-label, multiclass classification** problem, your network should **end with a softmax** activation so that it will output a probability distribution over the N output classes.\n",
    "3. **Categorical crossentropy is almost always the loss function you should use for such problems**. It minimizes the distance between the probability distributions output by the network and the true distribution of the targets.\n",
    "4. There are two ways to handle labels in multiclass classification:\n",
    "    - Encoding the labels via **categorical encoding** (also known as one-hot encoding) and using **categorical_crossentropy** as a loss function\n",
    "    - **Encoding the labels as integers** and using the **sparse_categorical_crossentropy** loss function\n",
    "5. If you need to classify data into a large number of categories, you should avoid creating information bottlenecks in your network due to intermediate layers that are too small (**less than 46 in this example**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u__pwflhSth8"
   },
   "source": [
    "## 3.6 -  Predicting house prices: a regression example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AyiafGyTEe6"
   },
   "source": [
    "\n",
    "The two previous examples were considered classification problems, where the goal was to predict a single discrete label of an input data point. Another common type of machine-learning problem is **regression**, which consists of predicting a continuous\n",
    "value instead of a discrete label: for instance, predicting the temperature tomorrow, given meteorological data; or predicting the time that a software project will take to complete, given its specifications.\n",
    "\n",
    "> Don’t confuse regression and the algorithm logistic regression. Confusingly, **logistic regression isn’t a regression algorithm—it’s a classification algorithm**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6lfRsikTQmO"
   },
   "source": [
    "\n",
    "### 3.6.1 -  The Boston Housing Price dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FSJZG1XTSoc"
   },
   "source": [
    "\n",
    "- You’ll attempt to predict the median price of homes in a given **Boston suburb in the mid-1970s** given data points about the suburb at the time, such as the **crime rate, the local property tax rate**, and so on. \n",
    "- The dataset has relatively **few data points**: only 506, split between 404 training samples and 102 test samples. \n",
    "- Each **feature** in the input data (for example, the crime rate) has a **different scale**. \n",
    "- For instance, some values are proportions, which take values between 0 and 1; others take values between 1 and 12, others between 0 and 100, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PU4m3gNKSth-"
   },
   "outputs": [],
   "source": [
    "# Loading the Boston housing dataset\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# Training sample size\n",
    "print(train_data.shape)\n",
    "\n",
    "# Test sample size\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Cku_jT-Sth_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Train targets x  U$ 1,000 - value are between 5.0 and 50.0\n",
    "print(np.min(train_targets))\n",
    "print(np.max(train_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrzGxBsDStiB"
   },
   "source": [
    "### 3.6.2 - Preparing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FMqPsmOT0Pm"
   },
   "source": [
    "\n",
    "**It would be problematic to feed into a neural network** values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do **feature-wise normalization**: for each feature in the input data (a column in the input data matrix), you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in [Scikit-learn](http://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "\n",
    "The [**preprocessing module**](http://scikit-learn.org/stable/modules/preprocessing.html) further provides a utility **class StandardScaler** that implements the Transformer API to compute the **mean** and **standard deviation** on a **training set** so as to be able to **later reapply the same transformation on the testing set**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAnGXhDNStiC"
   },
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# create a scaler to fit train_data\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "\n",
    "# feature-wise normalization over train_data\n",
    "train_data_scaled = scaler.transform(train_data)\n",
    "\n",
    "# Scaled train data has zero mean and unit variance\n",
    "print(train_data_scaled.mean(axis=0))\n",
    "print(train_data_scaled.std(axis=0))\n",
    "\n",
    "# Note that the quantities used for normalizing the \n",
    "# test data are computed using the training data. \n",
    "# You should never use in your workflow any quantity \n",
    "# computed on the test data, \n",
    "# even for something as simple as data normalization.\n",
    "test_data_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTneghaDStiE"
   },
   "source": [
    "###  3.6.3 -  Building your network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2CQIexQW7zb"
   },
   "source": [
    "\n",
    "Because so few samples are available, you’ll use a very small network with two hidden layers, each with 64 units. **In general, the less training data you have, the worse overfitting will be**, and using a small network is one way to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0P3ZD9bStiF"
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data_scaled.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    \n",
    "    # The network ends with a single unit and no activation (it will be a linear layer). \n",
    "    # This is a typical setup for scalar regression (a regression where you’re trying \n",
    "    # to predict a single continuous value).\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # compile the network with the mse loss function—mean squared error,\n",
    "    # the square of the difference between the predictions and the targets. \n",
    "    # This is a widely used loss function for regression problems.\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # You’re also monitoring a new metric during training: mean absolute error (MAE). \n",
    "    # It’s the absolute value of the difference between the predictions and the targets.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbENldYtStiH"
   },
   "source": [
    "### 3.6.4 -  Validating your approach using K-fold validation\n",
    "\n",
    "- To evaluate your network while you keep adjusting its parameters (such as the number of epochs used for training), you could **split the data into a training set and a validation set**\n",
    "- Because you have so **few data points**, the validation set would end up being very small.\n",
    "- The best practice in such situations is to use **K-fold cross-validation**.\n",
    "    - It consists of splitting the available data into K partitions (typically K = 4 or 5)\n",
    "    - Instantiating K identical models, and training each one on K – 1 partitions while evaluating on the remaining partition. \n",
    "    - The **validation score** for the model used is then the **average of the K validation scores obtained**. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6PabpzMStiI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define k-fold cross validation test\n",
    "kf = KFold(n_splits=4, random_state=seed)\n",
    "\n",
    "all_mae_histories = []\n",
    "\n",
    "for train, test in kf.split(train_data_scaled, train_targets):\n",
    "    # create model\n",
    "    model = build_model()\n",
    "    history = model.fit(train_data_scaled[train], \n",
    "              train_targets[train], \n",
    "              epochs=100, \n",
    "              batch_size=1, \n",
    "              verbose=1,\n",
    "              validation_data=(train_data_scaled[test], \n",
    "                               train_targets[test]))\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JE3Hj4QNStiL"
   },
   "outputs": [],
   "source": [
    "# mae for each k-fold step\n",
    "np.mean(all_mae_histories,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NF5NuHxStiO"
   },
   "source": [
    "The different runs do indeed show rather different validation scores, from 2.1 to 2.7. The average (2.5) is a much more reliable metric than any single score - that’s the entire point of K-fold cross-validation. In this case, you’re off by  US2,500 on average, which is significant considering that the prices range from US5,000 to US50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5f7lgq1fStiO"
   },
   "outputs": [],
   "source": [
    "np.mean(all_mae_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7BkkaL9StiQ"
   },
   "outputs": [],
   "source": [
    "# Building the history of successive mean K-fold validation scores\n",
    "num_epochs = 100\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) \n",
    "                       for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SB8Puz-wStiR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sND8CST0StiU"
   },
   "outputs": [],
   "source": [
    "epochs_min = np.argmin(average_mae_history)\n",
    "print('Minimum MAE: {:.4f}\\nEpoch: {:d}'.format(average_mae_history[epochs_min],epochs_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYt5KsQrStiY"
   },
   "outputs": [],
   "source": [
    "# Training the final model\n",
    "\n",
    "model = build_model()\n",
    "model.fit(train_data_scaled, \n",
    "          train_targets,\n",
    "          epochs=76, \n",
    "          batch_size=1, \n",
    "          verbose=0)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data_scaled, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuZ3wVl6Stia"
   },
   "outputs": [],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7dekHP9Stid"
   },
   "source": [
    "### 3.6.5 - Wrapping up\n",
    "\n",
    "Here’s what you should take away from this example:\n",
    "\n",
    "- **Mean squared error (MSE)** is a **loss function** commonly used for **regression**.\n",
    "- A common **regression metric** is **mean absolute error (MAE)**.\n",
    "- When **features** in the input data have values in **different ranges**, each feature should be **scaled independently** as a preprocessing step.\n",
    "- When there is **little data available, using K-fold validation** is a great way to reliably evaluate a model.\n",
    "- When **little training data** is available, it’s preferable to use a **small network with few hidden layers** (typically only one or two), in order to avoid severe overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Deep Learning Fundamentals II.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
