{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Representing Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous few lessons, we explored the following machine learning models in depth:\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Bz9MLQ0bAD6J5YQIClImSH8DsMFw8P7O\">\n",
    "\n",
    "Because these are all supervised machine learning models, we repeated a similar process when using each model:\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1irRyfFn0GkxXQE0Nf9I72d46oi_I2y1F\">\n",
    "\n",
    "In this lesson, we'll add a new type of model to our toolbelt called **artificial neural networks**. Artificial neural networks (or neural networks for short) draw some inspiration from [biological neural networks](https://en.wikipedia.org/wiki/Neural_circuit), which describe how the cells in most brains (including ours) are connected and how they work together. Each cell in a neural network is called a neuron and is connected to multiple neurons. Neurons in human (and mammalian) brains communicate by sending electrical signals between each other.\n",
    "\n",
    "<img width=\"900\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Sv-FzVZ2OPhPd1_KiqVBNYgOOHZdtq4E\">\n",
    "\n",
    "Neural network models were inspired by the structure of neurons in our brain and message passing between neurons, but the similarities between how biological neural networks and artificial neural networks end here.\n",
    "\n",
    "A **deep neural network** is a specific type of neural network that excels at capturing nonlinear relationships in data. Deep neural networks have broken many benchmarks in audio and image classification. Previously, linear models were often used with nonlinear transformations that were discovered by hand through research.\n",
    "\n",
    "<img width=\"800\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Pu5kxAyn3-vMY-NJ92lkcsJ9wA5sXnzc\">\n",
    "\n",
    "Deep neural networks have some ability to discover how to structure the nonlinear transformations during the training process automatically and have grown to become a helpful tool for many problems.\n",
    "\n",
    "In the first part of this lesson, we'll focus on becoming familiar with how **neural networks** are represented and how to represent **linear regression** and **logistic regression** models in that representation. In later sections, we'll learn how to introduce **nonlinearity** in our networks, how to fit complex neural networks, and some real world **best practices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural networks** are usually represented as **graphs**. A graph is a data structure that consists of **nodes** (represented as circles) that are connected by **edges** (represented as lines).\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=16Mo9QntoF9jVptcWjsAPubaYtxanWvze\">\n",
    "\n",
    "Graphs are commonly used to represent how components of a system are related or linked. For example, the [Facebook Social Graph](https://en.wikipedia.org/wiki/Social_graph) describes how all of the users on Facebook are connected to each other (and this graph is changing constantly as friends are added and removed). Google Maps uses graphs to represent locations in the physical world as nodes and roads as edges.\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1EPKmyDL_W4r6GuC2oVKxobyw6Glx0TMg\">\n",
    "\n",
    "Graphs are a highly flexible data structure; you can even represent a list of values as a graph. Graphs are often categorized by their properties, which act as constraints. You can read about the many different ways graphs can be categorized on [Wikipedia](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29#Types_of_graphs).\n",
    "\n",
    "One way graphs can be categorized is the presence of edge direction.\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1PaiEzXZpvo15dK9Z5X1_0n75n6CbmpY7\">\n",
    "\n",
    "Within directed graphs, graphs are either cyclic or acyclic.\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1eQsM0eMKZ8dgW0_L2V9N-v2YVE0i-0tL\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs provide a mental model for thinking and reasoning about a specific class of models -- those that consist of a series of functions that are executed in a specific order. In the context of neural networks, graphs let us compactly express a pipeline of functions that we want to be executed in succession. Later in this lesson, we'll explore the math behind the following example:\n",
    "\n",
    "<img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1GO38dSY4Ere8lg6BqQFOwOx8dVa2_NSg\">\n",
    "\n",
    "This pipeline has 2 stages of functions that happen in sequence:\n",
    "\n",
    "- In the first stage, $L1$ is computed: $L_1 = Xa_1^T$\n",
    "- In the second stage, $L2$ is computed: $L_2 = L_1a_2^T$\n",
    "\n",
    "The second stage can't happen without the first stage, because $L_1$ is an input to the second stage. As we'll learn in the next lessons, at the heart of **neural network** models is the successive computation of functions. This is known as a **computational graph**. A computational graph uses **nodes** to describe **variables** and **edges** to describe **how variables are combined**. Here's a simple example:\n",
    "\n",
    "<img width=\"150\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1GpFyPVyGXZKyTZK7wwoPi-ut0r3x8T1t\">\n",
    "\n",
    "The **computational graph** is a powerful representation, as it allows us to compactly represent models with many layers of nesting. In fact, a **decision tree** is really a specific type of computational graph. There's no compact way to express a decision tree model using just equations and standard algebraic notation.\n",
    "\n",
    "<img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1zpPbFAgTMD81eydwmMSyi0ORku6ZYfxu\">\n",
    "\n",
    "To get more familiar with this representation, **we'll represent a linear regression model using neural network notation**. This will help you get more comfortable with this unique representation and allow us to explore some of the neural network terminology as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Network That Performs Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, a linear regression model is represented as:\n",
    "    \n",
    "$$\n",
    "\\hat{y} = a_0 + a_1x_1 + a_2x_2 + \\ldots + a_nx_n\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $a_0$ represents the intercept (also known as the **bias**)\n",
    "- $a_1$ to $a_n$ represent the trained model weights\n",
    "- $x_1$ to $x_n$  represent the features\n",
    "- $\\hat{y}$ represents the predicted value\n",
    "\n",
    "The first step is to rewrite this model using linear algebra notation, as a product of two vectors:\n",
    "\n",
    "$$\n",
    "Xa^T = \\hat{y}\n",
    "$$\n",
    "\n",
    "Here's a concrete example of this model:\n",
    "\n",
    "<img width=\"200\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1IPjnXF5aY7BQnWtF0wlrRKxGIRvLW60C\">\n",
    "\n",
    "\n",
    "In the **neural network** representation of this model:\n",
    "\n",
    "- each feature column in a data set is represented as an **input neuron**\n",
    "- each **weight value** is represented as an **arrow** from the feature column it multiples to the **output neuron**\n",
    "\n",
    "The **neurons** and **arrows** act as a visual metaphor for the weighted sum, which is how the feature columns and weights are combined.\n",
    "\n",
    "<img width=\"200\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1LH9ra2PBRKhEr9T91VvWdu0s00LMDDLQ\">\n",
    "\n",
    "Inspired by **biological neural networks**, an **activation function** determines if the neuron fires or not. In a neural network model, the activation function transforms the weighted sum of the input values. For this network, the activation function is the [identity function](https://en.wikipedia.org/wiki/Identity_function). The identity function returns the same value that was passed in:\n",
    "\n",
    "$$\n",
    "f(x) = x\n",
    "$$\n",
    "\n",
    "<img width=\"200\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1PxtytfRIju0kIL3z-gcN3a2S3VOFpBEt\">\n",
    "\n",
    "While the activation function isn't interesting for a network that performs linear regression, it's useful for logistic regression and more complex networks. Here's a comparison of both representations of the same linear regression model:\n",
    "\n",
    "<img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1bfneZhm7F7pV1CNzDe1JdmG_adgl-2ie\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Regression Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll work with data that we'll generate ourselves, instead of an external data set. Generating data ourselves gives us more control of the properties of the data set (e.g. like the number of features, observations, and the noise in the features). Data sets where neural networks excel contain the same non-linearity, so we can generalize the learnings to real data sets as well.\n",
    "\n",
    "Scikit-learn contains the following convenience functions for generating data:\n",
    "\n",
    "- [sklearn.datasets.make_regression()](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)\n",
    "- [sklearn.datasets.make_classification()](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)\n",
    "- [sklearn.datasets.make_moons()](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons)\n",
    "\n",
    "The following codes generates a regression data set with 3 features, 1000 observations, and a random seed of 1:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# make_regression return a tuple\n",
    "# data[0].shape  (1000,3) -> features\n",
    "# data[1].shape  (1000,) -> target\n",
    "data = make_regression(n_samples=1000, n_features=3, random_state=1)\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93514778,  1.81252782,  0.14010988],\n",
       "       [-3.06414136,  0.11537031,  0.31742716],\n",
       "       [-0.42914228,  1.20845633,  1.1157018 ],\n",
       "       [-0.59384307, -0.34390071, -1.00016919],\n",
       "       [ 0.91208686, -0.85882315, -0.68327072]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 255.52134901, -224.41673037,  177.69580838, -178.28847036,\n",
       "        -63.17367487])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the pandas.DataFrame() constructor to create dataframes:\n",
    "\n",
    "```python\n",
    "features = pd.DataFrame(data[0])\n",
    "```\n",
    "\n",
    "Let's generate some data for the network we're building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Generate a data set for regression:\n",
    "    - with 100 observations\n",
    "    - with 3 features\n",
    "    - using the random seed 1\n",
    "- Convert the NumPy array of generated features into a pandas dataframe and assign to **features**.\n",
    "- Convert the NumPy array of generated labels into a pandas series and assign to **labels**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting A Linear Regression Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the inputs from one layer of neurons feed to the next layer of the single, output neuron, this is known as a [feedforward network](https://en.wikipedia.org/wiki/Feedforward_neural_network). In the language of graphs, a feedforward network is a **directed, acyclic graph**.\n",
    "\n",
    "<left><img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1uc69vU4xRi1XlBT-cQGXSm_QnMQ0awtO\"></left>\n",
    "\n",
    "There are some neural network architectures that don't follow this pattern, but we won't be exploring those in this first lesson.\n",
    "\n",
    "### Fitting A Network\n",
    "\n",
    "In the **Linear Regression lesson**, we explored two different approaches to training a linear regression model: **gradient descent** and **ordinary least squares**. Gradient descent is the most common technique for fitting neural network models and we'll explore it in further details later. Because we've implemented gradient descent before in the Linear Regression lesson, we'll rely on the scikit-learn implementation of gradient descent in this time.\n",
    "\n",
    "This implementation lives in the [SGDRegressor class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html). We use it the same we do with the LinearRegression class:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "lr = linear_model.SGDRegressor()\n",
    "lr.fit(X,y)\n",
    "```\n",
    "\n",
    "We now have everything we need to implement this network. Because we're focusing on building intuition, we'll be training and testing on the same data set. In real life scenarios, you always want to use a cross validation technique of some kind.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Add a **\"bias\"** column containing the value 1 for each row into **features** dataframe.\n",
    "- Import **SGDRegressor** from **sklearn.linear_model**.\n",
    "- Define two functions:\n",
    "    - **train(features, labels)**: takes in the **features** dataframe and **labels** series and performs model fitting.\n",
    "        - Use the SGDRegressor class from scikit-learn to handle model fitting.\n",
    "        - This function should return just an **NumPy 2D array of weights** for the linear regression model.\n",
    "    - **feedforward(features, weights):** takes in the **features** dataframe and the **weights** NumPy array.\n",
    "        - Perform matrix multiplication between **features** (100 rows by 4 columns) and **weights** (4 rows by 1 column) and assign the result to **predictions**.\n",
    "        - Return **predictions**. We'll skip implementing the identity function, since it just returns the same value that was passed in.\n",
    "    - Uncomment the code we have added for you and run the **train()** and **feedforward()** functions. The final predictions will be stored in **linear_predictions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "data = make_regression(n_samples=100, n_features=3, random_state=1)\n",
    "features = pd.DataFrame(data[0])\n",
    "labels = pd.Series(data[1])\n",
    "\n",
    "# put your code here\n",
    "\n",
    "# Uncomment after you've implemented train() and feedforward()\n",
    "# train_weights = train(features, labels)\n",
    "# train_predictions = feedforward(features, train_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Classification Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate sample data friendly for classification, we can use the [make_classification()](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function from scikit-learn.\n",
    "\n",
    "The following codes generates a regression data set with 4 features, 1000 observations, 2 classes (default value) and a random seed of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "class_data = make_classification(n_samples=1000, \n",
    "                                 n_features=4, \n",
    "                                 random_state=1)\n",
    "type(class_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.91518414,  1.14995454, -1.52847073,  0.79430654],\n",
       "       [ 1.4685668 ,  0.80644722, -1.04912964,  0.74652026],\n",
       "       [ 1.47102089,  0.90060386, -1.20228498,  0.57845433],\n",
       "       [ 1.07642824, -0.1813636 ,  0.49116807,  1.95642108],\n",
       "       [-5.34139911, -2.29763222,  2.77907005, -3.87463248]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features\n",
    "class_features = class_data[0]\n",
    "class_features[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels\n",
    "class_labels = class_data[1]\n",
    "class_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the **pandas.DataFrame()** constructor to create dataframes:\n",
    "\n",
    "```python\n",
    "features = pd.DataFrame(data[0])\n",
    "```\n",
    "\n",
    "Let's generate some classification data for the network we're building.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Generate a data set for classification:\n",
    "    - with 100 observations\n",
    "    - with 4 features\n",
    "    - using the random seed 1\n",
    "- Convert the NumPy array of generated features into a pandas dataframe and assign to **class_features**.\n",
    "- Convert the NumPy array of generated labels into a pandas series and assign to **class_labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing A Neural Network That Performs Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the last few sections, we replicated linear regression as a feedforward neural network** model and learned about nonlinear activation functions. We now have a better idea of what defines a neural network. So far, we know that neural networks need:\n",
    "\n",
    "- a network structure (how are the nodes connected? which direction does the data and computation flow?)\n",
    "- a feedforward function (how are the node weights and observation values combined?)\n",
    "- an activation function (what transformations on the data are performed?)\n",
    "- a model fitting function (how is the model fit?)\n",
    "\n",
    "We'll now explore how to **build a neural network that replicates a logistic regression model**. We'll start with a quick recap.\n",
    "\n",
    "### Binary Classification and Logistic Regression\n",
    "\n",
    "In **binary classification**, we're interested in finding a model that can differentiate between two categorical values (usually 0 and 1 are used). The values 0 and 1 don't have any numerical weight and are instead act as numerical placeholders for the two categories. We can instead try to learn the probability that a given observation belongs in either category.\n",
    "\n",
    "In the language of conditional probability, we're interested in the probability that a given observation $x$  belongs to each category:\n",
    "\n",
    "$$\n",
    "P(y=0|x) = 0.3 \\\\\n",
    "P(y=1|x) = 0.7\n",
    "$$\n",
    " \n",
    "Because the universe of possibilities only consists of these two categories, the probabilities for both must add up to 1. This lets us simplify what we want a binary classification model to learn:\n",
    "\n",
    "$$\n",
    "P(y=1|x) = ? \n",
    "$$\n",
    " \n",
    "If $P(y=1|x) > 0.5$, we want the model to assign it to category 1. If $P(y=0|x) < 0.5$, we want the model to assign it to category 0.\n",
    "\n",
    "### Implementing A Logistic Regression Model\n",
    "\n",
    "A logistic regression model consists of two main components:\n",
    "\n",
    "- Computing the weighted linear combination of weights and features (just like in a linear regression model): $Xa^T$\n",
    "- Applying a transformation function to squash the result so it varies between 0 and 1: $\\sigma(Xa^T)$\n",
    "\n",
    "Combining these two steps yields the following definition of a logistic regression model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(Xa^T)\n",
    "$$\n",
    "\n",
    "In neural networks literature, this function is usually referred to as the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function):\n",
    "\n",
    "$$\n",
    "S(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Here's a plot of the sigmoid function:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1178cfba8>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sigmoid = lambda x: 1b/(1 + np.exp(-x))\n",
    "x=np.linspace(-10,10,100)\n",
    "\n",
    "plt.plot(x,sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the sigmoid function has horizontal asymptotes at 0 and 1, which means any input value will always output a value between 0 and 1.\n",
    "\n",
    "To implement a network that performs classification, the only thing we need to change from the **linear regression network** we implemented is the **activation function**. Instead of using the identity function, we need to use the sigmoid function.\n",
    "\n",
    "Here's a diagram of this network:\n",
    "\n",
    "<left><img width=\"250\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Onsw_cuy7cUxh4MhJflkeyOOhgOTbBsT\"></left>\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Add a bias column containing the value 1 for each row named **\"bias\"** to the **class_features** dataframe.\n",
    "- Define three functions:\n",
    "    - **log_train(class_features, class_labels)**: takes in the **class_features** dataframe and **class_labels** series and performs model fitting.\n",
    "        - Use the [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) class from scikit-learn to handle model fitting.\n",
    "        - This function should return just an **NumPy 2D array of weights** for the logistic regression model.\n",
    "    - **sigmoid(linear_combination)**: takes in a NumPy 2D array and applies the sigmoid function for every value: $\\frac{1}{1 + e^{-x}}$\n",
    "    - **log_feedforward(class_features, log_train_weights)**: takes in the **class_features** dataframe and the **log_train_weights** NumPy array.\n",
    "        - Perform matrix multiplication between features (100 rows by 4 columns) and weights (4 rows by 1 column) and assign to **linear_combination**.\n",
    "        - Use the **sigmoid()** function to transform **linear_combinations** and assign the result to **log_predictions**.\n",
    "        - Convert each value in **log_predictions** to a class label:\n",
    "        - If the value is greater than or equal to 0.5, overwrite the value to 1.\n",
    "        - If the value is less than 0.5, overwrite the value to 0.\n",
    "        - Return **log_predictions**\n",
    "- Uncomment the code we have added for you and run the **log_train()** and **log_feedforward()** functions. The final predictions will be stored in **log_predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "class_data = make_classification(n_samples=100, n_features=4, random_state=1)\n",
    "class_features = class_data[0]\n",
    "class_labels = class_data[1]\n",
    "\n",
    "# put your code here\n",
    "\n",
    "# Uncomment this code when you're ready to test your functions.\n",
    "# log_train_weights = log_train(class_features, class_labels)\n",
    "# log_predictions = log_feedforward(class_features, log_train_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this section, we learned about:\n",
    "\n",
    "- representing systems of equations as graphs\n",
    "- the different components of a neural network\n",
    "- how to represent linear and logistic regression models as graphs\n",
    "- In the next section, we'll dive deep into common activation functions and the role they play in neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Nonlinear Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction To Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section, we became familiar with **computational graphs** and how **neural network** models are represented. We also became familiar with **neural network terminology** like:\n",
    "\n",
    "- forward pass\n",
    "- input neurons\n",
    "- output neurons\n",
    "\n",
    "In this section, we'll dive deeper into the role **nonlinear activation functions** play. To help motivate our exploration, let's start by reflecting on the purpose of a machine learning model.\n",
    "\n",
    "The purpose of a machine learning model is to transform training data inputs to the model (which are features) to approximate the training output values. We accomplish this by:\n",
    "\n",
    "- Selecting a specific model to use\n",
    "- Finding the right parameters for this model that work the best\n",
    "- Testing the model to understand how well it generalizes to new data\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "We use linear regression when we think that the output values can be best approximated by a linear combination of the features and the learned weights. This model is a linear system, because any change in the output value is proportional to the changes in the input values.\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_0 + a_1x_1 + a_2x_2 + \\ldots + a_nx_n\n",
    "$$\n",
    "\n",
    "When the target values $y$ can be approximated by a linear combination of the features $x_1$ to $x_2$, linear regression is the ideal choice. Here's a GIF that visualizes the potential expressability of a linear regression model (by conceptually mimicking what gradient descent does):\n",
    "\n",
    "<left><img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1RPNyKITU9qXpFa7YI-69PoJquBSEH3FF\"></left>\n",
    "\n",
    "Let's now look at a situation where the output values can't be approximated effectively using a linear combination of the input values.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "In a binary classification problem, the target values are 0 and 1 and the relationship between the features and the target values is nonlinear. This means we need a function that can perform a **nonlinear transformation** of the input features.\n",
    "\n",
    "The sigmoid function is a good choice since all of its input values are squashed to range between 0  and 1.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(a_0 + a_1x_1 + a_2x_2 + \\ldots + a_nx_n)\n",
    "$$\n",
    "\n",
    "Adding the sigmoid transformation helps the model approximate this nonlinear relationship underlying common binary classification tasks. The following GIF shows how the shape of the logistic regression model changes as we increase the single weight (by conceptually mimicking what gradient descent does):\n",
    "\n",
    "<left><img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1fy5LEM6cKjq8n7qzBN1Trfq02GX3Idu-\"></left>\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "Logistic regression models learn a set of weights that impact the linear combination phase and then are fed through a single nonlinear function (the sigmoid function). In this section, we'll dive into the most commonly used activation functions. The three most commonly used activation functions in neural networks are:\n",
    "\n",
    "- the sigmoid function\n",
    "- the ReLU function\n",
    "- the tanh function\n",
    "\n",
    "Since we've covered the sigmoid function already, we'll focus on the latter two functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by introducing the **ReLU** activation function, which is a commonly used activation function in neural networks for solving **regression problems**. ReLU stands for **rectified linear unit** and is defined as follows:\n",
    "\n",
    "$$\n",
    "ReLU = max(0,x)\n",
    "$$\n",
    "\n",
    "The $max()$ function returns the maximum value between $0$ and $x$. This means that:\n",
    "\n",
    "- when $x$ is less than $0$, the value $0$ is returned\n",
    "- when $x$ is greater than $0$, the value $x$ is returned\n",
    "\n",
    "Here's a plot of the function:\n",
    "\n",
    "<left><img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1ubfxLYyI_tfMt7cGVPmDEvdNcM6X64tf\"></left>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU function returns the positive component of the input value. Let's visualize the expressivity of a model that performs a linear combination of the features and weights followed by the ReLU transformation:\n",
    "\n",
    "<left><img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=11aXYWLdlKJaNyIqGFjusc2SNoEcfmVLj\"></left>\n",
    "\n",
    "There are a few different ways we can implement the ReLU function in code. We'll leave it as an exercise for you to implement.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Define the relu() function.\n",
    "    - This function should be able to work with a single value or a list of values.\n",
    "- Call the relu() function, pass in x, and assign the returned value to **relu_y.**\n",
    "- Print both x and **relu_y**.\n",
    "- Generate a line chart with x on the x-axis and **relu_y** on the y-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-2, 2, 20)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigonometric Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last commonly used activation function in neural networks we'll discuss is the **tanh function** (also known as the **hyperbolic tangent function**). We'll start by reviewing some **trigonometry** by discussing the **tan** (short for tangent) function and then work our way up to the tanh function (in the next screen). While we won't provide the depth here needed to learn trigonometry from scratch, we do recommend the [Trigonometry Series on Khan Academy](https://www.khanacademy.org/math/trigonometry) if you're new to trigonometry.\n",
    "\n",
    "### What is trigonometry?\n",
    "\n",
    "Trigonometry is short for triangle geometry and provides formulas, frameworks, and mental models for reasoning about triangles. Triangles are used extensively in theoretical and applied mathematics, and build on mathematical work done over many centuries. Let's start by clearly defining what a triangle is.\n",
    "\n",
    "A triangle is a [polygon](https://en.wikipedia.org/wiki/Polygon) that has the following properties:\n",
    "\n",
    "- 3 edges\n",
    "- 3 vertices\n",
    "- angles between edges add up to 180 degrees\n",
    "\n",
    "Two main ways that triangles can be classified is by the internal angles or by the edge lengths. The following diagram outlines the three different types of triangles by their edge length properties:\n",
    "\n",
    "<left><img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=189s35_SIzjsdyPAVW4OqXCwId_LhwbjM\"></left>\n",
    "\n",
    "\n",
    "An important triangle that's classified by the internal angles is the **right angle triangle**. In a right angle triangle, one of the angles is 90 degrees (also known as the right angle). The edge opposite of the right angle is called the hypotenuse.\n",
    "\n",
    "<left><img width=\"200\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1WOxgo3Cv2BdHyRr_AiBYbK8-zFJ3JfBZ\"></left>\n",
    "\n",
    "A trigonometric function is a function that inputs an angle value (usually represented as $\\theta$) and outputs some value. These functions compute ratios between the edge lengths. Here are the first 3 trigonometric functions:\n",
    "\n",
    "$$\n",
    "sin(\\theta) = \\frac{opposite}{hypotenuse}\\\\\n",
    "cos(\\theta) = \\frac{adjacent}{hypotenuse}\\\\\n",
    "tan(\\theta) = \\frac{opposite}{adjacent}\n",
    "$$\n",
    "\n",
    "Let's define these terms further:\n",
    "\n",
    "- Hypotenuse describes the line that isn't touching the right angle.\n",
    "- Opposite refers to the line opposite the angle.\n",
    "- Adjacent refers to the line touching the angle that isn't the hypotenuse.\n",
    "\n",
    "Here's an example of the tangent function.\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=15Ny3YVcsxP6Utw32rS_uZeItg8XnheSL\"></left>\n",
    "\n",
    "Now, how does all of this relate to Cartesian coordinate system? The key idea that lets us plot trigonometric functions on a traditional Cartesian coordinate system is the important relationship between right angle triangles and the **unit circle**. The unit circle is a specific type of circle where the radius is 1 (and usually centered at  (0,0). In a unit circle, any line from the center to the circle itself (in red below) forms a right angle triangle.\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Gu0SnShJXmzgnKj8U3OuX6C0Sk00W2oY\"></left>\n",
    "\n",
    "The tangent of $\\theta$ the seen above is calculated by dividing the opposite edge length by the adjacent edge length.\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1mBA4sgjxtYarI-c09kAkcpW3v1lSw9L2\"></left>\n",
    "\n",
    "Linking the unit circle with the Cartesian coordinate system also has another happy byproduct. Doing so lets us reframe the sin and cos trigonometric functions as returning either the $x$ or $y$  coordinate of a point on that circle. Here's a visualization:\n",
    "\n",
    "<left><img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1TlEAQ-g-GA_CvizgbbDsxKDf646q5U-4\"></left>\n",
    "\n",
    "This means that the tan function (opposite / adjacent) is really just the ratio between the $y$ value (opposite) and the $x$ value (adjacent) for a point on the unit circle corresponding to the angle. We'll revisit the relevance of this in the next two screens.\n",
    "\n",
    "### Plotting Tan\n",
    "\n",
    "To plot the tangent function, we need to use [radians](https://en.wikipedia.org/wiki/Radian) on the x-axis instead of degrees. To describe a full trip around the circle, radians range from $0$ to $2\\pi$ while degrees range from $0$ to $360$. Wikipedia has a helpful diagram [here](https://en.wikipedia.org/wiki/Radian#/media/File:Degree-Radian_Conversion.svg).\n",
    "\n",
    "To understand how the tan function behaves, let's compute the values from $-2\\pi$ to $2\\pi$.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the [numpy.tan()](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.tan.html) function to compute the tangent of the values in x and assign the returned value to **tan_y**.\n",
    "- Print both x and **tan_y**.\n",
    "- Generate a line plot with x on the x-axis and **tan_y** on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2*np.pi, 2*np.pi, 100)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflecting On The Tangent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tangent function from the last section generated the following plot:\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1xBxztnfguR8mryBpZbd8suQogIGOt3JD\"></left>\n",
    "\n",
    "The periodic sharp spikes that you see in the plot are known as [vertical asymptotes](https://en.wikipedia.org/wiki/Asymptote#Vertical_asymptotes). At those points, the value isn't defined but the limit approaches either negative or positive infinity (depending on which direction you're approaching the  value from).\n",
    "\n",
    "The key takeaway from the plot is how the tangent function is a repeating, **periodic function**. A periodic function is one that returns the same value at regular intervals. Let's look at a table of some values from the tangent function:\n",
    "\n",
    "| x   | tan(x) |\n",
    "|-----|--------|\n",
    "| -pi | 0      |\n",
    "| 0   | 0      |\n",
    "| pi  | 0      |\n",
    "\n",
    "The tangent function repeats itself every $\\pi$, which is known as the period. The tangent function isn't known to be used as an activation function in neural networks (or any machine learning model really) because the periodic nature isn't a pattern that's found in real datasets.\n",
    "\n",
    "While there have been some [experiments with periodic functions](https://ieeexplore.ieee.org/document/819741) as the activation function for neural networks, the general conclusion has been that period functions like tangent don't offer any unique benefits for modeling.\n",
    "\n",
    "Generally speaking, the activation functions that are used in neural networks are [monotonically increasing functions](https://en.wikipedia.org/wiki/Monotonic_function). A monotonically increasing function consists of 2 parts:\n",
    "\n",
    "- a monotonic function: a function that preserves order in both the domain ($x$ values) and the range ($f(x)$)\n",
    "- an increasing function: a function where $f(x)$ always stays the same or increases as $x$ increases\n",
    "\n",
    "All of the activation functions we've looked at (and will look at) in this mission meet this criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the tangent function describes the ratio of the $y$ and $x$ values on the unit circle, the **hyperbolic tangent function** describes the ratio of $y$ and $x$ values on the [unit hyperbola](https://en.wikipedia.org/wiki/Hyperbola). Here's a diagram of the hyperbola from Wikipedia:\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1hXVYY1maOCz2BYpLEbVfr9FJ9cHEYE43\"></left>\n",
    "\n",
    "The math behind hyperbolic functions is quite involved and we won't be diving into it in this course. We highly recommend the excellent video on hyperbolic functions on [Khan Academy](https://www.youtube.com/watch?v=Wfpb-fniSSk).\n",
    "\n",
    "We'll rely instead on the [numpy.tanh()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html) function to visualize the behavior of this function.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the **numpy.tanh()** function to compute the hyperbolic tangent of the values in x and assign the returned value to **tanh_y**.\n",
    "- Print both x and **tanh_y.**\n",
    "- Generate a line plot with x on the x-axis and tanh_y on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-40, 40, 100)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflecting On The Hyperbolic Tangent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the plot we generated in the last section:\n",
    "\n",
    "<left><img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1kuwLK17pDp74PLK2UYvK7lMXbJ_UKxth\"></left>\n",
    "\n",
    "You'll notice that like the **sigmoid** function, the **tanh** function has horizontal asymptotes as $x$ approaches negative or positive infinity. In addition, the **tanh** function also constrains the range ($y$) to between $-1$ and $1$. Because of this property, both the **sigmoid** and the **tanh** functions are commonly used in neural networks for **classification tasks**.\n",
    "\n",
    "The **ReLU** function, on the other hand, is known to be more effective in **regression tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- In this section, we learned about:\n",
    "\n",
    "    - The importance of nonlinear activation functions in increasing the complexity of patterns that can be learned.\n",
    "    - The most common nonlinear activation functions and their unique properties.\n",
    "\n",
    "In the next section, we'll dive into how adding layers of weights between the input layer and the output layer enables deep neural network models to learn more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last 2 sections, we worked with single layer neural networks. These networks had a single layer of neurons. To make a prediction, a single layer of neurons in these networks directly fed their results into the output neuron(s).\n",
    "\n",
    "In this section, we'll explore how **multi-layer networks** (also known as **deep neural networks**) are able to better capture nonlinearity in the data. In a deep neural network, the first layer of input neurons feeds into a second, intermediate layer of neurons. Here's a diagram representing this architecture:\n",
    "\n",
    "<left><img width=250 alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1FSVFxXoMEnX7ob1ajegGpV8NXtbgFjNI\"></left>\n",
    "\n",
    "We included both of the functions that are used to compute each hidden neuron and output neuron to help clear up any confusion. You'll notice that the number of neurons in the second layer was more than those in the input layer. **Choosing the number of neurons in this layer is a bit of an art** form and not quite a science yet in neural network literature. We can actually add more intermediate layers, and this often leads to improved model accuracy (because of an increased capability in learning nonlinearity).\n",
    "\n",
    "<left><img width=450 alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1TyF5tUpJeORG737eAsoXG1SrVsyAMnSg\"></left>\n",
    "\n",
    "The intermediate layers are known as **hidden layers**, because they aren't directly represented in the input data or the output predictions. Instead, we can think of each hidden layer as intermediate features that are learned during the training process.\n",
    "\n",
    "### Comparison With Decision Tree Models\n",
    "\n",
    "This is actually very similar to how decision trees are structured. The branches and splits represent some intermediate features that are useful for making predictions and are analogous to the hidden layers in a neural network:\n",
    "\n",
    "<left><img width=650 alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1hY9s1zM5Ky-sAfK9U-NM48topA6X-Zc4\"></left>\n",
    "\n",
    "Each of these hidden layers has it's own set of weights and biases, which are discovered during the training process. In decision tree models, the intermediate features in the model represented something more concrete we can understand (feature ranges).\n",
    "\n",
    "Decision tree models are referred to as **white box** models because they can be observed and understood but not easily altered. After we train a decision tree model, we can visualize the tree, interpret it, and have new ideas for tweaking the model. Neural networks, on the other hand, are much closer to being a **black box**. In a black box model, we can understand the inputs and the outputs but the intermediate features are actually difficult to interpret and understand. Even harder and perhaps more importantly, it's difficult to understand how to tweak a neural network based on these intermediate features.\n",
    "\n",
    "In this section, we'll learn how adding more layers to a network and adding more neurons in the hidden layers can improve the model's ability to learn more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data That Contains Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data with nonlinearity in the features (both between the features and between the features and the target column), we can use the [make_moons()](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons) function from scikit-learn:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "data = make_moons()\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, make_moons() will generate 100 rows of data with 2 features. Here's a plot that visualizes one feature against the other:\n",
    "\n",
    "<left><img width=350 alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Dw-iNlbp6_JHxwuh1bHH5MhoFA8k3T2H\"></left>\n",
    "\n",
    "To make things interesting, let's add some Gaussian noise to the data. Gaussian noise is a kind of statistical noise that follows the [Gaussian distribution](https://en.wikipedia.org/wiki/Gaussian_noise), and it's a common way to try to recreate the noise that's often found in real world data.\n",
    "\n",
    "We can use the **noise** parameter to specify the standard deviation of the Gaussian noise we want added to the data. Let's also set the **random_state** to 3 so the generated data can be recreated:\n",
    "\n",
    "```python\n",
    "data = make_moons(random_state = 3, noise=0.04)\n",
    "```\n",
    "\n",
    "Just like in a previous mission, we can separate the resulting NumPy object into 2 pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(data[0])\n",
    "labels = pd.Series(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.801414</td>\n",
       "      <td>0.598111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991790</td>\n",
       "      <td>0.127877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.981559</td>\n",
       "      <td>0.308841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.375267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0 -0.623490  0.781831\n",
       "1  0.801414  0.598111\n",
       "2  0.991790  0.127877\n",
       "3  1.981559  0.308841\n",
       "4  0.926917  0.375267"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Use the **make_moons()** function to generate data with nonlinearity:\n",
    "    - Generate 100 values.\n",
    "    - Set the random seed to 3.\n",
    "    - Set the noise parameter to 0.04.\n",
    "- Convert the NumPy array of generated features into a pandas dataframe and assign to **features**.\n",
    "- Convert the NumPy array of generated labels into a pandas series and assign to **labels**.\n",
    "- Generate a 3d scatter plot of the data:\n",
    "    - Create a matplotlib figure object and set to figsize to (8,8) (**plt.figure()**).\n",
    "    - Create and attach single axes object to this figure using the 3d projection: **ax = fig.add_subplot(111, projection='3d')**\n",
    "    - Generate a 3d scatter plot with the first column from features on the x-axis and the second column from features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_moons\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer With A Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections 1 and 2, we learned how adding a nonlinear activation function expanded the range of patterns that a model could try to learn. The following GIF demonstrates how adding the sigmoid function enables a logistic regression model to capture nonlinearity more effectively:\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1z2BLfw6XHL3RbphDHAmgXxLhNplXHmml\"></left>\n",
    "\n",
    "We can think of a logistic regression model as a neural network with an activation function but no hidden layers. To make predictions, a linear combination of the features and weights is performed followed by a single sigmoid transformation.\n",
    "\n",
    "<left><img width=\"200\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1vBUzJo11X6uybCfRIYKIZO2JCt1T_Jsq\"></left>\n",
    "\n",
    "To improve the expressive power, we can add a hidden layer of neurons in between the input layer and the output layer. Here's an example where we've added a single hidden layer with a single neuron in between the input layer and the output layer:\n",
    "\n",
    "<left><img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=15eTYAknKzI2SMWu-8NbSjlJYV5r3uxiB\"></left>\n",
    "\n",
    "This network contains two sets of weights that are learned during the training phase:\n",
    "\n",
    "- 4 weights between the input layer and the hidden layer\n",
    "- 1 weight between the hidden layer and the output layer\n",
    "\n",
    "In the next section, we'll learn how to train a neural network with a hidden layer using scikit-learn. We'll compare this model with a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Neural Network Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn contains two classes for working with neural networks:\n",
    "\n",
    "- [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "- [MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "\n",
    "Let's focus on the **MLPClassifier** class. As with all of the model classes in scikit-learn, MLPClassifier follows the standard model.fit() and model.predict() pattern:\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "```\n",
    "\n",
    "We can specify the number of hidden neurons we want to use in each layer using the **hidden_layer_sizes** parameter. This parameter accepts a tuple where the index value corresponds to the number of neurons in that hidden layer. The parameter is set to the tuple (100,) by default, which corresponds to a hundred neurons in a single hidden layer. The following code specifies a hidden layer of six neurons:\n",
    "\n",
    "```python\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(6,))\n",
    "```\n",
    "\n",
    "We can specify the **activation function** we want used in all layers using the activation parameter. This parameter accepts only the following string values:\n",
    "\n",
    "- **'identity'**: the identity function\n",
    "- **'logistic'**: the sigmoid function\n",
    "- **'tanh'**: the hyperbolic tangent (tanh) function\n",
    "- **'relu'**: the ReLU function\n",
    "\n",
    "\n",
    "Here's a model instantiated with the sigmoid activation function:\n",
    "\n",
    "```python\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(6,), activation='logistic')\n",
    "```\n",
    "\n",
    "While scikit-learn is friendly to use when learning new concepts, it has a few limitations when it comes to working with neural networks in production.\n",
    "\n",
    "- At the time of writing, scikit-learn only supports using the same activation function for all layers.\n",
    "Scikit-learn also struggles to scale to larger datasets.\n",
    "- Libraries like [Theano](http://deeplearning.net/software/theano/) and [TensorFlow](https://www.tensorflow.org/) support offloading some computation to the GPU to overcome bottlenecks.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Train two different models using scikit-learn on the training set:\n",
    "    - A standard logistic regression model\n",
    "    - A neural network with:\n",
    "        - A single hidden layer\n",
    "        - A single neuron in the hidden layer\n",
    "        - The sigmoid activation function\n",
    "        - evaluate differents **iter_max** values\n",
    "- Make predictions on the test set using the logistic regression model and assign to **log_predictions**\n",
    "- Make predictions on the test set using the neural network model and assign to **nn_predictions.**\n",
    "- Compute the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for **log_predictions** and assign to **log_accuracy.**\n",
    "- Compute the **accuracy score** for **nn_predictions** and assign to **nn_accuracy.**\n",
    "- Print both **log_accuracy** and **nn_accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = make_moons(1000, random_state=3, noise=0.04)\n",
    "features = pd.DataFrame(data[0])\n",
    "labels = pd.Series(data[1])\n",
    "features[\"bias\"] = 1\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer With Multiple Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, the neural network and logistic regression models produced the same accuracy score. While we don't recommend using the accuracy score to benchmark classification models, they can helpful when we're learning and experimenting because they are easy to understand.\n",
    "\n",
    "Let's take a look at a network with a single hidden layer of multiple neurons:\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1LET7Oz3hfTOFY2uqGRqleWNCZXHjsWmD\"></left>\n",
    "\n",
    "This network has 3 input neurons, 6 neurons in the single hidden layer, and 1 output neuron. You'll notice that there's an arrow between every input neuron and every hidden neuron (3 x 6 = 18 connections), representing a weight that needs to be learned during the training process. You'll notice that there's also a weight that needs to be learned between every hidden neuron and the final output neuron (6 x 1 = 6 connections).\n",
    "\n",
    "Because every neuron has a connection between itself and the next layer of neurons, this is known as a **fully connected network**. Lastly, because the computation flows from left (input layer) to right (hidden layer then to output layer), we can call this network a **fully connected, feedforward network**.\n",
    "\n",
    "There are two weight matrices ($\\alpha_1$ and $\\alpha_2$) that need to be learned during the training process, one for each stage of the computation. Let's look at the linear algebra representation of this network.\n",
    "\n",
    "<left><img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1nAT3aFvxw9qI4VWwldvOi0u3RTdMJ5eP\"></left>\n",
    "\n",
    "While we've discussed different architectures in this course, a deep neural network boils down to a series of matrix multiplications paired with nonlinear transformations! These are the key ideas that underlie all neural network architectures. Take a look at this conceptual diagram from the **Asimov Institute** that demonstrates a variety of neural network architectures:\n",
    "\n",
    "<left><img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1anN1gmLL7ttnD8shF4IXM15kKgDpVwE0\"></left>\n",
    "\n",
    "Let's experimenting with adding more neurons to the hidden layer in the neural network from the last screen and see how the accuracy changes.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Create a the following list of neuron counts and assign to **neurons: [1, 5, 10, 15, 20, 25]**\n",
    "- Create an empty list named **accuracies**.\n",
    "- For each value in neurons:\n",
    "    - Train a neural network:\n",
    "        - with the number of neurons in the hidden layer set to the current value\n",
    "        - using the sigmoid activation function\n",
    "    - Make predictions on the test set and compute the accuracy value.\n",
    "    - Append the accuracy value to **accuracies.**\n",
    "- Print **accuracies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = make_moons(1000, random_state=3, noise=0.04)\n",
    "features = pd.DataFrame(data[0])\n",
    "labels = pd.Series(data[1])\n",
    "features[\"bias\"] = 1\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the test set prediction accuracy improved to 0.88 when using ten neurons in the hidden layer. In general, the accuracy didn't vastly improve between the models:\n",
    "\n",
    "```python\n",
    "print(accuracies)\n",
    "[0.5, 0.8766666666666667, 0.88, 0.88, 0.88, 0.88]\n",
    "```\n",
    "\n",
    "Next, we can try increasing the number of hidden layers to see if that helps improve the accuracy of the network. Here's a diagram representing a neural network with six neurons in the first hidden layer and four neurons in the second hidden layer:\n",
    "\n",
    "<left><img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=11pXUHkxYlsExuoRNfbBSM7A00SaYi0Xq\"></left>\n",
    "\n",
    "To determine the number of weights between the layers, multiply the number of neurons between those two layers. Remember that these weights will be represented as weight matrices.\n",
    "\n",
    "To specify the number of hidden layers and the number of neurons in each hidden layer, we change the tuple we pass in to the hidden_layer_sizes parameter:\n",
    "\n",
    "```python\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(n,k))\n",
    "```\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Create a the following list of neuron counts and assign to **neurons: [1, 5, 10, 15, 20, 25]**\n",
    "- Create an empty list named **nn_accuracies.**\n",
    "- For each value in **neurons:**\n",
    "    - Train a neural network:\n",
    "        - with two hidden layers, each containing the same number of neurons (the current value in neurons)\n",
    "        - using the sigmoid activation function\n",
    "    - Make predictions on the test set and compute the accuracy value.\n",
    "    - Append the accuracy value to **nn_accuracies.**\n",
    "- Print nn_accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = make_moons(1000, random_state=3, noise=0.04)\n",
    "features = pd.DataFrame(data[0])\n",
    "labels = pd.Series(data[1])\n",
    "features[\"bias\"] = 1\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the accuracy didn't increase with two hidden layers.\n",
    "\n",
    "```python\n",
    "[0.5, 0.88, 0.8833333333333333, 0.88, 0.88, 0.88]\n",
    "```\n",
    "\n",
    "Adding more layers can cause the network to overfit, which may have happened here. Let's shift our attention to a problem where neural networks are known to shine.\n",
    "\n",
    "Next, and last in this lesson, is a few steps where you'll build deep neural networks to classify handwritten digits. Neural networks have known to excel at image classification tasks, so this will be an exciting real-world problem for you to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Case Study: Building A Handwritten Digits Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section, we learned how adding hidden layers of neurons to a neural network can improve its ability to capture nonlinearity in the data. We tested different neural networks models on a dataset that we generated with deliberate nonlinearity.\n",
    "\n",
    "Now, in this section we'll:\n",
    "\n",
    "- explore why image classification is a hard task\n",
    "- observe the limitations of traditional machine learning models for image classification\n",
    "- train, test, and improve a few different deep neural networks for image classification\n",
    "\n",
    "\n",
    "As we mentioned in the first section in this lesson, deep neural networks have been used to reach state-of-the-art performance on **image classification** tasks in the last decade. For some image classification tasks, deep neural networks actually perform as well as or slightly better than the human benchmark. You can read about the history of deep neural networks [here](https://arxiv.org/pdf/1803.01164.pdf).\n",
    "\n",
    "To end this lesson, we'll build models that can classify handwritten digits. Before the year 2000, institutions like the United States Post Office used handwriting recognition software to read addresses, zip codes, and more. One of their approaches, which consists of pre-processing handwritten images then feeding to a neural network model is detailed in this [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.852.5499&rep=rep1&type=pdf).\n",
    "\n",
    "### Why is image classifcation a hard task?\n",
    "\n",
    "Within the field of machine learning and pattern recognition, image classification (especially for handwritten text) is towards the difficult end of the spectrum. There are a few reasons for this.\n",
    "\n",
    "First, each image in a training set is high dimensional. Each pixel in an image is a feature and a separate column. This means that a 128 x 128 image has 16384 features.\n",
    "\n",
    "Second, images are often downsampled to lower resolutions and transformed to grayscale (no color). This is a limitation of compute power unfortunately. The resolution of a 8 megapixel photo has 3264 by 2448 pixels, for a total of 7,990,272 features (or about 8 million). Images of this resolution are usually scaled down to between 128 and 512 pixels in either direction for significantly faster processing. This often results in a loss of detail that's available for training and pattern matching.\n",
    "\n",
    "Third, the features in an image don't have an obvious linear or nonlinear relationship that can be learned with a model like linear or logistic regression. In grayscale, each pixel is just represented as a brightness value ranging from 0 to 256.\n",
    "\n",
    "Here's an example of how an image is represented across the different abstractions we care about:\n",
    "\n",
    "<left><img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1TEFvHIvnBTJevVGDhMlRvu-0_0jSxfVb\"></left>\n",
    "\n",
    "### Why is deep learning effective in image classification?\n",
    "\n",
    "Deep learning is effective in image classification because of the models' **ability to learn hierarchical representations**. At a high level, an effective deep learning model learns intermediate representations at each layer in the model and uses them in the prediction process. Here's a diagram that visualizes what the weights represent at each layer of a **convolutional neural network**, a type of network that's often used in image classification and unfortunately out of scope for this lesson, which was trained to identify faces.\n",
    "\n",
    "<left><img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Rd4p4C9Xg75FZ9IPaTWQq56sIhGEQpm-\"></left>\n",
    "\n",
    "You'll notice in the first hidden layer the network learned to represent edges and specific features of faces. In the second hidden layer, the weights seemed to represent higher level facial features like eyes and noses. Finally, the weights in the last hidden layer resemble faces that could be matched against. Each successive layer uses weights from previous layers to try to learn more complex representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because dataframes are a tabular representation of data, each image is represented as a row of pixel values. To visualize an image from the dataframe, we need to reshape the image back to its original dimensions (28 x 28 pixels). To visualize the image, we need to reshape these pixel values back into the 28 by 28 and plot them on a coordinate grid.\n",
    "\n",
    "To reshape the image, we need to convert a training example to a numpy array (excluding the label column) and pass the result into that into the [numpy.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) function:\n",
    "\n",
    "```python\n",
    "first_image = train.iloc[0]\n",
    "first_image = first_image.drop('label', axis=1)\n",
    "np_image = first_image.values\n",
    "np_image = np_image.reshape(28,28)\n",
    "```\n",
    "\n",
    "Now that the data is in the right shape, we can visualize it using [pyplot.imshow()](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html?highlight=matplotlib%20pyplot%20imshow#matplotlib.pyplot.imshow) function:\n",
    "\n",
    "```python\n",
    "plt.imshow(np_image, cmap='gray_r')\n",
    "```\n",
    "\n",
    "To display multiple images in one matplotlib figure, we can use the equivalent [axes.imshow()](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.imshow.html#matplotlib.axes.Axes.imshow) function. Let's use what we've learned to display images from both classes.\n",
    "\n",
    "```python\n",
    "f, ax = plt.subplots(1, 2)\n",
    "\n",
    "axarr[0, 0].imshow(data.iloc[0].values.reshape(28,28), cmap='gray_r')\n",
    "axarr[0, 1].imshow(data.iloc[100].values.reshape(28,28), cmap='gray_r')\n",
    "```\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Import **load_digits()** from the **sklearn.datasets** package. This function load and return the digits dataset (classification). Each datapoint is a 8x8 image of a digit.\n",
    "- Run **load_digits()** function and store the result to **digits**. \n",
    "- Transform the NumPy 2D array (**digits[\"data\"]**) into the dataframe **df_digits**.\n",
    "- Transform the Numpy 1D array (**digits[\"target\"]**) into the serie **label**.\n",
    "- Use matplotlib to visualize some of the images in the dataset.\n",
    "    - Generate a subplot grid, with 2 rows and 4 columns.\n",
    "    - In the first row:\n",
    "        - Display the images corresponding to rows 0, 100, 200, and 300.\n",
    "    - In the second row:\n",
    "        - Display the images corresponding to rows 1000, 1100, 1200, and 1300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **linear** and **logistic regression** models make assumptions about the linearity between the features and the output labels, the **k-nearest neighbors** algorithm make no such assumption. This allows them to capture nonlinearity in the data. If you recall, k-nearest neighbors don't have a specific model representation (hence why it's referred to as an algorithm and not a model).\n",
    "\n",
    "The k-nearest neighbors algorithm compares every unseen observation in the test set to all (or many, as some implementations constrain the search space) training observations to look for similar (or the \"nearest\") observations. Then, the algorithm finds the label with the most nearby observations and assigns that as the prediction for the unseen observation.\n",
    "\n",
    "Recall that you can use the [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) package to train and test k-nearest neighbors models.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Create a plot to evaluate the relation between mean accuracy of models and the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9840915984228086"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into train and test\n",
    "# just touch in test_x and test_y only on final version of model\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_digits, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# number of neighbors\n",
    "neighbors = 3\n",
    "\n",
    "# store the accuracy results\n",
    "fold_accuracies = []\n",
    "\n",
    "# k-fold validation\n",
    "# df_digits is our original data\n",
    "kf = KFold(n_splits = 4, random_state=2)\n",
    "\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    \n",
    "    # split each fold into train and test\n",
    "    train_features, test_features = train_x.iloc[train_index], train_x.iloc[test_index]\n",
    "    train_labels, test_labels = train_y.iloc[train_index], train_y.iloc[test_index]\n",
    "    \n",
    "    # create a knn classifier model\n",
    "    knn = KNeighborsClassifier(n_neighbors = neighbors)\n",
    "    knn.fit(train_features, train_labels)\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = knn.predict(test_features)\n",
    "    \n",
    "    # evaluate accuracy\n",
    "    overall_accuracy = accuracy_score(test_labels, predictions)\n",
    "    fold_accuracies.append(overall_accuracy)\n",
    "\n",
    "np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9888888888888889"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Final Model\n",
    "# just change the number of neighbors by the best hyperparameter result\n",
    "\n",
    "# number of neighbors\n",
    "neighbors = 3\n",
    "\n",
    "# create a knn classifier model\n",
    "knn = KNeighborsClassifier(n_neighbors = neighbors)\n",
    "knn.fit(train_x, train_y)\n",
    "    \n",
    "# make predictions\n",
    "predictions = knn.predict(test_x)\n",
    "    \n",
    "# evaluate accuracy\n",
    "overall_accuracy = accuracy_score(test_y, predictions)\n",
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network With One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few downsides to using k-nearest neighbors:\n",
    "\n",
    "- high memory usage (for each new unseen observation, many comparisons need to be made to seen observations)\n",
    "- no model representation to debug and explore\n",
    "\n",
    "Let's now try a neural network with a single hidden layer. Use the [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) package from scikit-learn.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "Display the **accuracy values** for each of the following models:\n",
    "\n",
    "- Train and test a neural network model using a single hidden layer of 8 neurons.\n",
    "- Train and test a neural network model using a single hidden layer of 16 neurons.\n",
    "- Train and test a neural network model using a single hidden layer of 32 neurons.\n",
    "- Train and test a neural network model using a single hidden layer of 64 neurons.\n",
    "- Train and test a neural network model using a single hidden layer of 128 neurons.\n",
    "- Train and test a neural network model using a single hidden layer of 256 neurons.\n",
    "\n",
    "Create a new Markdown cell summarizing what you saw.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network With Two Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It looks like adding more neurons to the single hidden layer helped massively improved simple accuracy from approximately 89% to approximately 97%. Simple accuracy computes the number of correct classifications the model made, but doesn't tell us anything about false or true positives or false or true negatives.\n",
    "\n",
    "Given that k-nearest neighbors achieved approximately 99% accuracy, there doesn't seem to be any advantages to using a single hidden layer neural network for this problem.\n",
    "\n",
    "Let's try using two hidden layers and continue to increase the number of neurons in each layer.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Modify the neural network training pipeline to use 2 hidden layers instead in the network.\n",
    "- Train a neural network model using 2 hidden layers:\n",
    "    - 64 neurons in the first hidden layer.\n",
    "    - 64 neurons in the second hidden layer.\n",
    "- Train a neural network model using 2 hidden layers:\n",
    "    - 128 neurons in the first hidden layer.\n",
    "    - 128 neurons in the second hidden layer.\n",
    "- Train a neural network model using 2 hidden layers:\n",
    "    - 256 neurons in the first hidden layer.\n",
    "    - 256 neurons in the second hidden layer.\n",
    "- Create a new Markdown cell summarizing what you saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network With Three Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using two hidden layers instead of one definitely improved the classification accuracy. Using three layers will also improve classification accuracy, but you need to be very careful about overfitting. Let's increase the number of folds to 6 while testing 3 layer networks.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Modify the neural network training pipeline to use 6 fold cross validation instead.\n",
    "- Train a neural network model using 3 hidden layers:\n",
    "    - 10 neurons in the first hidden layer.\n",
    "    - 10 neurons in the second hidden layer.\n",
    "    - 10 neurons in the third hidden layer.\n",
    "- Train a neural network model using 3 hidden layers:\n",
    "    - 64 neurons in the first hidden layer.\n",
    "    - 64 neurons in the second hidden layer.\n",
    "    - 64 neurons in the third hidden layer.\n",
    "- Train a neural network model using 3 hidden layers:\n",
    "    - 128 neurons in the first hidden layer.\n",
    "    - 128 neurons in the second hidden layer.\n",
    "    - 128 neurons in the third hidden layer.\n",
    "- Create a new Markdown cell summarizing what you saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some next steps to try:\n",
    "\n",
    "- Use the existing pipeline you've built to try other machine learning models:\n",
    "    - Decision Tree Models\n",
    "    - Random Forest Models\n",
    "- Write up a summary of the effectiveness of the different models you tried, with data to back up your conclusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
