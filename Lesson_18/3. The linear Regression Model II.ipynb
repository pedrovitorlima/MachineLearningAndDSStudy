{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xwkIPyqwMt0"
   },
   "source": [
    "# Upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18466,
     "status": "ok",
     "timestamp": 1537728483456,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "ILce_K6dwiyE",
    "outputId": "559a334b-39a9-482b-b6c6-775b32dc04e3"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# ONLY IF YOUR RUN ON GOOGLE COLABORATORY\n",
    "#\n",
    "#\n",
    "#Uploading files from your local file system\n",
    "# AmesHousing.txt\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQRDlYETItBG",
    "toc-hr-collapsed": true
   },
   "source": [
    "# The Linear Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUS-vTmpI-iT"
   },
   "source": [
    "\n",
    "## Introduction to the data\n",
    "\n",
    "\n",
    "To get familiar with this machine learning approach, we'll work with a dataset on sold houses in Ames, Iowa. Each row in the dataset describes the properties of a single house as well as the amount it was sold for. In this course, we'll build models that predict the final sale price from its other attributes. Specifically, we'll explore the following questions:\n",
    "\n",
    "- Which properties of a house most affect the final sale price?\n",
    "- How effectively can we predict the sale price from just its properties?\n",
    "\n",
    "This dataset was originally compiled by [Dean De Cock](http://www.truman.edu/faculty-staff/decock/) for the primary purpose of having a high quality dataset for regression. You can read more about his process and motivation [here](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf) and download the dataset [here](https://ww2.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt).\n",
    "\n",
    "Here are some of the columns:\n",
    "\n",
    "- **Lot Area**: Lot size in square feet.\n",
    "- **Overall Qual**: Rates the overall material and finish of the house.\n",
    "- **Overall Cond**: Rates the overall condition of the house.\n",
    "- **Year Built**: Original construction date.\n",
    "- **Low Qual Fin SF**: Low quality finished square feet (all floors).\n",
    "- **Full Bath**: Full bathrooms above grade.\n",
    "- **Fireplaces**: Number of fireplaces.\n",
    "\n",
    "Let's start by generating train and test datasets and getting more familiar with the data.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "\n",
    "- Read **AmesHousing.txt** into a dataframe using the tab delimiter (**\\t**) and assign to **data**.\n",
    "- Select the first **1460** rows from **data** and assign to **train**.\n",
    "- Select the remaining rows from **data** and assign to **test**.\n",
    "- Use the **dataframe.info()** method to display information about each column.\n",
    "- Read the [data documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt) to get more familiar with each column.\n",
    "- Using the data documentation, determine which column is the target column we want to predict. Assign the column name as a string to **target**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i28ArIAVJIJp"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FB_vGEOeJqkx"
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "\n",
    "We'll start by understanding the univariate case of linear regression, also known as **simple linear regression**. The following equation is the general form of the simple linear regression model.\n",
    "\n",
    "$$\\hat{y}=a_1x_1+a_0$$\n",
    "\n",
    "$\\hat{y}$ represents the target column while $x_1$ represents the feature column we choose to use in our model. These values are independent of the dataset. On the other hand, $a_0$ and $a_1$ represent the **parameter** values that are specific to the dataset. The goal of simple linear regression is to find the optimal parameter values that best describe the relationship between the feature column and the target column. The following diagram shows different simple linear regression models depending on the data:\n",
    "\n",
    "<img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1aYSv01fP3tEoEukxRmMZFcfJwZfzS-UV\">\n",
    "\n",
    "\n",
    "The first step is to select the feature, $x_1$, we want to use in our model. Once we select this feature, we can use scikit-learn to determine the optimal parameter values $a_1$ and $a_0$ based on the training data. Because one of the assumptions of linear regression is that the relationship between the feature(s) and the target column is linear, we want to pick a feature that seems like it has the strongest correlation with the final sale price.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Generate 3 scatter plots in the same column:\n",
    "  - The first plot should plot the **Garage Area** column on the x-axis against the **SalePrice** column on the y-axis.\n",
    "  - The second one should plot the **Gr Liv Area** column on the x-axis against the **SalePrice** column on the y-axis.\n",
    "  - The third one should plot the **Overall Cond** column on the x-axis against the **SalePrice** column on the y-axis.\n",
    "- Read more about these 3 columns in the [data documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evozi7ZuJzzE"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_6N5CT7KvfD"
   },
   "source": [
    "## Using Scikit-Learn To Train And Predict\n",
    "\n",
    "\n",
    "Let's now use scikit-learn to find the optimal parameter values for our model. The scikit-learn library was designed to easily swap and try different models. Because we're familiar with the scikit-learn workflow for k-nearest neighbors, switching to using linear regression is straightforward.\n",
    "\n",
    "We will work with the **sklearn.linear_model.LinearRegression** class. The **LinearRegression** class also has it's own **fit()** method. Specific to this model, however, is the **coef_** and **intercept_** attributes, which return $a_1$ ($a_1$ to $a_n$ if it were a multivariate regression model) and $a_0$ accordingly.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Import and instantiate a **linear regression** model.\n",
    "- Fit a linear regression model that uses the feature and target columns we explored in the last 2 screens. **Use the default arguments**.\n",
    "- Display the coefficient and intercept of the fitted model using the **coef_** and **intercept_** attributes.\n",
    "- Assign  to **a1** and  to **a0**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-97p_ZylK7EG"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4m-ke73Oj4m"
   },
   "source": [
    "## Making Predictions\n",
    "\n",
    "In the last step, we fit a univariate linear regression model between the **Gr Liv Area** and **SalePrice** column. We then displayed the single coefficient and the residuel value. If we refer back to the format of our linear regression model, the fitted model can be represented as:\n",
    "\n",
    "$$\\hat{y}=116.86624683x_1+5366.82171006$$\n",
    "\n",
    "\n",
    "One way to interpret this model is \"for every 1 square foot increase in above ground living area, we can expect the home's value to increase by approximately 116.87 dollars\".\n",
    "\n",
    "We can now use the **predict()** method to predict the labels using the training data and compare them with the actual labels. To quantify the fit, we can use mean squared error. Let's also perform simple validation by making predictions on the test set and calculate the **MSE** value for those predictions as well.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Use the fitted model to make predictions on both the training and test sets.\n",
    "- Calculate the RMSE value for the predictions on the training set and assign to **train_rmse**.\n",
    "- Calculate the RMSE value for the predictions on the test set and assign to **test_rmse**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuXB0gNVOx9Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[['Gr Liv Area']], train['SalePrice'])\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Q71nRlmPuRx"
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Now that we've explored the basics of simple linear regression, we can extend what we've learned to the multivariate case (often called **multiple linear regression**). A multiple linear regression model allows us to capture the relationship between multiple feature columns and the target column. Here's what the formula looks like:\n",
    "\n",
    "$$\\hat{y}=a_0+a_1x_1+a_2x_2+...+a_nx_n$$\n",
    "\n",
    "When using multiple features, the main challenge is **selecting relevant features**. In a later mission in this course, we'll dive into some approaches for feature selection. For now, let's train a model using the following columns from the dataset to see how train and test RMSE values are improved.\n",
    "\n",
    "- **Overall Cond**\n",
    "- **Gr Liv Area**\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Train a linear regression model using the columns in cols.\n",
    "- Use the fitted model to make predictions on both the training and test dataset.\n",
    "- Calculate the RMSE value for the predictions on the training set and assign to **train_rmse_2**.\n",
    " Calculate the RMSE value for the predictions on the test set and assign to **test_rmse_2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSyYmB8RP3Y6"
   },
   "outputs": [],
   "source": [
    "cols = ['Overall Cond', 'Gr Liv Area']\n",
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HU5QZX0DQQJU",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Feature Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFxiSg9PRX32"
   },
   "source": [
    "## Missing Values\n",
    "\n",
    "\n",
    "In the machine learning workflow, once we've selected the model we want to use, selecting the appropriate features for that model is the next important step. In this section, we'll explore how to use correlation between features and the target column, correlation between features, and variance of features to select features. We'll continue working with the same housing dataset from the last mission.\n",
    "\n",
    "We'll specifically focus on selecting from feature columns that don't have any missing values or don't need to be transformed to be useful (e.g. columns like **Year Built** and **Year Remod/Add**). We'll explore how to deal with both of these in a later mission in this course.\n",
    "\n",
    "To start, let's look at which columns fall into either of these two categories.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "- Drop the following columns from **numerical_train**:\n",
    "  - **PID** (place ID isn't useful for modeling)\n",
    "  - **Year Built**\n",
    "  - **Year Remod/Add**\n",
    "  - **Garage Yr Blt**\n",
    "  - **Mo Sold**\n",
    "  - **Yr Sold**\n",
    "- Calculate the number of missing values from each column in **numerical_train**. Create a Series object where the index is made up of column names and the associated values are the number of missing values:\n",
    "\n",
    "```python\n",
    "Order                0\n",
    "PID                  0\n",
    "MS SubClass          0\n",
    "MS Zoning            0\n",
    "...\n",
    "```\n",
    "- Assign this Series object to **null_series**. Select the subset of **null_series** to keep only the columns with no missing values, and assign the resulting Series object to **full_cols_series**.\n",
    "- Display **full_cols_series** using the **print()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dm85utWxRd20"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Dataset/AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "numerical_train = train.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfJ-whH1Sd_t"
   },
   "source": [
    "## Correlating Feature Columns With Target Column\n",
    "\n",
    "\n",
    "In the last mission, we selected the feature for the simple linear regression model by comparing how some of the features correlate with the target column. If you recall, we focused on 4 features in particular and used the **pandas.DataFrame.corr()** method to return the correlation coefficients between each pairs of columns. This means that the correlation matrix for 4 columns results in 16 correlation values:\n",
    "\n",
    "```python\n",
    ">>> train[['GarageArea', 'GrLivArea', 'OverallCond', 'SalePrice']].corr()\n",
    "             GarageArea  GrLivArea  OverallCond  SalePrice\n",
    "GarageArea     1.000000   0.468997    -0.151521   0.623431\n",
    "GrLivArea      0.468997   1.000000    -0.079686   0.708624\n",
    "OverallCond   -0.151521  -0.079686     1.000000  -0.077856\n",
    "SalePrice      0.623431   0.708624    -0.077856   1.000000\n",
    "```\n",
    "\n",
    "\n",
    "The subset of features we want to focus on, full_cols_series, contains 27 columns:\n",
    "\n",
    "```python\n",
    ">>> len(full_cols_series)\n",
    "27\n",
    "```\n",
    "\n",
    "The resulting correlation matrix will contain **27 * 27** or **729** correlation values. Comparing and contrasting this many values is incredibly difficult. Let's instead focus on just how the feature columns correlate with the target column (**SalePrice**) instead.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Compute the pairwise correlation coefficients between all of the columns in **train_subset.**\n",
    "- Select just the **SalePrice** column from the resulting data frame, compute the **absolute value** of each term, sort the resulting Series by the correlation values, and assign to **sorted_corrs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JrQxXJiXTGeh"
   },
   "outputs": [],
   "source": [
    "train_subset = train[full_cols_series.index]\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpTs8lUAUWkS"
   },
   "source": [
    "## Correlation Matrix Heatmap\n",
    "\n",
    "We now have a decent list of candidate features to use in our model, sorted by how strongly they're correlated with the **SalePrice** column. For now, let's keep only the features that have a correlation of **0.3** or higher. This cutoff is a bit arbitrary and, in general, it's a good idea to experiment with this cutoff. For example, you can train and test models using the columns selected using different cutoffs and see where your model stops improving.\n",
    "\n",
    "The next thing we need to look for is for potential **collinearity** between some of these feature columns. Collinearity is when 2 feature columns are highly correlated and stand the risk of duplicating information. If we have 2 features that convey the same information using 2 different measures or metrics, we don't need to keep both.\n",
    "\n",
    "While we can check for collinearity between 2 columns using the correlation matrix, we run the risk of information overload. We can instead generate a [correlation matrix heatmap](http://seaborn.pydata.org/examples/heatmap_annotation.html) using Seaborn to visually compare the correlations and look for problematic pairwise feature correlations. Because we're looking for outlier values in the heatmap, this visual representation is easier.\n",
    "\n",
    "Here's what the example correlation matrix heatmap looks like from the documentation:\n",
    "\n",
    "<img width=\"450\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=11v5f5BaFC2OhRM4bAVT1CzjxGws5jGPZ\">\n",
    "\n",
    "\n",
    "To generate a correlation matrix heatmap, we need to pass in the data frame containing the correlation matrix as a data frame into the **seaborn.heatmap()** function.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Select only the columns in **sorted_corrs** with a correlation above 0.3 and assign to **strong_corrs**.\n",
    "- Use the **seaborn.heatmap()** function to generate a correlation matrix heatmap for the columns in **strong_corrs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwzlk8cRUatA"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgG4gw1NU657"
   },
   "source": [
    "## Train And Test Model\n",
    "\n",
    "Based on the correlation matrix heatmap, we can tell that the following pairs of columns are strongly correlated:\n",
    "\n",
    "- **Gr Liv Area** and **TotRms AbvGrd**\n",
    "- **Garage Area** and **Garage Cars**\n",
    "\n",
    "If we read the descriptions of these columns from the [data documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt), we can tell that each pair of column reflects very similar information. Bceause **Gr Liv Area** and **Garage Area** are continuous variables that capture more nuance, let's drop the **TotRms AbvGrd** and **Garage Cars**.\n",
    "\n",
    "The last thing we'll need to do is confirm that the test set contains no missing values for these columns:\n",
    "\n",
    "```python\n",
    ">>> final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    ">>> test[final_corr_cols.index].info()\n",
    "class 'pandas.core.frame.DataFrame'\n",
    "RangeIndex: 1470 entries, 1460 to 2929\n",
    "Data columns (total 9 columns):\n",
    "Wood Deck SF     1470 non-null int64\n",
    "Open Porch SF    1470 non-null int64\n",
    "Fireplaces       1470 non-null int64\n",
    "Full Bath        1470 non-null int64\n",
    "1st Flr SF       1470 non-null int64\n",
    "Garage Area      1469 non-null float64\n",
    "Gr Liv Area      1470 non-null int64\n",
    "Overall Qual     1470 non-null int64\n",
    "SalePrice        1470 non-null int64\n",
    "dtypes: float64(1), int64(8)\n",
    "memory usage: 103.4 KB\n",
    "```\n",
    "\n",
    "Looks like the test set has one pesky row with a missing value for **Garage Area**. Let's just drop this row for now. Finally, let's train and test a model using these columns to see how they fare.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Filter the test data frame so it only contains the columns from **final_corr_cols.index**. Then, drop the row containing missing values and assign the result to **clean_test**\n",
    "- Build a linear regression model using the features in **features.**\n",
    "- Calculate the RMSE on the test and train sets (use predict on both of them)\n",
    "- Assign the train RMSE to **train_rmse** and the test RMSE to **test_rmse.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXud_nghVU7-"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    "features = final_corr_cols.drop(['SalePrice']).index\n",
    "target = 'SalePrice'\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEHtQGHNVxWf"
   },
   "source": [
    "## Removing Low Variance Features\n",
    "\n",
    "The last technique we'll explore is removing features with low variance. When the values in a feature column have low variance, they don't meaningfully contribute to the model's predictive capability. On the extreme end, let's imagine a column with a variance of **0**. This would mean that all of the values in that column **were exactly the same**. This means that the column isn't informative and isn't going to help the model make better predictions.\n",
    "\n",
    "To make apples to apples comparisons between columns, we need to rescale all of the columns to vary between **0** and **1**. Then, we can set a cutoff value for variance and remove features that have less than that variance amount. This is known as min-max scaling or as [rescaling](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling). Here's the formula for rescaling:\n",
    "\n",
    "$$\\frac{x−min(x)}{max(x)−min(x)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ is an individual value\n",
    "- $min(x)$ is the minimum value for the column $x$ belongs to\n",
    "- $max(x)$ is the maximum value for the column $x$ belongs to\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Select the columns in **features** from the **train** data frame. Rescale each of the columns so the values range from 0 to 1.\n",
    "- Calculate and display the column minimum and maximum values to ensure that all values range from 0 to 1.\n",
    "- Calculate the variance of these columns, sort the resulting series by its values, and assign to **sorted_vars.**\n",
    "- Display **sorted_vars** using the **print()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BR2Ct8RbWLwi"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "maK10kcJWnN7"
   },
   "source": [
    "## Final Model\n",
    "\n",
    "\n",
    "To wrap up this mission, let's set a cutoff variance of **0.015**, remove the **Open Porch SF feature**, and train and test a model using the remaining features.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Build a linear regression model using the remaining features.\n",
    "- Calculate the RMSE on the test and train sets.\n",
    "- Assign the train RMSE to **train_rmse_2** and the test RMSE to **test_rmse_2**.\n",
    "- Display both RMSE values using the **print()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAz1BeXjXCHB"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2XCXYqoXTi-"
   },
   "source": [
    "## Next-Steps\n",
    "\n",
    "We were able to improve the RMSE value to approximately **40591** by removing the **Open Porch SF** feature. This is most likely the furthest we can go without transforming and utilizing the other features in the dataset so we'll stop here for now. In the next 2 sections, we'll explore 2 different ways of fitting models. Afterwards, we'll explore ways to clean and engineer new features from the existing features to improve model accuracy even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DpwIe7ZXcVQ"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YObCsW_NYMgg",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YKeASATY4Qz"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In the previous section, we learned how the linear regression model estimates the relationship between the feature columns and the target column and how we can use that for making predictions. In this mission and the next, we'll discuss the 2 most common ways for finding the optimal parameter values for a linear regression model. Each combination of unique parameter values forms a unique linear regression model, and the process of finding these optimal values is known as **model fitting**. Both approaches to model fitting we'll explore aim to minimize the following function:\n",
    "\n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i−y_i)^2$$\n",
    "\n",
    "\n",
    "This function is the mean squared error between the predicted labels made using a given model and the true labels. The problem of choosing a set of values that minimize or maximize another function is known as an [optimization problem](https://en.wikipedia.org/wiki/Mathematical_optimization).\n",
    "\n",
    "To build intuition for the optimization process, let's start with a single parameter linear regression model:\n",
    "\n",
    "$$\\hat{y}=a_1x_1$$\n",
    "\n",
    "Note that this is different from a simple linear regression model, which actually has two parameters: $x_0$ and $x_1$.\n",
    "\n",
    "$$\\hat{y}=a_1x_1+a_0$$\n",
    "\n",
    "Let's use the **Gr Liv Area** column for the single parameter:\n",
    "\n",
    "$$\\hat{SalePrice}=a_1∗GrLivArea$$\n",
    "\n",
    "<img width=\"300\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1k0Mj4NgMElCcDh1NAggOiS95ZAzg5SmF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JK8KgNAY9gk"
   },
   "source": [
    "## Single Variable Gradient Descent\n",
    "\n",
    "In the previous figure, we observed how the optimization function follows a curve with a minimum value. **This should remind you of our exploration of relative minimum values from calculus**. If you recall, we computed the critical points by calculating the curve's derivative, setting it equal to 0, and finding the x value at this point. Unfortunately, this approach won't work when we have multiple parameter values because minimizing one parameter value may increase another parameter's value. In addition, while we can plot the MSE curve when we only have a single parameter we're trying to find and visually select the value that minimizes the MSE, this approach won't work when we have multiple parameter value because we can't visualize past 3 dimensions.\n",
    "\n",
    "In this mission, we'll explore an iterative technique for solving this problem, known as **gradient descent**. The [gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent) works by iteratively trying different parameter values until the model with the lowest mean squared error is found. Gradient descent is a commonly used optimization technique for other models as well, like neural networks, which we'll explore later in this track.\n",
    "\n",
    "Here's an overview of the gradient descent algorithm for a single parameter linear regression model:\n",
    "\n",
    "- select initial values for the parameter: $a_1$\n",
    "- repeat until convergence (usually implemented with a max number of iterations):\n",
    "    - calculate the error (MSE) of model that uses current parameter value: $MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}−y^{(i)})^2$\n",
    "    - calculate the derivative of the error (MSE) at the current parameter value: $\\frac{d}{da_1}MSE(a_1)$\n",
    "    - update the parameter value by subtracting the derivative times a constant ($\\alpha$, called the learning rate): $a_1:=a_1−\\alpha \\frac{d}{da_1}MSE(a_1)$\n",
    "    \n",
    "    \n",
    "In the last step of the algorithm, you'll notice we used we used := to indicate that the value on the right is assigned to the variable on the left. While in Python, we've used to the equals operator (=) for assignment, we've used it in math (=) to signify equality. For example, a = 1 in Python assigns the value 1 to the variable a. In math, a=1 asserts that a is equal to 1. In mathematical papers, sometimes $\\leftarrow$ is also used to signify assignment:\n",
    "\n",
    "$a_1\\leftarrow a_1 − \\alpha \\frac{d}{da_1}MSE(a_1)$\n",
    "\n",
    "Selecting an appropriate initial parameter and learning rate will reduce the number of iterations required to converge, and is part of hyperparameter optimization. We won't dive into those techniques in this course and will instead focus on how the algorithm works. In the next section, we'll unpack how to calculate the derivative of the error function at each iteration of the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIbVzruna1LW"
   },
   "source": [
    "## Derivative Of The Cost Function\n",
    "\n",
    "\n",
    "In mathematical optimization, a function that we optimize through minimization is known as a **cost function** or sometime as the [loss function](https://en.wikipedia.org/wiki/Loss_function). Because we're trying to fit a single parameter model, we can replace with $y^{(i)}$ with $a_1x_1^{(i)}$ in the cost function:\n",
    "\n",
    "$$MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(a_1x_1^{(i)}−y^{(i)})^2$$\n",
    "\n",
    "In this screen, we'll apply calculus properties to simplify this derivative to something we can compute. **We encourage you to follow along using pencil and paper, and see if you can apply the properties we mention at each step to obtain the same result we did**. Note that while you'll probably never have to implement gradient descent yourself (as most packages have high performance implementations), understanding the math will help make it easier for you to debug when you run into issues.\n",
    "\n",
    "$$\\frac{d}{da_1}MSE(a_1)=\\frac{d}{da_1}\\frac{1}{m}\\sum_{i=1}^{m}(a_1x_1^{(i)}−y^{(i)})^2$$\n",
    "\n",
    "\n",
    "By applying the [linearity of differentiation](https://en.wikipedia.org/wiki/Linearity_of_differentiation) property from calculus, we can bring the derivative term inside the summation:\n",
    "\n",
    "$$\\frac{d}{da_1}MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{d}{da_1}(a_1x_1^{(i)}−y^{(i)})^2$$\n",
    "\n",
    "We can apply both the power rule and the chain rule to simplify this. You can read more about the chain rule [here](https://en.wikipedia.org/wiki/Chain_rule) or observe how both are applied together [here](https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/differentiating-powers-of-functions):\n",
    "\n",
    "$$\\frac{d}{da_1}MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}2(a_1x_1^{(i)}−y^{(i)})\\frac{d}{da_1}(a_1x_1^{(i)}−y^{(i)})$$\n",
    "\n",
    "Because we're differentiating $a_1x_1^{(i)}−y^{(i)}$ with respect to $a_1$, we treat $y^{(i)}$ and $x_1^{(i)}$ as constants. $\\frac{d}{da_1}(a_1x_1^{(i)}−y^{(i)})$ then simplifies to just $x_1^{(i)}$:\n",
    "\n",
    "$$\\frac{d}{da_1}MSE(a_1)=\\frac{2}{m}\\sum_{i=1}^{m}x_1^{(i)}(a_1x_1^{(i)}−y^{(i)})$$\n",
    "\n",
    "For every iteration of gradient descent:\n",
    "\n",
    "- this derivative is computed using the current $a_1$ value\n",
    "- the derivative is multiplied by the learning rate ($\\alpha$): $\\alpha \\frac{d}{da_1}MSE(a_1)$\n",
    "- the result is subtracted from the current parameter value and assigned as the new parameter value: $a1:=a_1−\\alpha \\frac{d}{da_1}MSE(a_1)$\n",
    "\n",
    "Here's what this would look like in code if we ran gradient descent for **10** iterations:\n",
    "\n",
    "```python\n",
    "a1_list = [1000]\n",
    "alpha = 10\n",
    "for x in range(0, 10):\n",
    "    a1 = a1_list[x]\n",
    "    deriv = derivative(a1, xi_list, yi_list)\n",
    "    a1_new = a1 - alpha*deriv\n",
    "    a1_list.append(a1_new)\n",
    "```\n",
    "\n",
    "To test your understanding, implement the **derivative()** function.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Finish implementing the **derivative()** function:\n",
    "  - This function should return the derivative at the current value of $a_1$.\n",
    "- Uncomment the 2 lines of code that run the **gradient_descent()** function, assign the list of iterations for the  $a_1$ parameter to **param_iterations**, and assign the last iteration for $a_1$  to **final_param**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgBnM4fGbxEc"
   },
   "outputs": [],
   "source": [
    "def derivative(a1, xi_list, yi_list):\n",
    "    # Modify this function.\n",
    "    return 0\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
    "    a1_list = [a1_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        deriv = derivative(a1, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha*deriv\n",
    "        a1_list.append(a1_new)\n",
    "    return(a1_list)\n",
    "\n",
    "# Uncomment when ready.\n",
    "# param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
    "# final_param = param_iterations[-1]\n",
    "\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lT0Cpm1Zdd_c"
   },
   "source": [
    "## Understanding Multi Parameter Gradient Descent\n",
    "\n",
    "\n",
    "Now that we've understood how single parameter gradient descent works, let's build some intuition for multi parameter gradient descent. Let's start by visualizing the MSE as a function of the parameter values for the following simple linear regression model:\n",
    "\n",
    "$$SalePrice=a_1∗GrLivArea+a_0$$\n",
    "\n",
    "In the below image, we've generated a 3D scatter plot with:\n",
    "\n",
    "- a_0 on the x-axis\n",
    "- a_1 on the y-axis\n",
    "- MSE on the z-axis\n",
    "\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1437-oWIyZlUkEgW1Kdb2fWlR3jbjfDRV\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0U68_YaOduM0"
   },
   "source": [
    "## Gradient Of The Cost Function\n",
    "\n",
    "\n",
    "The [gradient](https://en.wikipedia.org/wiki/Gradient) is a multi variable generalization of the derivative. In the last few screens, we were concerned with minimizing the following cost function:\n",
    "\n",
    "$$MSE(a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(a_1x_1^{(i)}−y^{(i)})^2$$\n",
    "\n",
    "When we have 2 parameter values ($a_0$ and $a_1$), the cost function is now a function of 2 variables, not 1:\n",
    "\n",
    "$$MSE(a_0,a_1)=\\frac{1}{m}\\sum_{i=1}^{m}(a_0+a_1x_1^{(i)}−y^{(i)})^2$$\n",
    "\n",
    "\n",
    "Instead of one update rule, we now need two update rules. We need one for $a_0$:\n",
    "\n",
    "$$a_0:=a_0−\\alpha \\frac{d}{da_0}MSE(a_0,a_1)$$\n",
    "\n",
    "and one for $a_1$:\n",
    "\n",
    "$$a_1:=a_1−\\alpha \\frac{d}{da_1}MSE(a_0,a_1)$$\n",
    "\n",
    "Earlier in this mission, we determined that $\\frac{d}{da_1}MSE(a_1)$ worked out to $\\frac{2}{n}\\sum_{i=1}^{n}x_1^{(i)}(a_1x_1^{(i)}−y^{(i)})$. For the multiparameter case, we need to include the additional parameter :\n",
    "\n",
    "$$\\frac{d}{da_1}MSE(a_0,a_1)=\\frac{2}{m}\\sum_{i=1}^{m}x_1^{(i)}(a_0+a_1x_1^{(i)}−y^{(i)})$$\n",
    "\n",
    "For $\\frac{d}{da_0}MSE(a_0,a_1)$, we won't walk through the proof for this derivative, but it's similar to the one we did for $a_1$ and **we encourage you to derive this yourself on pencil and paper**:\n",
    "\n",
    "$$\\frac{d}{da_0}MSE(a_0,a_1)=\\frac{2}{m}\\sum_{i=1}^{m}(a_0+a_1x_1^{(i)}−y^{(i)})$$\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Implement the **a0_derivative()** function, which implements the gradient for $a_0$.\n",
    "  - Even though we're working on the multiparameter case, let's keep this function name consistent with the previous one we implemented (**a1_derivative()**).\n",
    "  - You'll notice that we added the **a0** parameter to the function parameters. This is because we need both parameters for the individual parameter updates (verify this by looking at the math we explored in this screen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_P58Sfzfpzg"
   },
   "outputs": [],
   "source": [
    "def a1_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def a0_derivative(a0, a1, xi_list, yi_list):\n",
    "    return 1\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
    "    a1_list = [a1_initial]\n",
    "    a0_list = [a0_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        a0 = a0_list[i]\n",
    "        \n",
    "        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)\n",
    "        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)\n",
    "        \n",
    "        a1_new = a1 - alpha*a1_deriv\n",
    "        a0_new = a0 - alpha*a0_deriv\n",
    "        \n",
    "        a1_list.append(a1_new)\n",
    "        a0_list.append(a0_new)\n",
    "    return(a0_list, a1_list)\n",
    "\n",
    "# Uncomment when ready.\n",
    "# a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49wXYd7BhBy2"
   },
   "source": [
    "## Gradient Descent For Higher Dimensions\n",
    "\n",
    "\n",
    "What if we want to use many parameters in our model? Gradient descent actually scales to as many variables as you want. Each parameter value will need its own update rule, and it closely matches the update rule for $a_1$:\n",
    "\n",
    "$$a_0:=a_0−\\alpha \\frac{d}{da_0}MSE$$\n",
    "$$a_1:=a_1−\\alpha \\frac{d}{da_1}MSE$$\n",
    "$$a_2:=a_2−\\alpha \\frac{d}{da_2}MSE$$\n",
    "$$a_3:=a_3−\\alpha \\frac{d}{da_3}MSE$$\n",
    "$$a_n:=a_n−\\alpha \\frac{d}{da_n}MSE$$\n",
    "\n",
    "Besides the derivative for the MSE with respect to the intercept value ($a_0$), the derivative for other parameters are identical:\n",
    "\n",
    "$$\\frac{d}{da_1}MSE=\\frac{2}{m}\\sum_{i=1}^{m}x_1^{(i)}(\\hat{y}^{(i)}−y^{(i)})$$\n",
    "$$\\frac{d}{da_2}MSE=\\frac{2}{m}\\sum_{i=1}^{m}x_2^{(i)}(\\hat{y}^{(i)}−y^{(i)})$$\n",
    "$$\\frac{d}{da_n}MSE=\\frac{2}{m}\\sum_{i=1}^{m}x_n^{(i)}(\\hat{y}^{(i)}−y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdgEAu-dhFxe"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In this section, we explored how to find a linear regression model using the gradient descent algorithm. The main challenges with gradient descent include:\n",
    "\n",
    "- choosing good initial parameter values\n",
    "- choosing a good learning rate (falls under the domain of hyperparameter optimization)\n",
    "\n",
    "In the next section, we'll explore a technique called OLS estimation which doesn't require any parameter or hyperparameter value selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Dab-1DchykU",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Ordinary Least Squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2AVWRP1kBMC"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In the last section, we explored an iterative technique for model fitting named gradient descent. The gradient descent algorithm requires multiple iterations to converge on the optimal parameter values and the number of iterations is highly dependent on the initial parameter values and the learning rate we select.\n",
    "\n",
    "In this mission, we'll explore a technique called **ordinary least squares** estimation or OLS estimation for short. Unlike gradient descent, OLS estimation provides a clear formula to directly calculate the optimal parameter values that minimizes the cost function. To understand OLS estimation, we need to first frame our linear regression problem in the matrix form. We've mostly worked with the following form of the linear regression model:\n",
    "\n",
    "$\\hat{y} = a_0 + a_1x_1 + a_2x_2 + \\ldots + a_nx_n$\n",
    "\n",
    "While this form represents the relationship between the features ($x_1$ to $x_2$ ) and the target column ($y$) well when there are just a few parameter values, it doesn't scale well to when we have hundreds of parameters. If you recall from the Linear Algebra course, we can explore how matrix notation lets us better represent and reason about a linear system with many variables. With that in mind, here's what the matrix form of our linear regression model looks like:\n",
    "\n",
    "$Xa=\\hat{y}$\n",
    "\n",
    "\n",
    "Where $X$ is a matrix representing the columns from the training set our model uses, $a$  is a vector representing the parameter values, and $\\hat{y}$  is the vector of predictions. Here's a diagram with some sample values for each:\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1U2R9tC1SOJnHGZd-R4vo78tHFS94vPNp\">\n",
    "\n",
    "Now that we've gained an understanding for the matrix representation of the linear regression model, let's take a peek at the OLS estimation formula that results in the optimal vector $a$:\n",
    "\n",
    "$a = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "Let's start by computing OLS estimation to find the best parameters for a model using the following features:\n",
    "\n",
    "```python\n",
    "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
    "       'Gr Liv Area', 'Overall Qual']\n",
    "```\n",
    "\n",
    "In the following screens, we'll dive into the mathematical derivation of the OLS estimation technique. It's important to note that you'll most likely never implement this technique in a data science role and will instead use an existing, efficient implementation (scikit-learn uses OLS under the hood when you call **fit()** on a [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) instance).\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Select just the **features** in features from the training set and assign to **X**.\n",
    "- Select the **SalePrice** column from the training set and assign to **y**.\n",
    "- Use the OLS estimation formula to return the optimal parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P45tpRvplWwT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
    "       'Gr Liv Area', 'Overall Qual']\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tMCWBOYnbZq"
   },
   "source": [
    "## Cost Function\n",
    "\n",
    "Unlike gradient descent, OLS estimation provides what is known as a **closed form solution** to the problem of finding the optimal parameter values. A closed form solution is one where a solution can be computed arithmetically with a predictable amount of mathematical operations. Gradient descent, on the other hand, is an algorithmic approach that can require a different number of iteration (and therefore a different number of mathematical operations) based on the initial parameter values, the learning rate, etc. While the approach is different, both techniques share the high level objective of minimizing the cost function.\n",
    "\n",
    "Before we can dive into how the cost function is represented in the matrix form, let's understand how the error is represented. Because the error is the difference between the predictions made using the model $\\hat{y}$  and the actual labels $y$ , it's represented as a vector. The greek letter for E (epsilon $\\epsilon$) is often used to represent the error vector:\n",
    "\n",
    "$\\epsilon = \\hat{y} - y$\n",
    "\n",
    "We can build on this to define $y$:\n",
    "\n",
    "$y = Xa - \\epsilon$\n",
    "\n",
    "\n",
    "Even though this closely resembles the matrix equation of $Ax=b$, we have 2 unknowns (the vector $a$ and the vector $\\hat{y}$ . We're looking for a model, represented using the parameter vector $a$ , that will minimize the mean squared error between the labels, $y$ , and the predictions, $\\hat{y}$. Said another way, the cost function is this mean squared error.\n",
    "\n",
    "Here's what the cost function looks like in matrix form:\n",
    "\n",
    "$\\displaystyle J(a) = \\frac{1}{n}(Xa- y)^T(Xa-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oDedTY9ocPb"
   },
   "source": [
    "## Derivative Of The Cost Function\n",
    "\n",
    "The derivative of the cost function is decently involved, and out of scope for this lesson. Understanding the derivation requires some familiarity with [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus), which is a specific notation for applying calculus concepts to matrices. If you're interested in the derivation, we recommend that you read Eli Bendersky's wonderful walkthrough of the derivation on his [blog](http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/).\n",
    "\n",
    "Here's the derivative of the cost function:\n",
    "\n",
    "$\\displaystyle \\frac{dJ(a)}{da}=2X^TXa - 2X^Ty$\n",
    "\n",
    "To find the vector $a$  that minimizes the cost function $J(a)$, we need to set the derivative equal 0 to $a$  and solve for :\n",
    "\n",
    "$2X^TXa - 2X^Ty = 0$\n",
    "\n",
    "Let's move the second term to the right hand side and divide both sides by 2:\n",
    "\n",
    "$X^TXa = X^Ty $\n",
    "\n",
    "Our goal is to isolate $a$, the parameter vector. The last step we need to perform is \"divide out\" $X^TX$ from the left hand side.\n",
    "\n",
    "If you recall, we can \"divide\" matrix terms by computing the inverse. Let's dig up the example we explored in the linear algebra course. We can cancel $A$ from the following equation  $Ax=b$ by multiplying both sides by the inverse $A^{-1}Ax = A^{-1}b$. This leaves us with  $x = A^{-1}b$.\n",
    "\n",
    "To cancel $X^TX$ from the left side, we need to compute the inverse of it and multiply it by both sides. We're now left with the OLS estimation formula:\n",
    "\n",
    "$a = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avidqXlzqP4J"
   },
   "source": [
    "## Gradient Descent vs. Ordinary Least Squares\n",
    "\n",
    "Now that we've explored a lot of the math that underlies OLS estimation, let's understand its limitations. The biggest limitation is that OLS estimation is computationally expensive when the data is large. This is because computing a matrix inverse has a computational complexity of apprximately O(n^3). You can read more about computational complexity of the matrix inverse and other common matrix operations on [Wikipedia](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra).\n",
    "\n",
    "OLS is commonly used when the number of elements in the dataset (and therefore the matrix that's inverted) is less than a few million elements. On larger datasets, gradient descent is used because it's much more flexible. For many practical problems, we can set a threshold accuracy value (or a set number of iterations) and use a \"good enough\" solution. This is especially useful when iterating and trying different features in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9kFyZDFtMgu"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "In this section, we explored a closed form solution to fitting a linear regression model called OLS estimation. We explored some of the intuition behind the math for this technique and ended by exploring it's computational complexity. In the next section, we'll explore how to clean some of the remaining features in the training set to use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkvXq3j_teqa",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Processing and Transforming Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehXimuKatv5X"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "To understand how linear regression works, we've stuck to using features from the training dataset that contained no missing values and were already in a convenient numeric representation. In this mission, we'll explore how to transform some of the the remaining features so we can use them in our model. Broadly, the process of processing and creating new features is known as [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering). Feature engineering is a bit of an art and having knowledge in the specific domain (in this case real estate) can help you create better features. In this mission, we'll focus on some domain-independent strategies that work for all problems.\n",
    "\n",
    "In the first half of this mission, we'll focus only on columns that contain no missing values but still aren't in the proper format to use in a linear regression model. In the latter half of this mission, we'll explore some ways to deal with missing values.\n",
    "\n",
    "Amongst the columns that don't contain missing values, some of the common issues include:\n",
    "\n",
    "- the column is not numerical (e.g. a zoning code represented using text)\n",
    "- the column is numerical but not ordinal (e.g. zip code values)\n",
    "- the column is numerical but isn't representative of the type of relationship with the target column (e.g. year values)\n",
    "\n",
    "Let's start by filtering the training set to just the columns containing no missing values.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Select just the columns from the **train** data frame that contain no missing values.\n",
    "- Assign the resulting data frame, that contains just these columns, to **df_no_mv**.\n",
    "- Use the variables display to become familiar with these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iH5YiIkuEAw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "train_null_counts = train.isnull().sum()\n",
    "print(train_null_counts)\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFtp_kXEu-At"
   },
   "source": [
    "## Categorical features\n",
    "\n",
    "You'll notice that some of the columns in the data frame **df_no_mv** contain string values. If these columns contain only a limited set of uniuqe values, they're known as **categorical features**. As the name suggests, a categorical feature groups a specific training example into a specific category. Here are some examples from the dataset:\n",
    "\n",
    "```python\n",
    ">>> train['Utilities'].value_counts()\n",
    "AllPub    1457\n",
    "NoSewr       2\n",
    "NoSeWa       1\n",
    "Name: Utilities, dtype: int64\n",
    ">>> train['Street'].value_counts()\n",
    "Pave    1455\n",
    "Grvl       5\n",
    ">>> train['House Style'].value_counts()\n",
    "1Story    743\n",
    "2Story    440\n",
    "1.5Fin    160\n",
    "SLvl       60\n",
    "SFoyer     35\n",
    "2.5Unf     11\n",
    "1.5Unf      8\n",
    "2.5Fin      3\n",
    "```\n",
    "\n",
    "To use these features in our model, we need to transform them into numerical representations. Thankfully, pandas makes this easy because the library has a special categorical data type. We can convert any column that contains no missing values (or an error will be thrown) to the categorical data type using the **pandas.Series.astype()** method:\n",
    "\n",
    "```python\n",
    ">>> train['Utilities'] = train['Utilities'].astype('category')\n",
    "```\n",
    "\n",
    "When a column is converted to the categorical data type, pandas assigns a code to each unique value in the column. Unless we access these values directly, most of the pandas manipulation operations that work for string columns will work for categorical ones as well.\n",
    "\n",
    "```python\n",
    ">>> train['Utilities']\n",
    "0       AllPub\n",
    "1       AllPub\n",
    "2       AllPub\n",
    "3       AllPub\n",
    "4       AllPub\n",
    "5       AllPub\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "We need to use the **.cat** accessor followed by the **.codes** property to actually access the underlying numerical representation of a column:\n",
    "\n",
    "```python\n",
    ">>> train['Utilities'].cat.codes\n",
    "```\n",
    "\n",
    "Let's convert all of the text columns that contain no missing values into the categorical data type.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Convert all of the text columns in **train** to the categorical data type.\n",
    "- Select the **Utilities** column, return the categorical codes, and display the unique value counts for those codes: **train['Utilities'].cat.codes.value_counts()**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noQAAmv_vYF6"
   },
   "outputs": [],
   "source": [
    "text_cols = df_no_mv.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in text_cols:\n",
    "    print(col+\":\", len(train[col].unique()))\n",
    "    \n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EF1GPIxfwIyr"
   },
   "source": [
    "## Dummy Coding\n",
    "\n",
    "\n",
    "When we convert a column to the categorical data type, pandas assigns a number from 0 to n-1 (where n is the number of unique values in a column) for each value. The drawback with this approach is that one of the assumptions of linear regression is violated here. Linear regression operates under the assumption that the features are linearly correlated with the target column. For a categorical feature, however, there's no actual numerical meaning to the categorical codes that pandas assigned for that colum. An increase in the **Utilities** column from 1 to 2 has no correlation value with the target column, and the categorical codes are instead used for uniqueness and exclusivity (the category associated with 0 is different than the one associated with 1).\n",
    "\n",
    "The common solution is to use a technique called [dummy coding](https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29). Instead of having a single column with n integer codes, we have n binary columns. Here's what that would look like for the **Utilities** column:\n",
    "\n",
    "| Utilities_AllPub  | Utilities_NoSewr | Utilities_NoSeWa  |\n",
    "|-----------------------------------|------------------|---|\n",
    "| 1                                 | 0                | 0 |\n",
    "| 1                                 | 0                | 0 |\n",
    "| 1                                 | 0                | 0 |\n",
    "| 1                                 | 0                | 0 |\n",
    "\n",
    "\n",
    "Because the original values for the first 4 rows were **AllPub**, in the new scheme, they contain the binary value for true (1) in the **Utilities_AllPub** column and 0 for the other 2 columns.\n",
    "\n",
    "Pandas thankfully has a convenience method to help us apply this transformation for all of the text columns called **pandas.get_dummies()**:\n",
    "\n",
    "```python\n",
    "dummy_cols = pd.get_dummies()\n",
    "```\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Convert all of the columns in **text_cols** from the **train** data frame into dummy columns.\n",
    "- Delete the original columns from **text_cols** from the **train** data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWbLcCZiwNPp"
   },
   "outputs": [],
   "source": [
    "dummy_cols = pd.DataFrame()\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wAWbgE-xfQI"
   },
   "source": [
    "## Transforming Improper Numerical Features\n",
    "\n",
    "In the last few sections, we focused on categorical values that were represented as text columns. Some of the numerical columns in the data set are also categorical and only have a limited set of unique values. We won't explicitly explore those columns in this mission, but the feature transformation process is the same if the numbers used in those categories have no numerical meaning.\n",
    "\n",
    "Let's now look at numerical features that aren't categorical, but whose numerical representation needs to be improved. We'll focus on the **Year Remod/Add** and **Year Built** columns:\n",
    "\n",
    "```python\n",
    ">>> train[['Year Remod/Add', 'Year Built']]\n",
    "0   1960    1960\n",
    "1   1961    1961\n",
    "2   1958    1958\n",
    "3   1968    1968\n",
    "4   1998    1997\n",
    "...\n",
    "```\n",
    "\n",
    "The two main issues with these features are:\n",
    "\n",
    "- Year values aren't representative of how old a house is\n",
    "- The **Year Remod/Add** column doesn't actually provide useful information for a linear regression model\n",
    "\n",
    "\n",
    "The challenge with year values like **1960** and **1961** is that they don't do a good job of capturing how old a house is. For example, a house that was built in **1960** but sold in **1980** was sold in half the time one built in **1960** and sold in **2000**. Instead of the years certain events happened, we want the difference between those years. We should create a new column that's the difference between both of these columns.\n",
    "\n",
    "For this particular piece of information (years until remodeled), this is a sensible approach. Domain knowledge can help you understand how to best transform features to represent information well for a linear model. If you're ever confused about a feature or how it should be represented, reading scientific papers or posts by researchers in the specific domain is critical. Many winners of [Kaggle data science competitions](https://www.import.io/post/how-to-win-a-kaggle-competition/), for example, claim that their focus on data preparation and feature engineering combined with common machine learning models helped them win.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Create a new column **years_until_remod** in the train data frame that represents the difference between **Year Remod/Add** (the later value) and **Year Built** (the earlier value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSwjqLJuz6C3"
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okGiML820Bjv"
   },
   "source": [
    "## Missing Values\n",
    "\n",
    "In the next few screens, we'll focus on handling columns with missing values. When values are missing in a column, there are two main approaches we can take:\n",
    "\n",
    "- Remove rows containing missing values for specific columns\n",
    "    - Pro: Rows containing missing values are removed, leaving only clean data for modeling\n",
    "    - Con: Entire observations from the training set are removed, which can reduce overall prediction accuracy\n",
    "- Impute (or replace) missing values using a descriptive statistic from the column\n",
    "    - Pro: Missing values are replaced with potentially similar estimates, preserving the rest of the observation in the model.\n",
    "    - Con: Depending on the approach, we may be adding noisy data for the model to learn\n",
    "\n",
    "Given that we only have 1460 training examples (with ~80 potentially useful features), we don't want to remove any of these rows from the dataset. Let's instead focus on **imputation** techniques.\n",
    "\n",
    "We'll focus on columns that contain at least 1 missing value but less than 365 missing values (or 25% of the number of rows in the training set). There's no strict threshold, and many people instead use a 50% cutoff (if half the values in a column are missing, it's automatically dropped). Having some domain knowledge can help with determining an acceptable cutoff value.\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Select only the columns from **train** that contain more than 0 missing values but less than 584 missing values. Assign the resulting data frame to **df_missing_values**.\n",
    "- Display the number of missing values for each column in **df_missing_values**.\n",
    "- Display the data type for each column in **df_missing_values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83i_e5YF0E4t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "train_null_counts = train.isnull().sum()\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbV5fokY0mn5"
   },
   "source": [
    "## Imputing Missing Values\n",
    "\n",
    "\n",
    "It looks like about half of the columns in **df_missing_values** are string columns (**object** data type), while about half are **float64** columns. For numerical columns with missing values, a common strategy is to compute the mean, median, or mode of each column and replace all missing values in that column with that value.\n",
    "\n",
    "Because imputation is a common task, pandas contains a [pandas.DataFrame.fillna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) method that we can use for this. If we pass in a value, all of the missing values (**NaN**) in the data frame are replaced by that value:\n",
    "\n",
    "```python\n",
    "# Only select float columns.\n",
    "missing_floats = df_missing_vals.select_dtypes(include=['float'])\n",
    "# Returns a data frame with missing values replaced with 0.\n",
    "fill_with_zero = missing_floats.fillna(0)\n",
    "```\n",
    "\n",
    "You can also pass in a column-wise summarization function and fill in missing values that way:\n",
    "\n",
    "\n",
    "```python\n",
    "# Returns a data frame with missing values replaced with mean of that column.\n",
    "fill_with_mean = missing_floats.fillna(missing_floats.mean())\n",
    "```\n",
    "\n",
    "Let's impute all of the missing values in float columns with each column's mean.\n",
    "\n",
    "\n",
    "**Exercise Start**\n",
    "\n",
    "<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
    "\n",
    "- Impute the missing values from **float_cols** with the column's mean.\n",
    "- Check for any missing values in **float_cols**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2ycs88G0qUp"
   },
   "outputs": [],
   "source": [
    "float_cols = df_missing_values.select_dtypes(include=['float'])\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLgIFXY_3YXu",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Predicting House Sale Prices (guided project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNrr9yKQ3p9S"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesson, we started by building intuition for model based learning, explored how the linear regression model worked, understood how the two different approaches to model fitting worked, and some techniques for **cleaning, transforming, and selecting features**. In this guided project, you can practice what you learned in this course by exploring ways to improve the models we built.\n",
    "\n",
    "You'll work with housing data for the city of Ames, Iowa, United States from 2006 to 2010. You can read more about why the data was collected [here](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf). You can also read about the different columns in the data [here](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt).\n",
    "\n",
    "Let's start by setting up a pipeline of functions that will let us quickly iterate on different models.\n",
    "\n",
    "\n",
    "<img width=\"200\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1ZEtgNq4ig4GfQWZQlOulAHLRu1JKfiKh\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqoQ8N2R3nmp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSemPcqX363u"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/AmesHousing.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HOi5e_e4BNH"
   },
   "outputs": [],
   "source": [
    "def transform_features(df):\n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
    "\n",
    "def train_and_test(df):  \n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    \n",
    "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
    "    ## and return only those columns as a data frame.\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    ## You can use `pd.Series.drop()` to drop a value.\n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHOQs2rV4HXi"
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Handle missing values:\n",
    "  - All columns:\n",
    "    - Drop any with 5% or more missing values for now.\n",
    "  - Text columns:\n",
    "    - Drop any with 1 or more missing values for now.\n",
    "  - Numerical columns:\n",
    "     - For columns with missing values, fill in with the most common value in that column\n",
    "     \n",
    "1: All columns: Drop any with 5% or more missing values **for now**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnicBXgu4whY"
   },
   "outputs": [],
   "source": [
    "## Series object: column name -> number of missing values\n",
    "num_missing = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPbzdOUp40Ju"
   },
   "outputs": [],
   "source": [
    "# Filter Series to columns containing >5% missing values\n",
    "drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
    "\n",
    "# Drop those columns from the data frame. Note the use of the .index accessor\n",
    "df = df.drop(drop_missing_cols.index, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEcJabsk43J8"
   },
   "source": [
    "2: Text columns: Drop any with 1 or more missing values **for now**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kZbnk_F48Mx"
   },
   "outputs": [],
   "source": [
    "## Series object: column name -> number of missing values\n",
    "text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "## Filter Series to columns containing *any* missing values\n",
    "drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
    "\n",
    "df = df.drop(drop_missing_cols_2.index, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhYhKXQi4_qz"
   },
   "source": [
    "3: Numerical columns: For columns with missing values, fill in with the most common value in that column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_g57emn5GsG"
   },
   "outputs": [],
   "source": [
    "## Compute column-wise missing value counts\n",
    "num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "fixable_numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGCMO4jq5MLa"
   },
   "outputs": [],
   "source": [
    "## Compute the most common value for each column in `fixable_nmeric_missing_cols`.\n",
    "replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "replacement_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fuap2wFH5PTr"
   },
   "outputs": [],
   "source": [
    "## Use `pd.DataFrame.fillna()` to replace missing values.\n",
    "df = df.fillna(replacement_values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwb-Cp885SJp"
   },
   "outputs": [],
   "source": [
    "## Verify that every column has 0 missing values\n",
    "df.isnull().sum().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKnFzTR05Vzv"
   },
   "source": [
    "What new features can we create, that better capture the information in some of the features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNydSUqe5XCx"
   },
   "outputs": [],
   "source": [
    "years_sold = df['Yr Sold'] - df['Year Built']\n",
    "years_sold[years_sold < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTQAvsi75Z9D"
   },
   "outputs": [],
   "source": [
    "years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
    "years_since_remod[years_since_remod < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvxuG0_x5cwk"
   },
   "outputs": [],
   "source": [
    "## Create new columns\n",
    "df['Years Before Sale'] = years_sold\n",
    "df['Years Since Remod'] = years_since_remod\n",
    "\n",
    "## Drop rows with negative values for both of these new features\n",
    "df = df.drop([1702, 2180, 2181], axis=0)\n",
    "\n",
    "## No longer need original year columns\n",
    "df = df.drop([\"Year Built\", \"Year Remod/Add\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91YFKRrB5fvP"
   },
   "source": [
    "Drop columns that:\n",
    "\n",
    "- that aren't useful for ML\n",
    "- leak data about the final sale, read more about columns [here](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzFy4ey25-ww"
   },
   "outputs": [],
   "source": [
    "## Drop columns that aren't useful for ML\n",
    "df = df.drop([\"PID\", \"Order\"], axis=1)\n",
    "\n",
    "## Drop columns that leak info about the final sale\n",
    "df = df.drop([\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbstjHFL6CEC"
   },
   "source": [
    "Let's update **transform_features()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "528NwhWQ6DPy"
   },
   "outputs": [],
   "source": [
    "def transform_features(df):\n",
    "    num_missing = df.isnull().sum()\n",
    "    drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
    "    df = df.drop(drop_missing_cols.index, axis=1)\n",
    "    \n",
    "    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
    "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
    "    df = df.drop(drop_missing_cols_2.index, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "    \n",
    "    years_sold = df['Yr Sold'] - df['Year Built']\n",
    "    years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    df['Years Before Sale'] = years_sold\n",
    "    df['Years Since Remod'] = years_since_remod\n",
    "    df = df.drop([1702, 2180, 2181], axis=0)\n",
    "\n",
    "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis=1)\n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
    "\n",
    "def train_and_test(df):  \n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    \n",
    "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
    "    ## and return only those columns as a data frame.\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    ## You can use `pd.Series.drop()` to drop a value.\n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "df = pd.read_csv(\"Dataset/AmesHousing.txt\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaNb5DZi6SSy"
   },
   "source": [
    "## Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvkgoU-O6Wv-"
   },
   "outputs": [],
   "source": [
    "numerical_df = transform_df.select_dtypes(include=['int', 'float'])\n",
    "numerical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJNmAnxF6bHn"
   },
   "outputs": [],
   "source": [
    "abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "abs_corr_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W52VZheI6fB8"
   },
   "outputs": [],
   "source": [
    "## Let's only keep columns with a correlation coefficient of larger than 0.4 (arbitrary, worth experimenting later!)\n",
    "abs_corr_coeffs[abs_corr_coeffs > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-2Gcvhb6itT"
   },
   "outputs": [],
   "source": [
    "## Drop columns with less than 0.4 correlation with SalePrice\n",
    "transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs < 0.4].index, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j48djW9S6riG"
   },
   "source": [
    " Which categorical columns should we keep?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04gdKrjD6x70"
   },
   "outputs": [],
   "source": [
    "## Create a list of column names from documentation that are *meant* to be categorical\n",
    "nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_upuqTiZ638s"
   },
   "source": [
    "- Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don't have any semantic meaning)?\n",
    "- If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJtLQahR7DR4"
   },
   "outputs": [],
   "source": [
    "## Which categorical columns have we still carried with us? We'll test tehse \n",
    "transform_cat_cols = []\n",
    "for col in nominal_features:\n",
    "    if col in transform_df.columns:\n",
    "        transform_cat_cols.append(col)\n",
    "\n",
    "## How many unique values in each categorical column?\n",
    "uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "## Aribtrary cutoff of 10 unique values (worth experimenting)\n",
    "drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "transform_df = transform_df.drop(drop_nonuniq_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ssf34mxP7Vx0"
   },
   "outputs": [],
   "source": [
    "## Select just the remaining text columns and convert to categorical\n",
    "text_cols = transform_df.select_dtypes(include=['object'])\n",
    "for col in text_cols:\n",
    "    transform_df[col] = transform_df[col].astype('category')\n",
    "    \n",
    "## Create dummy columns and add back to the dataframe!\n",
    "transform_df = pd.concat([\n",
    "    transform_df, \n",
    "    pd.get_dummies(transform_df.select_dtypes(include=['category']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fml7rAha7Yx1"
   },
   "source": [
    "Update **select_features()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QE49vTlK7ZxX"
   },
   "outputs": [],
   "source": [
    "def transform_features(df):\n",
    "    num_missing = df.isnull().sum()\n",
    "    drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
    "    df = df.drop(drop_missing_cols.index, axis=1)\n",
    "    \n",
    "    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
    "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
    "    df = df.drop(drop_missing_cols_2.index, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "    \n",
    "    years_sold = df['Yr Sold'] - df['Year Built']\n",
    "    years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    df['Years Before Sale'] = years_sold\n",
    "    df['Years Since Remod'] = years_since_remod\n",
    "    df = df.drop([1702, 2180, 2181], axis=0)\n",
    "\n",
    "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis=1)\n",
    "    return df\n",
    "\n",
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
    "    \n",
    "    transform_cat_cols = []\n",
    "    for col in nominal_features:\n",
    "        if col in df.columns:\n",
    "            transform_cat_cols.append(col)\n",
    "\n",
    "    uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "    df = df.drop(drop_nonuniq_cols, axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_and_test(df, k=0):\n",
    "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
    "    features = numeric_df.columns.drop(\"SalePrice\")\n",
    "    lr = linear_model.LinearRegression()\n",
    "    \n",
    "    if k == 0:\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "\n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions = lr.predict(test[features])\n",
    "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        return rmse\n",
    "    \n",
    "    if k == 1:\n",
    "        # Randomize *all* rows (frac=1) from `df` and return\n",
    "        shuffled_df = df.sample(frac=1, )\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "        \n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions_one = lr.predict(test[features])        \n",
    "        \n",
    "        mse_one = mean_squared_error(test[\"SalePrice\"], predictions_one)\n",
    "        rmse_one = np.sqrt(mse_one)\n",
    "        \n",
    "        lr.fit(test[features], test[\"SalePrice\"])\n",
    "        predictions_two = lr.predict(train[features])        \n",
    "       \n",
    "        mse_two = mean_squared_error(train[\"SalePrice\"], predictions_two)\n",
    "        rmse_two = np.sqrt(mse_two)\n",
    "        \n",
    "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
    "        print(rmse_one)\n",
    "        print(rmse_two)\n",
    "        return avg_rmse\n",
    "    else:\n",
    "        kf = KFold(n_splits=k, shuffle=True)\n",
    "        rmse_values = []\n",
    "        for train_index, test_index, in kf.split(df):\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            lr.fit(train[features], train[\"SalePrice\"])\n",
    "            predictions = lr.predict(test[features])\n",
    "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "        print(rmse_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        return avg_rmse\n",
    "\n",
    "df = pd.read_csv(\"Dataset/AmesHousing.txt\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df, k=4)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEWWZzS3eLKp"
   },
   "source": [
    "# Gradient Descent (algebra based) for any number of features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0sNAxalwjXZ"
   },
   "source": [
    "\n",
    "The next step is to implement gradient descent for any number of features. Fortunately, the update step generalizes easily, and can be vectorized to avoid iterating through $\\theta_j$ values as might be suggested by the single variable implementation presented in lesson 5:\n",
    "\n",
    "$$\n",
    "\\displaystyle \\theta_j = \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pkQ5keDxQvx"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def gradient_descent_multi(X, y, theta, alpha, iterations):\n",
    "    m = len(X)\n",
    "    for i in range(iterations):\n",
    "        gradient = (1/m) * np.dot(X.T, np.dot(X, theta) - y)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n",
    "  \n",
    "def mini_batch_gradient_descent_multi(X, y, theta, alpha, iterations, batch_size):\n",
    "  \n",
    "    for i in range(iterations):\n",
    "        # generate random numbers from 0 to X.shape[0] (exclusive)\n",
    "        # select an amount of batch sizes random numbers\n",
    "        idx = np.random.randint(X.shape[0], size=batch_size)\n",
    "      \n",
    "        # new batch dataset\n",
    "        new_X = X[idx]\n",
    "        new_y = y[idx]\n",
    "      \n",
    "        m = len(new_X)\n",
    "      \n",
    "        gradient = (1/m) * np.dot(new_X.T, np.dot(new_X, theta) - new_y)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta  \n",
    "\n",
    "def normal_eq(X, y):\n",
    "    return inv(X.T.dot(X)).dot(X.T).dot(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDo7r3TKyNsg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the dataset\n",
    "df = pd.read_csv(\"Dataset/AmesHousing.txt\", delimiter=\"\\t\")\n",
    "\n",
    "# split into train and test\n",
    "train = df[0:1460]\n",
    "test = df[1460:]\n",
    "\n",
    "# for the sake of understanding select only two features\n",
    "features = ['Overall Cond', 'Gr Liv Area']\n",
    "target = 'SalePrice'\n",
    "\n",
    "X = np.column_stack((np.ones(len(train[features])), train[features]))\n",
    "y = train[target]\n",
    "\n",
    "# basic parameters\n",
    "iterations = 2000\n",
    "alpha = .0000007\n",
    "theta_initial = [10000,100,0]\n",
    "\n",
    "# mini batch size\n",
    "mini_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1538176494440,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "aftOtst32r1y",
    "outputId": "71d35c00-2790-4dfb-dde3-f1e663511aa1"
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "theta_gradient = gradient_descent_multi(X,y,theta_initial,alpha,iterations)\n",
    "print(theta_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2050,
     "status": "ok",
     "timestamp": 1538176497801,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "c8NHD1bMPc74",
    "outputId": "63ed0d11-28c2-4ac7-90be-0bf6ee3d36c5"
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "theta_mini_batch = mini_batch_gradient_descent_multi(X,y,theta_initial,alpha,iterations,mini_batch_size)\n",
    "print(theta_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1538176503006,
     "user": {
      "displayName": "Ivanovitch Silva",
      "photoUrl": "",
      "userId": "06428777505436195303"
     },
     "user_tz": 180
    },
    "id": "b5kmA1680T4N",
    "outputId": "eb32a2c8-8a1f-4a27-9ea4-9c5b7d0438f4"
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "theta_normal = normal_eq(X,y)\n",
    "print(theta_normal)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson 5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
